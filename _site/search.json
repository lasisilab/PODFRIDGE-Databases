[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "This project analyzes 15+ years of CODIS statistics scraped from the Wayback Machine, revealing growth patterns, data anomalies, and the evolution of DNA databasing in the United States.\n\n\n\n\n\n\n\n\nThe California Typo\n\n\n\nWe discovered a data entry error in California’s October 2024 data where investigations aided was listed as “130,4657” instead of “130,465” - causing an apparent spike of over 1.3 million investigations!\n\n\n\nTotal Growth: Offender profiles grew from 8.5M (2010) to 18.4M (2025)\nData Lag: FBI statistics are typically 2-3 months old when published\nJurisdictional Changes: D.C./Metro PD stopped reporting after 2018\n\nContinue to full analysis →"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "This project analyzes 15+ years of CODIS statistics scraped from the Wayback Machine, revealing growth patterns, data anomalies, and the evolution of DNA databasing in the United States.\n\n\n\n\n\n\n\n\nThe California Typo\n\n\n\nWe discovered a data entry error in California’s October 2024 data where investigations aided was listed as “130,4657” instead of “130,465” - causing an apparent spike of over 1.3 million investigations!\n\n\n\nTotal Growth: Offender profiles grew from 8.5M (2010) to 18.4M (2025)\nData Lag: FBI statistics are typically 2-3 months old when published\nJurisdictional Changes: D.C./Metro PD stopped reporting after 2018\n\nContinue to full analysis →"
  },
  {
    "objectID": "index.html#quick-stats",
    "href": "index.html#quick-stats",
    "title": "PODFRIDGE-Databases",
    "section": "Quick Stats",
    "text": "Quick Stats\n#| echo: false #| warning: false import pandas as pd import matplotlib.pyplot as plt"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "ndis_analysis.html",
    "href": "ndis_analysis.html",
    "title": "NDIS Database Analysis",
    "section": "",
    "text": "This analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025."
  },
  {
    "objectID": "ndis_analysis.html#introduction",
    "href": "ndis_analysis.html#introduction",
    "title": "NDIS Database Analysis",
    "section": "",
    "text": "This analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025."
  },
  {
    "objectID": "ndis_analysis.html#setup",
    "href": "ndis_analysis.html#setup",
    "title": "NDIS Database Analysis",
    "section": "Setup",
    "text": "Setup\n\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom datetime import datetime\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\n\n# Set up paths\nOUTPUT_DIR = Path(\"output\")\nHTML_DIR = OUTPUT_DIR / \"wayback_html\"\n\n# Check that we have the HTML files\nhtml_files = list(HTML_DIR.glob(\"*.html\"))\nprint(f\"Found {len(html_files)} HTML files to process\")\n\nFound 317 HTML files to process"
  },
  {
    "objectID": "ndis_analysis.html#configuration",
    "href": "ndis_analysis.html#configuration",
    "title": "NDIS Database Analysis",
    "section": "Configuration",
    "text": "Configuration\nDefine our jurisdiction mappings and known typos:\n\n# Jurisdiction name standardization\nJURISDICTION_NAME_MAP = {\n    'D.C./FBI Lab': 'DC/FBI Lab',\n    'US Army': 'U.S. Army'\n}\n\n# Known data typos to fix\nKNOWN_TYPOS = [\n    {\n        'timestamp': '20250105164014',\n        'jurisdiction': 'California', \n        'field': 'investigations_aided',\n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    },\n    {\n        'timestamp': '20250116205311',\n        'jurisdiction': 'California',\n        'field': 'investigations_aided', \n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    }\n]"
  },
  {
    "objectID": "ndis_analysis.html#parser-functions",
    "href": "ndis_analysis.html#parser-functions",
    "title": "NDIS Database Analysis",
    "section": "Parser Functions",
    "text": "Parser Functions\n\ndef clean_jurisdiction_name(name):\n    \"\"\"Clean up jurisdiction names by removing common prefixes\"\"\"\n    name = re.sub(r'^.*?Back to top\\s*', '', name)\n    name = re.sub(r'^.*?Tables by NDIS Participant\\s*', '', name)\n    name = re.sub(r'^.*?ation\\.\\s*', '', name)\n    name = name.strip()\n    return name\n\ndef standardize_jurisdiction_name(name):\n    \"\"\"Standardize jurisdiction names to handle variations\"\"\"\n    name = clean_jurisdiction_name(name)\n    if name in JURISDICTION_NAME_MAP:\n        return JURISDICTION_NAME_MAP[name]\n    return name\n\ndef extract_data_date(html_content):\n    \"\"\"Extract the 'Statistics as of' date from HTML content\"\"\"\n    match = re.search(r'Statistics as of (\\w+ \\d{4})', html_content, re.IGNORECASE)\n    if match:\n        date_str = match.group(1)\n        try:\n            # Convert \"October 2024\" to datetime\n            return datetime.strptime(date_str, \"%B %Y\")\n        except:\n            pass\n    return None\n\ndef parse_ndis_snapshot(html_file):\n    \"\"\"Parse a single NDIS snapshot file\"\"\"\n    timestamp = html_file.stem\n    year = int(timestamp[:4])\n    \n    html_content = html_file.read_text('utf-8', errors='ignore')\n    soup = BeautifulSoup(html_content, 'lxml')\n    text = soup.get_text(' ', strip=True)\n    \n    # Extract the \"as of\" date\n    data_date = extract_data_date(html_content)\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    records = []\n    \n    # Pattern for 2010 (no arrestee data)\n    if year &lt;= 2010:\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Samples\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': '0',\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    else:\n        # Pattern for 2011+ (includes arrestee data)\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Arrestee\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Profiles\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, arrestee, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': arrestee.replace(',', ''),\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    \n    return records"
  },
  {
    "objectID": "ndis_analysis.html#process-all-snapshots",
    "href": "ndis_analysis.html#process-all-snapshots",
    "title": "NDIS Database Analysis",
    "section": "Process All Snapshots",
    "text": "Process All Snapshots\n\ndef process_all_snapshots():\n    \"\"\"Parse all downloaded snapshots and create datasets\"\"\"\n    print(\"Processing all snapshots...\")\n    \n    all_records = []\n    html_files = sorted(HTML_DIR.glob(\"*.html\"))\n    \n    for html_file in tqdm(html_files, desc=\"Parsing HTML files\"):\n        try:\n            records = parse_ndis_snapshot(html_file)\n            all_records.extend(records)\n        except Exception as e:\n            print(f\"Error parsing {html_file.name}: {e}\")\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(all_records)\n    \n    # Convert numeric fields\n    numeric_fields = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n    for field in numeric_fields:\n        df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0).astype(int)\n    \n    # Add datetime columns\n    df['capture_datetime'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S')\n    df['capture_date'] = df['capture_datetime'].dt.date\n    \n    # Sort by timestamp and jurisdiction\n    df = df.sort_values(['timestamp', 'jurisdiction'])\n    \n    return df\n\n# Process all files\ndf_raw = process_all_snapshots()\nprint(f\"\\nProcessed {len(df_raw)} total records\")\nprint(f\"Unique jurisdictions: {df_raw['jurisdiction'].nunique()}\")\nprint(f\"Date range: {df_raw['capture_datetime'].min()} to {df_raw['capture_datetime'].max()}\")\n\nProcessing all snapshots...\n\n\n\n\n\n\nProcessed 16118 total records\nUnique jurisdictions: 54\nDate range: 2010-10-14 04:38:19 to 2025-06-29 17:15:50"
  },
  {
    "objectID": "ndis_analysis.html#save-raw-data",
    "href": "ndis_analysis.html#save-raw-data",
    "title": "NDIS Database Analysis",
    "section": "Save Raw Data",
    "text": "Save Raw Data\n\n# Save the raw data (with typos)\ndf_raw.to_csv(OUTPUT_DIR / 'ndis_data_raw.csv', index=False)\nprint(f\"Saved raw data: {len(df_raw)} records\")\n\n# Show a sample\ndf_raw.head()\n\nSaved raw data: 16118 records\n\n\n\n\n\n\n\n\n\ntimestamp\njurisdiction\noffender_profiles\narrestee\nforensic_profiles\nndis_labs\ninvestigations_aided\ndata_as_of_date\ncapture_datetime\ncapture_date\n\n\n\n\n0\n20101014043819\nAlabama\n183916\n0\n6243\n4\n3338\nNaT\n2010-10-14 04:38:19\n2010-10-14\n\n\n1\n20101014043819\nAlaska\n20287\n0\n847\n1\n304\nNaT\n2010-10-14 04:38:19\n2010-10-14\n\n\n2\n20101014043819\nArizona\n178774\n0\n9424\n7\n3632\nNaT\n2010-10-14 04:38:19\n2010-10-14\n\n\n3\n20101014043819\nArkansas\n108969\n0\n3955\n1\n1233\nNaT\n2010-10-14 04:38:19\n2010-10-14\n\n\n4\n20101014043819\nCalifornia\n1295199\n0\n28456\n20\n12777\nNaT\n2010-10-14 04:38:19\n2010-10-14"
  },
  {
    "objectID": "ndis_analysis.html#fix-known-typos",
    "href": "ndis_analysis.html#fix-known-typos",
    "title": "NDIS Database Analysis",
    "section": "Fix Known Typos",
    "text": "Fix Known Typos\n\n# Create fixed version\ndf_fixed = df_raw.copy()\n\n# Apply typo fixes\nfor typo in KNOWN_TYPOS:\n    mask = (\n        (df_fixed['timestamp'] == typo['timestamp']) & \n        (df_fixed['jurisdiction'] == typo['jurisdiction'])\n    )\n    if mask.any():\n        df_fixed.loc[mask, typo['field']] = int(typo['correct_value'])\n        print(f\"Fixed typo: {typo['jurisdiction']} on {typo['timestamp'][:8]} - \"\n              f\"{typo['field']} from {typo['wrong_value']} to {typo['correct_value']}\")\n\n# Save fixed data\ndf_fixed.to_csv(OUTPUT_DIR / 'ndis_data_fixed.csv', index=False)\nprint(f\"\\nSaved fixed data: {len(df_fixed)} records\")\n\nFixed typo: California on 20250105 - investigations_aided from 1304657 to 130465\nFixed typo: California on 20250116 - investigations_aided from 1304657 to 130465\n\nSaved fixed data: 16118 records"
  },
  {
    "objectID": "ndis_analysis.html#verify-the-fix",
    "href": "ndis_analysis.html#verify-the-fix",
    "title": "NDIS Database Analysis",
    "section": "Verify the Fix",
    "text": "Verify the Fix\nLet’s check that we correctly fixed the California typo:\n\n# Compare California data before and after fix\ncal_timestamps = ['20250105164014', '20250116205311']\n\nprint(\"California investigations aided:\")\nprint(\"\\nRaw data:\")\nfor ts in cal_timestamps:\n    raw_val = df_raw[(df_raw['timestamp'] == ts) & (df_raw['jurisdiction'] == 'California')]['investigations_aided'].values\n    if len(raw_val) &gt; 0:\n        print(f\"  {ts}: {raw_val[0]:,}\")\n\nprint(\"\\nFixed data:\")\nfor ts in cal_timestamps:\n    fixed_val = df_fixed[(df_fixed['timestamp'] == ts) & (df_fixed['jurisdiction'] == 'California')]['investigations_aided'].values\n    if len(fixed_val) &gt; 0:\n        print(f\"  {ts}: {fixed_val[0]:,}\")\n\nCalifornia investigations aided:\n\nRaw data:\n  20250105164014: 1,304,657\n  20250116205311: 1,304,657\n\nFixed data:\n  20250105164014: 130,465\n  20250116205311: 130,465"
  },
  {
    "objectID": "ndis_analysis.html#summary-statistics",
    "href": "ndis_analysis.html#summary-statistics",
    "title": "NDIS Database Analysis",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n# Calculate summary statistics\nlatest_data = df_fixed[df_fixed['capture_datetime'] == df_fixed['capture_datetime'].max()]\nlatest_data = latest_data[latest_data['jurisdiction'] != 'D.C./Metro PD']\n\nprint(\"\\nLatest Statistics Summary:\")\nprint(f\"  As of: {latest_data['capture_datetime'].iloc[0]}\")\nprint(f\"  Data from: {latest_data['data_as_of_date'].iloc[0] if latest_data['data_as_of_date'].iloc[0] else 'Unknown'}\")\nprint(f\"  Jurisdictions reporting: {len(latest_data)}\")\nprint(f\"  Total offender profiles: {latest_data['offender_profiles'].sum():,}\")\nprint(f\"  Total arrestee profiles: {latest_data['arrestee'].sum():,}\")\nprint(f\"  Total forensic profiles: {latest_data['forensic_profiles'].sum():,}\")\nprint(f\"  Total investigations aided: {latest_data['investigations_aided'].sum():,}\")\n\n\nLatest Statistics Summary:\n  As of: 2025-06-29 17:15:50\n  Data from: 2025-04-01 00:00:00\n  Jurisdictions reporting: 53\n  Total offender profiles: 18,431,162\n  Total arrestee profiles: 5,879,537\n  Total forensic profiles: 1,405,917\n  Total investigations aided: 730,426"
  },
  {
    "objectID": "ndis_analysis.html#quick-visualization",
    "href": "ndis_analysis.html#quick-visualization",
    "title": "NDIS Database Analysis",
    "section": "Quick Visualization",
    "text": "Quick Visualization\n\n# Plot total offender profiles over time\ntotals_by_date = df_fixed.groupby('capture_datetime')['offender_profiles'].sum()\n\nplt.figure(figsize=(10, 6))\nplt.plot(totals_by_date.index, totals_by_date.values / 1e6)\nplt.title('Growth of NDIS Offender Profiles Over Time')\nplt.xlabel('Year')\nplt.ylabel('Millions of Profiles')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ndis_analysis.html#data-files-created",
    "href": "ndis_analysis.html#data-files-created",
    "title": "NDIS Database Analysis",
    "section": "Data Files Created",
    "text": "Data Files Created\nThis analysis creates two CSV files in the output/ directory:\n\nndis_data_raw.csv - Original parsed data including typos\nndis_data_fixed.csv - Corrected data with typos fixed\n\nBoth files contain the following columns: - timestamp: Wayback Machine capture timestamp - jurisdiction: State/territory name (standardized) - offender_profiles: Number of offender DNA profiles - arrestee: Number of arrestee DNA profiles - forensic_profiles: Number of forensic DNA profiles - ndis_labs: Number of participating labs - investigations_aided: Number of investigations aided - data_as_of_date: The date the FBI data is from - capture_datetime: Parsed capture date - capture_date: Date only version ```"
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/numpy/random/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "Copyright (c) 2012-2023, Michael L. Waskom All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2025 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/pyzmq-27.0.0.dist-info/licenses/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/pyzmq-27.0.0.dist-info/licenses/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/httpcore-1.0.9.dist-info/licenses/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/httpcore-1.0.9.dist-info/licenses/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "ndis_analysis.html#setup-and-configuration",
    "href": "ndis_analysis.html#setup-and-configuration",
    "title": "NDIS Database Analysis",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\n\nInstalling beautifulsoup4...\nRequirement already satisfied: beautifulsoup4 in ./podfridge-db-env/lib/python3.13/site-packages (4.13.4)\nRequirement already satisfied: soupsieve&gt;1.2 in ./podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in ./podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (4.14.1)\n\n\n\nfrom pathlib import Path\nimport re, json, requests, time\nfrom datetime import datetime\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Configuration\nBASE_DIR = Path(\".\")  # Current directory\nROOT = BASE_DIR\nHTML_DIR = ROOT / \"output\" / \"wayback_html\"\nHTML_DIR.mkdir(parents=True, exist_ok=True)\nCSV_PATH = ROOT / \"output\" / \"ndis_state_metrics.csv\"\n\n# Jurisdiction name standardization mapping\nJURISDICTION_NAME_MAP = {\n    'D.C./FBI Lab': 'DC/FBI Lab',\n    'US Army': 'U.S. Army'\n}\n\n# Known data typos to fix\nKNOWN_TYPOS = [\n    {\n        'timestamp': '20250105164014',\n        'jurisdiction': 'California', \n        'field': 'investigations_aided',\n        'wrong_value': '1304657',  # How it parses\n        'correct_value': '130465'   # What it should be\n    },\n    {\n        'timestamp': '20250116205311',\n        'jurisdiction': 'California',\n        'field': 'investigations_aided', \n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    }\n]\n\nprint(f\"HTML directory: {HTML_DIR}\")\n\nHTML directory: output/wayback_html"
  },
  {
    "objectID": "ndis_analysis.html#wayback-machine-functions",
    "href": "ndis_analysis.html#wayback-machine-functions",
    "title": "NDIS Database Analysis",
    "section": "Wayback Machine Functions",
    "text": "Wayback Machine Functions\n\ndef make_request_with_retry(params, max_retries=3, initial_delay=5):\n    \"\"\"Make a request with exponential backoff retry logic\"\"\"\n    base = \"https://web.archive.org/cdx/search/cdx\"\n    \n    for attempt in range(max_retries):\n        try:\n            r = requests.get(base, params=params, timeout=30)\n            if r.status_code == 200:\n                return r\n            elif r.status_code == 429:  # Rate limited\n                wait_time = initial_delay * (2 ** attempt)\n                print(f\"    Rate limited. Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                return r\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except Exception as e:\n            print(f\"    Unexpected error: {e}\")\n            return None\n    return None"
  },
  {
    "objectID": "ndis_analysis.html#search-for-all-ndis-snapshots",
    "href": "ndis_analysis.html#search-for-all-ndis-snapshots",
    "title": "NDIS Database Analysis",
    "section": "Search for All NDIS Snapshots",
    "text": "Search for All NDIS Snapshots\n\n\nShow search function code\ndef search_all_ndis_snapshots():\n    \"\"\"Search for NDIS snapshots across all known URL variations\"\"\"\n    \n    # Search for both http and https variants\n    protocols = [\"http://\", \"https://\"]\n    subdomains = [\"www\", \"le\", \"*\"]  # Known subdomains plus wildcard\n    \n    all_rows = []\n    seen_timestamps = set()\n    \n    # First, try broad searches with protocol wildcards\n    print(\"Starting wildcard searches...\")\n    for protocol in protocols:\n        for subdomain in subdomains:\n            pattern = f\"{protocol}{subdomain}.fbi.gov/*ndis-statistics*\"\n            print(f\"\\nSearching: {pattern}\")\n            \n            params = {\n                \"url\":         pattern,\n                \"matchType\":   \"wildcard\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) &gt; 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests to avoid rate limiting\n            time.sleep(2)\n    \n    # Also search your specific known URLs with both protocols\n    known_paths = [\n        \"www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n        \"www.fbi.gov/about-us/laboratory/biometric-analysis/codis/ndis-statistics\", \n        \"www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n        \"le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\",\n    ]\n    \n    print(\"\\n\\nStarting exact URL searches...\")\n    for path in known_paths:\n        for protocol in protocols:\n            url = f\"{protocol}{path}\"\n            print(f\"\\nSearching: {url}\")\n            \n            params = {\n                \"url\":         url,\n                \"matchType\":   \"exact\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) &gt; 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests\n            time.sleep(2)\n    \n    # Create DataFrame\n    snap_df = (pd.DataFrame(\n                    all_rows,\n                    columns=[\"timestamp\", \"original\", \"mimetype\", \"status\"])\n               .sort_values(\"timestamp\")\n               .reset_index(drop=True))\n    \n    return snap_df\n\n# Check if we already have snapshot data or need to search\nsnapshot_csv = HTML_DIR.parent / 'snapshots_found.csv'\nif snapshot_csv.exists():\n    print(\"Loading existing snapshot list...\")\n    snap_df = pd.read_csv(snapshot_csv)\n    print(f\"Loaded {len(snap_df)} snapshots\")\nelse:\n    print(\"Searching for all NDIS snapshots...\")\n    snap_df = search_all_ndis_snapshots()\n    if len(snap_df) &gt; 0:\n        snap_df.to_csv(snapshot_csv, index=False)\n        print(f\"\\nSaved {len(snap_df)} snapshots to {snapshot_csv}\")\n\nif len(snap_df) &gt; 0:\n    print(f\"\\nTotal unique snapshots found: {len(snap_df):,}\")\n    print(f\"Unique URLs found: {snap_df['original'].nunique()}\")\n    print(\"\\nUnique URL patterns found:\")\n    for url in sorted(snap_df['original'].unique()):\n        print(f\"  {url}\")\n\n\nLoading existing snapshot list...\nLoaded 317 snapshots\n\nTotal unique snapshots found: 317\nUnique URLs found: 8\n\nUnique URL patterns found:\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics/\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics/\n  https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics/\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics//"
  },
  {
    "objectID": "ndis_analysis.html#download-functions",
    "href": "ndis_analysis.html#download-functions",
    "title": "NDIS Database Analysis",
    "section": "Download Functions",
    "text": "Download Functions\n\ndef download_with_retry(url, max_retries=3, initial_delay=5, consecutive_failures=0):\n    \"\"\"Download with adaptive retry logic based on consecutive failures\"\"\"\n    if consecutive_failures &gt; 0:\n        extra_wait = consecutive_failures * 10\n        print(f\"\\n    Adding {extra_wait}s cooldown due to {consecutive_failures} consecutive failures...\")\n        time.sleep(extra_wait)\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            return response, True\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"\\n    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:\n                wait_time = initial_delay * (2 ** attempt) * 2\n                print(f\"\\n    Rate limited (429). Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"\\n    HTTP Error: {e}\")\n                return None, False\n        except Exception as e:\n            print(f\"\\n    Unexpected error: {e}\")\n            return None, False\n    return None, False\n\ndef download_missing_snapshots(snap_df, output_folder):\n    \"\"\"Download HTML snapshots with resume capability\"\"\"\n    \n    # Check what we already have\n    existing_files = list(output_folder.glob(\"*.html\"))\n    existing_timestamps = {f.stem for f in existing_files}\n    print(f\"\\nFiles already downloaded: {len(existing_files)}\")\n    \n    # Check what needs to be downloaded\n    to_download = []\n    for _, row in snap_df.iterrows():\n        timestamp = row['timestamp']\n        url = row['original']\n        filename = output_folder / f\"{timestamp}.html\"\n        \n        if timestamp not in existing_timestamps and not filename.exists():\n            to_download.append((timestamp, url, filename))\n    \n    print(f\"Files to download: {len(to_download)}\")\n    \n    if len(to_download) == 0:\n        print(\"\\n✓ All files already downloaded! Nothing to do.\")\n        return\n    \n    # Download configuration\n    BATCH_SIZE = 15\n    PAUSE_BETWEEN_DOWNLOADS = 3\n    PAUSE_BETWEEN_BATCHES = 45\n    PAUSE_AFTER_FAILURE = 60\n    \n    # Track statistics\n    successful_downloads = 0\n    failed_downloads = []\n    consecutive_failures = 0\n    \n    # Download in batches\n    for i in range(0, len(to_download), BATCH_SIZE):\n        batch = to_download[i:i + BATCH_SIZE]\n        batch_num = (i // BATCH_SIZE) + 1\n        total_batches = (len(to_download) + BATCH_SIZE - 1) // BATCH_SIZE\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Batch {batch_num}/{total_batches} ({len(batch)} files)\")\n        print(f\"Overall progress: {len(existing_timestamps) + successful_downloads}/{len(snap_df)} total files\")\n        print(f\"{'='*60}\")\n        \n        for j, (timestamp, url, filename) in enumerate(batch, 1):\n            # Double-check file doesn't exist\n            if filename.exists():\n                print(f\"\\n[{j}/{len(batch)}] {timestamp} - Already exists, skipping...\")\n                continue\n                \n            wayback_url = f\"https://web.archive.org/web/{timestamp}/{url}\"\n            \n            print(f\"\\n[{j}/{len(batch)}] Downloading {timestamp}...\", end=\"\")\n            \n            response, success = download_with_retry(wayback_url, consecutive_failures=consecutive_failures)\n            \n            if response and response.status_code == 200:\n                try:\n                    with open(filename, 'w', encoding='utf-8') as f:\n                        f.write(response.text)\n                    \n                    print(\" ✓ Success\")\n                    successful_downloads += 1\n                    consecutive_failures = 0\n                    \n                except Exception as e:\n                    print(f\" ✗ Error saving file: {e}\")\n                    failed_downloads.append((timestamp, url, str(e)))\n                    consecutive_failures += 1\n            else:\n                print(\" ✗ Failed after retries\")\n                failed_downloads.append((timestamp, url, \"Download failed\"))\n                consecutive_failures += 1\n                \n                if j &lt; len(batch):\n                    print(f\"    Taking {PAUSE_AFTER_FAILURE}s break after failure...\")\n                    time.sleep(PAUSE_AFTER_FAILURE)\n                    continue\n            \n            if j &lt; len(batch) and consecutive_failures == 0:\n                print(f\"    Waiting {PAUSE_BETWEEN_DOWNLOADS} seconds...\")\n                time.sleep(PAUSE_BETWEEN_DOWNLOADS)\n        \n        if i + BATCH_SIZE &lt; len(to_download):\n            print(f\"\\nBatch complete. Pausing {PAUSE_BETWEEN_BATCHES} seconds...\")\n            print(f\"This session: {successful_downloads} downloaded, {len(failed_downloads)} failed\")\n            time.sleep(PAUSE_BETWEEN_BATCHES)\n    \n    # Final summary\n    print(f\"\\n{'='*60}\")\n    print(f\"Download session complete!\")\n    print(f\"  Successfully downloaded: {successful_downloads}\")\n    print(f\"  Failed downloads: {len(failed_downloads)}\")\n    \n    if failed_downloads:\n        print(f\"\\nFailed downloads:\")\n        for timestamp, url, error in failed_downloads[:10]:\n            print(f\"  {timestamp}: {error}\")\n        if len(failed_downloads) &gt; 10:\n            print(f\"  ... and {len(failed_downloads) - 10} more\")\n\n# Download missing files\nif len(snap_df) &gt; 0:\n    download_missing_snapshots(snap_df, HTML_DIR)\n\n\nFiles already downloaded: 317\nFiles to download: 0\n\n✓ All files already downloaded! Nothing to do."
  },
  {
    "objectID": "ndis_analysis.html#apply-typo-fixes",
    "href": "ndis_analysis.html#apply-typo-fixes",
    "title": "NDIS Database Analysis",
    "section": "Apply Typo Fixes",
    "text": "Apply Typo Fixes\n\ndef apply_typo_fixes(df):\n    \"\"\"Apply known typo corrections\"\"\"\n    df_fixed = df.copy()\n    \n    for typo in KNOWN_TYPOS:\n        mask = (\n            (df_fixed['timestamp'] == typo['timestamp']) & \n            (df_fixed['jurisdiction'] == typo['jurisdiction'])\n        )\n        if mask.any():\n            df_fixed.loc[mask, typo['field']] = int(typo['correct_value'])\n            print(f\"Fixed typo: {typo['jurisdiction']} on {typo['timestamp'][:8]} - \"\n                  f\"{typo['field']} from {typo['wrong_value']} to {typo['correct_value']}\")\n    \n    return df_fixed\n\n# Apply fixes\ndf_fixed = apply_typo_fixes(df_raw)\n\nFixed typo: California on 20250105 - investigations_aided from 1304657 to 130465\nFixed typo: California on 20250116 - investigations_aided from 1304657 to 130465"
  },
  {
    "objectID": "ndis_analysis.html#save-datasets",
    "href": "ndis_analysis.html#save-datasets",
    "title": "NDIS Database Analysis",
    "section": "Save Datasets",
    "text": "Save Datasets\n\n# Save datasets\nOUTPUT_DIR = HTML_DIR.parent\ndf_raw.to_csv(OUTPUT_DIR / 'ndis_data_raw.csv', index=False)\ndf_fixed.to_csv(OUTPUT_DIR / 'ndis_data_fixed.csv', index=False)\nprint(f\"\\nSaved raw data to: {OUTPUT_DIR / 'ndis_data_raw.csv'}\")\nprint(f\"Saved fixed data to: {OUTPUT_DIR / 'ndis_data_fixed.csv'}\")\n\n\nSaved raw data to: output/ndis_data_raw.csv\nSaved fixed data to: output/ndis_data_fixed.csv"
  },
  {
    "objectID": "ndis_analysis.html#visualizations",
    "href": "ndis_analysis.html#visualizations",
    "title": "NDIS Database Analysis",
    "section": "Visualizations",
    "text": "Visualizations\n\ndef create_visualizations(df_raw, df_fixed):\n    \"\"\"Create comprehensive visualizations\"\"\"\n    # Exclude D.C./Metro PD from visualizations\n    df_raw_viz = df_raw[df_raw['jurisdiction'] != 'D.C./Metro PD']\n    df_fixed_viz = df_fixed[df_fixed['jurisdiction'] != 'D.C./Metro PD']\n    \n    # Create figure with subplots\n    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n    \n    # 1. Jurisdictions reporting over time\n    for ax, (df, title_suffix) in zip([axes[0,0], axes[0,1]], \n                                      [(df_raw_viz, 'Raw'), (df_fixed_viz, 'Fixed')]):\n        jurisdictions_per_date = df.groupby('capture_datetime')['jurisdiction'].nunique()\n        ax.plot(jurisdictions_per_date.index, jurisdictions_per_date.values, 'b-', linewidth=2)\n        ax.set_title(f'Jurisdictions Reporting ({title_suffix})')\n        ax.set_ylabel('Number of Jurisdictions')\n        ax.grid(True, alpha=0.3)\n        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n    \n    # 2. Total investigations aided\n    for ax, (df, title_suffix) in zip([axes[1,0], axes[1,1]], \n                                      [(df_raw_viz, 'Raw with Typo'), (df_fixed_viz, 'Fixed')]):\n        total_inv = df.groupby('capture_datetime')['investigations_aided'].sum()\n        ax.plot(total_inv.index, total_inv.values / 1e3, 'purple', linewidth=2)\n        ax.set_title(f'Total Investigations Aided ({title_suffix})')\n        ax.set_ylabel('Thousands of Investigations')\n        ax.grid(True, alpha=0.3)\n        \n        # Highlight the typo in raw data\n        if 'Raw' in title_suffix:\n            typo_dates = df[(df['jurisdiction'] == 'California') & \n                          (df['investigations_aided'] &gt; 1000000)]['capture_datetime']\n            for date in typo_dates:\n                ax.axvline(x=date, color='red', linestyle='--', alpha=0.5)\n                ax.text(date, ax.get_ylim()[1]*0.9, 'Typo', rotation=90, \n                       verticalalignment='bottom', color='red')\n    \n    # 3. Data lag analysis\n    ax = axes[2, 0]\n    df_with_lag = df_fixed_viz[df_fixed_viz['data_as_of_date'].notna()].copy()\n    df_with_lag['data_lag_days'] = (df_with_lag['capture_datetime'] - df_with_lag['data_as_of_date']).dt.days\n    \n    avg_lag = df_with_lag.groupby('capture_datetime')['data_lag_days'].mean()\n    ax.plot(avg_lag.index, avg_lag.values, 'orange', linewidth=2)\n    ax.set_title('Average Data Lag (Capture Date vs \"As Of\" Date)')\n    ax.set_ylabel('Days')\n    ax.grid(True, alpha=0.3)\n    \n    # 4. California investigations over time (showing typo fix)\n    ax = axes[2, 1]\n    cal_raw = df_raw_viz[df_raw_viz['jurisdiction'] == 'California']\n    cal_fixed = df_fixed_viz[df_fixed_viz['jurisdiction'] == 'California']\n    \n    ax.plot(cal_raw['capture_datetime'], cal_raw['investigations_aided'], \n            'r-', label='Raw (with typo)', linewidth=2, alpha=0.7)\n    ax.plot(cal_fixed['capture_datetime'], cal_fixed['investigations_aided'], \n            'g-', label='Fixed', linewidth=2)\n    ax.set_title('California Investigations Aided: Raw vs Fixed')\n    ax.set_ylabel('Investigations Aided')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / 'ndis_analysis_complete.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Create visualizations\nprint(\"\\nCreating visualizations...\")\ncreate_visualizations(df_raw, df_fixed)\nprint(\"\\nProcessing complete!\")\n\n\nCreating visualizations...\n\n\n\n\n\n\n\n\n\n\nProcessing complete!"
  },
  {
    "objectID": "analysis/foia_processing.html",
    "href": "analysis/foia_processing.html",
    "title": "FOIA Document OCR Processing",
    "section": "",
    "text": "This document details the processing of Freedom of Information Act (FOIA) responses from seven U.S. states regarding the demographic composition of their State DNA Index System (SDIS) databases. These responses were obtained by Professor Erin Murphy (NYU Law) in 2018 as part of research on racial disparities in DNA databases."
  },
  {
    "objectID": "analysis/foia_processing.html#overview",
    "href": "analysis/foia_processing.html#overview",
    "title": "FOIA Document OCR Processing",
    "section": "",
    "text": "This document details the processing of Freedom of Information Act (FOIA) responses from seven U.S. states regarding the demographic composition of their State DNA Index System (SDIS) databases. These responses were obtained by Professor Erin Murphy (NYU Law) in 2018 as part of research on racial disparities in DNA databases."
  },
  {
    "objectID": "analysis/foia_processing.html#data-sources",
    "href": "analysis/foia_processing.html#data-sources",
    "title": "FOIA Document OCR Processing",
    "section": "Data Sources",
    "text": "Data Sources\n\nRaw FOIA Responses\nThe original FOIA responses are stored in two formats:\n\nPDFs: raw/foia_pdfs/ - Original scanned documents\nHTML: raw/foia_html/ - OCR’d versions for easier extraction\n\nStates included:\n\nCalifornia\nFlorida\n\nIndiana\nMaine\nNevada\nSouth Dakota\nTexas"
  },
  {
    "objectID": "analysis/foia_processing.html#file-structure-and-contents",
    "href": "analysis/foia_processing.html#file-structure-and-contents",
    "title": "FOIA Document OCR Processing",
    "section": "File Structure and Contents",
    "text": "File Structure and Contents\n\n1. Raw Data File: foia_raw.csv\nPurpose: Contains exactly what was reported in each FOIA response with zero calculations or modifications.\nStructure: Long format with columns:\n\nstate: State name\noffender_type: Category of individuals (Convicted Offender, Arrestee, Combined, etc.)\nvariable_category: Type of data (total, gender, race, gender_race)\nvariable_detailed: Specific value (e.g., Male, Female, African American)\nvalue: The reported number or percentage\nvalue_type: Whether value is a “count” or “percentage”\ndate: Date of data snapshot\n\n\n\n2. State-Specific Files: per_state/[state]_foia_data.csv\nPurpose: Individual files for each state containing only their reported data.\n\n\n3. Metadata File: state_reporting_metadata.csv\nPurpose: Documents what each state provided and their reporting quirks.\n\n\n4. Data Dictionary: foia_data_dictionary.csv\nPurpose: Defines all column names and their possible values."
  },
  {
    "objectID": "analysis/foia_processing.html#data-processing-steps",
    "href": "analysis/foia_processing.html#data-processing-steps",
    "title": "FOIA Document OCR Processing",
    "section": "Data Processing Steps",
    "text": "Data Processing Steps\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom IPython.display import display, Markdown, HTML\n\n# Set display options for better formatting\npd.options.display.max_columns = None\npd.options.display.width = None\npd.options.display.max_colwidth = None\n\n# Set up paths - assuming we're running from analysis/ directory\nbase_dir = Path(\"..\") \noutput_dir = base_dir / \"output\" / \"foia\"\n\n# Load the raw FOIA data\nfoia_raw = pd.read_csv(output_dir / \"foia_raw.csv\")\nmetadata = pd.read_csv(output_dir / \"state_reporting_metadata.csv\")\n\nprint(f\"Loaded {len(foia_raw)} rows of FOIA data from {foia_raw['state'].nunique()} states\")\nprint(\"\\nStates included:\", ', '.join(foia_raw['state'].unique()))\n\n# Check for any non-numeric values in 'value' column\nnon_numeric = foia_raw[pd.to_numeric(foia_raw['value'], errors='coerce').isna()]['value'].unique()\nif len(non_numeric) &gt; 0:\n    print(f\"\\nNote: Found non-numeric value(s): {non_numeric}\")\n\n# Convert value column, handling special cases\nfoia_raw['value_numeric'] = foia_raw['value'].apply(\n    lambda x: 0.5 if str(x) == '&lt;1' else pd.to_numeric(x, errors='coerce')\n)\n\n# Show metadata summary\ndisplay(Markdown(\"### State Reporting Summary\"))\ndisplay(HTML(metadata[['state', 'separates_offender_types', 'reports_counts', 'reports_percentages']].to_html(index=False)))\n\nLoaded 143 rows of FOIA data from 7 states\n\nStates included: California, Florida, Indiana, Maine, Nevada, South Dakota, Texas\n\nNote: Found non-numeric value(s): ['&lt;1']\n\n\n\nState Reporting Summary\n\n\n\n\n\n\nstate\nseparates_offender_types\nreports_counts\nreports_percentages\n\n\n\n\nCalifornia\nYes\nYes\nNo\n\n\nFlorida\nNo\nYes\nYes\n\n\nIndiana\nTotals only\nTotals only\nDemographics only\n\n\nMaine\nNo\nYes\nYes\n\n\nNevada\nYes\nYes\nSome percentages\n\n\nSouth Dakota\nNo\nYes\nYes\n\n\nTexas\nYes\nYes (partial)\nNo\n\n\n\n\n\n\nStep 1: States with Both Counts AND Percentages\nThese states provided both counts and percentages, allowing us to verify their calculations.\n\nFlorida: Complete Count and Percentage Data\n\n# Florida provides both counts and percentages\nflorida_data = foia_raw[foia_raw['state'] == 'Florida'].copy()\n\n# Pivot to get counts and percentages side by side\nflorida_wide = florida_data.pivot_table(\n    index=['offender_type', 'variable_category', 'variable_detailed'],\n    columns='value_type',\n    values='value_numeric',\n    aggfunc='first'\n).reset_index()\n\n# Calculate percentages from counts and compare\nif 'count' in florida_wide.columns and 'percentage' in florida_wide.columns:\n    florida_check = florida_wide[florida_wide['count'].notna()].copy()\n    total_profiles = 1175391\n\n    florida_check['calc_percentage'] = np.where(\n        florida_check['variable_detailed'] == 'total_profiles',\n        100.0,\n        round(florida_check['count'] / total_profiles * 100, 2)\n    )\n    florida_check['diff'] = florida_check['calc_percentage'] - florida_check['percentage']\n\n    display(Markdown(\"**Florida: Comparing reported vs calculated percentages**\"))\n    \n    # Create summary table\n    summary_df = florida_check[['variable_category', 'variable_detailed', 'count', \n                               'percentage', 'calc_percentage', 'diff']]\n    summary_df.columns = ['Category', 'Detail', 'Count', 'Reported %', 'Calculated %', 'Difference']\n    \n    # Format numbers nicely\n    summary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\n    summary_df['Reported %'] = summary_df['Reported %'].apply(lambda x: f\"{x:.2f}%\")\n    summary_df['Calculated %'] = summary_df['Calculated %'].apply(lambda x: f\"{x:.2f}%\")\n    summary_df['Difference'] = summary_df['Difference'].apply(lambda x: f\"{x:+.2f}\" if pd.notna(x) else \"\")\n    \n    display(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n    # Check if percentages sum to 100\n    florida_pct_sum = florida_check.groupby('variable_category')['percentage'].sum()\n    display(Markdown(\"\\n**Florida percentage totals by category:**\"))\n    totals_df = pd.DataFrame({\n        'Category': florida_pct_sum[florida_pct_sum.index != 'total'].index,\n        'Total %': [f\"{x:.2f}%\" for x in florida_pct_sum[florida_pct_sum.index != 'total'].values]\n    })\n    display(HTML(totals_df.to_html(index=False, classes='table table-striped')))\n\nFlorida: Comparing reported vs calculated percentages\n\n\n\n\n\nCategory\nDetail\nCount\nReported %\nCalculated %\nDifference\n\n\n\n\ngender\nFemale\n260,885\n22.20%\n22.20%\n+0.00\n\n\ngender\nMale\n901,126\n76.67%\n76.67%\n+0.00\n\n\ngender\nUnknown\n13,380\n1.14%\n1.14%\n+0.00\n\n\nrace\nAfrican American\n413,733\n35.20%\n35.20%\n+0.00\n\n\nrace\nAsian\n2,659\n0.23%\n0.23%\n+0.00\n\n\nrace\nCaucasian\n721,485\n61.38%\n61.38%\n+0.00\n\n\nrace\nHispanic\n28,452\n2.42%\n2.42%\n+0.00\n\n\nrace\nNative American\n667\n0.06%\n0.06%\n+0.00\n\n\nrace\nOther\n1,176\n0.10%\n0.10%\n+0.00\n\n\nrace\nUnknown\n7,219\n0.61%\n0.61%\n+0.00\n\n\ntotal\ntotal_profiles\n1,175,391\n100.00%\n100.00%\n+0.00\n\n\n\n\n\nFlorida percentage totals by category:\n\n\n\n\n\nCategory\nTotal %\n\n\n\n\ngender\n100.01%\n\n\nrace\n100.00%\n\n\n\n\n\n\n\nMaine: Count and Percentage Verification\n\n# Maine provides both counts and percentages\nmaine_data = foia_raw[foia_raw['state'] == 'Maine'].copy()\n\nmaine_wide = maine_data.pivot_table(\n    index=['offender_type', 'variable_category', 'variable_detailed'],\n    columns='value_type',\n    values='value_numeric',\n    aggfunc='first'\n).reset_index()\n\nif 'count' in maine_wide.columns and 'percentage' in maine_wide.columns:\n    maine_check = maine_wide[maine_wide['count'].notna()].copy()\n    total_maine = 33711\n\n    maine_check['calc_percentage'] = round(maine_check['count'] / total_maine * 100, 1)\n    maine_check['diff'] = maine_check['calc_percentage'] - maine_check['percentage']\n\n    display(Markdown(\"**Maine: Comparing reported vs calculated percentages**\"))\n    \n    summary_df = maine_check[maine_check['variable_category'] != 'total'][\n        ['variable_category', 'variable_detailed', 'count', 'percentage', 'calc_percentage', 'diff']\n    ].copy()\n    summary_df.columns = ['Category', 'Detail', 'Count', 'Reported %', 'Calculated %', 'Difference']\n    \n    # Format for display\n    summary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\n    summary_df['Reported %'] = summary_df['Reported %'].apply(lambda x: f\"{x:.1f}%\")\n    summary_df['Calculated %'] = summary_df['Calculated %'].apply(lambda x: f\"{x:.1f}%\")\n    summary_df['Difference'] = summary_df['Difference'].apply(lambda x: f\"{x:+.1f}\")\n    \n    display(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n    # Check sums\n    maine_pct_sum = maine_check.groupby('variable_category')['percentage'].sum()\n    display(Markdown(\"\\n**Maine percentage totals by category:**\"))\n    totals_df = pd.DataFrame({\n        'Category': maine_pct_sum[maine_pct_sum.index != 'total'].index,\n        'Total %': [f\"{x:.1f}%\" for x in maine_pct_sum[maine_pct_sum.index != 'total'].values]\n    })\n    display(HTML(totals_df.to_html(index=False, classes='table table-striped')))\n\nMaine: Comparing reported vs calculated percentages\n\n\n\n\n\nCategory\nDetail\nCount\nReported %\nCalculated %\nDifference\n\n\n\n\ngender\nFemale\n5,734\n17.0%\n17.0%\n+0.0\n\n\ngender\nMale\n27,694\n82.7%\n82.2%\n-0.5\n\n\ngender\nUnknown\n83\n0.2%\n0.2%\n+0.0\n\n\nrace\nAsian\n128\n0.4%\n0.4%\n+0.0\n\n\nrace\nBlack\n1,299\n3.9%\n3.9%\n+0.0\n\n\nrace\nHispanic\n171\n0.5%\n0.5%\n+0.0\n\n\nrace\nNative American\n345\n1.0%\n1.0%\n+0.0\n\n\nrace\nUnknown\n470\n1.4%\n1.4%\n+0.0\n\n\nrace\nWhite\n31,298\n92.8%\n92.8%\n+0.0\n\n\n\n\n\nMaine percentage totals by category:\n\n\n\n\n\nCategory\nTotal %\n\n\n\n\ngender\n99.9%\n\n\nrace\n100.0%\n\n\n\n\n\n\n\nSouth Dakota: Count and Percentage Verification\n\n# South Dakota - check main categories (not intersections)\nsd_data = foia_raw[\n    (foia_raw['state'] == 'South Dakota') & \n    (foia_raw['variable_category'].isin(['total', 'gender', 'race']))\n].copy()\n\nsd_wide = sd_data.pivot_table(\n    index=['offender_type', 'variable_category', 'variable_detailed'],\n    columns='value_type',\n    values='value_numeric',\n    aggfunc='first'\n).reset_index()\n\nsd_check = sd_wide[sd_wide['count'].notna()].copy()\ntotal_sd = 67753\n\nsd_check['calc_percentage'] = np.where(\n    sd_check['variable_detailed'] == 'total_profiles',\n    100.0,\n    round(sd_check['count'] / total_sd * 100, 2)\n)\nsd_check['diff'] = sd_check['calc_percentage'] - sd_check['percentage']\n\ndisplay(Markdown(\"**South Dakota: Comparing reported vs calculated percentages**\"))\n\nsummary_df = sd_check[sd_check['variable_category'] != 'total'][\n    ['variable_category', 'variable_detailed', 'count', 'percentage', 'calc_percentage', 'diff']\n].copy()\nsummary_df.columns = ['Category', 'Detail', 'Count', 'Reported %', 'Calculated %', 'Difference']\n\n# Format for display\nsummary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\nsummary_df['Reported %'] = summary_df['Reported %'].apply(lambda x: f\"{x:.2f}%\")\nsummary_df['Calculated %'] = summary_df['Calculated %'].apply(lambda x: f\"{x:.2f}%\")\nsummary_df['Difference'] = summary_df['Difference'].apply(lambda x: f\"{x:+.2f}\")\n\ndisplay(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Check sums\nsd_pct_sum = sd_check.groupby('variable_category')['percentage'].sum()\ndisplay(Markdown(\"\\n**South Dakota percentage totals by category:**\"))\ntotals_df = pd.DataFrame({\n    'Category': sd_pct_sum[sd_pct_sum.index != 'total'].index,\n    'Total %': [f\"{x:.2f}%\" for x in sd_pct_sum[sd_pct_sum.index != 'total'].values]\n})\ndisplay(HTML(totals_df.to_html(index=False, classes='table table-striped')))\n\ndisplay(Markdown(\"\\n*Note: South Dakota uniquely provides race×gender intersection data (24 additional rows not shown here)*\"))\n\nSouth Dakota: Comparing reported vs calculated percentages\n\n\n\n\n\nCategory\nDetail\nCount\nReported %\nCalculated %\nDifference\n\n\n\n\ngender\nFemale\n16,556\n24.44%\n24.44%\n+0.00\n\n\ngender\nMale\n51,197\n75.56%\n75.56%\n+0.00\n\n\nrace\nAsian\n5\n0.08%\n0.01%\n-0.07\n\n\nrace\nBlack\n4,041\n5.96%\n5.96%\n+0.00\n\n\nrace\nHispanic\n2,949\n4.35%\n4.35%\n+0.00\n\n\nrace\nNative American\n14,593\n21.54%\n21.54%\n+0.00\n\n\nrace\nOther/Unknown\n891\n1.32%\n1.32%\n+0.00\n\n\nrace\nWhite/Caucasian\n45,223\n66.75%\n66.75%\n+0.00\n\n\n\n\n\nSouth Dakota percentage totals by category:\n\n\n\n\n\nCategory\nTotal %\n\n\n\n\ngender\n100.00%\n\n\nrace\n100.00%\n\n\n\n\n\nNote: South Dakota uniquely provides race×gender intersection data (24 additional rows not shown here)\n\n\n\n\n\nStep 2: States with Only Counts\nThese states need percentages calculated.\n\nCalifornia: Calculate Percentages\n\n# California only provided counts\ncalifornia_data = foia_raw[foia_raw['state'] == 'California'].copy()\n\n# Get totals for each offender type\nca_totals = california_data[california_data['variable_category'] == 'total'].set_index('offender_type')['value_numeric'].to_dict()\n\n# Calculate percentages\nca_with_pct = california_data[california_data['value_type'] == 'count'].copy()\n\ndef get_total(row):\n    if row['variable_category'] == 'total':\n        return row['value_numeric']\n    else:\n        return ca_totals.get(row['offender_type'], 0)\n\nca_with_pct['total'] = ca_with_pct.apply(get_total, axis=1)\nca_with_pct['percentage'] = round(ca_with_pct['value_numeric'] / ca_with_pct['total'] * 100, 2)\n\ndisplay(Markdown(\"**California: Calculated percentages**\"))\n\n# Show sample by offender type\nfor offender_type in ['Convicted Offender', 'Arrestee']:\n    subset = ca_with_pct[\n        (ca_with_pct['offender_type'] == offender_type) & \n        (ca_with_pct['variable_category'] != 'total')\n    ].copy()\n    \n    display(Markdown(f\"\\n*{offender_type}:*\"))\n    summary_df = subset[['variable_category', 'variable_detailed', 'value_numeric', 'percentage']]\n    summary_df.columns = ['Category', 'Detail', 'Count', 'Calculated %']\n    summary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\n    summary_df['Calculated %'] = summary_df['Calculated %'].apply(lambda x: f\"{x:.2f}%\")\n    display(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Check if percentages sum to 100%\nca_pct_check = ca_with_pct[ca_with_pct['variable_category'] != 'total'].groupby(\n    ['offender_type', 'variable_category']\n)['percentage'].sum().reset_index()\n\ndisplay(Markdown(\"\\n**California: Percentage totals by category**\"))\nca_pct_check.columns = ['Offender Type', 'Category', 'Total %']\nca_pct_check['Total %'] = ca_pct_check['Total %'].apply(lambda x: f\"{x:.2f}%\")\ndisplay(HTML(ca_pct_check.to_html(index=False, classes='table table-striped')))\n\ndisplay(Markdown(\"\\n⚠️ **Warning**: California race percentages don't sum to 100%, indicating missing categories\"))\n\nCalifornia: Calculated percentages\n\n\nConvicted Offender:\n\n\n\n\n\nCategory\nDetail\nCount\nCalculated %\n\n\n\n\ngender\nFemale\n309,827\n15.34%\n\n\ngender\nMale\n1,603,222\n79.37%\n\n\ngender\nUnknown\n106,850\n5.29%\n\n\nrace\nAfrican American\n368,952\n18.27%\n\n\nrace\nCaucasian\n588,555\n29.14%\n\n\nrace\nHispanic\n652,121\n32.28%\n\n\nrace\nAsian\n16,384\n0.81%\n\n\n\n\n\nArrestee:\n\n\n\n\n\nCategory\nDetail\nCount\nCalculated %\n\n\n\n\ngender\nFemale\n208,225\n27.70%\n\n\ngender\nMale\n524,231\n69.73%\n\n\ngender\nUnknown\n19,366\n2.58%\n\n\nrace\nAfrican American\n104,741\n13.93%\n\n\nrace\nCaucasian\n231,313\n30.77%\n\n\nrace\nHispanic\n308,450\n41.03%\n\n\nrace\nAsian\n11,191\n1.49%\n\n\n\n\n\nCalifornia: Percentage totals by category\n\n\n\n\n\nOffender Type\nCategory\nTotal %\n\n\n\n\nArrestee\ngender\n100.01%\n\n\nArrestee\nrace\n87.22%\n\n\nConvicted Offender\ngender\n100.00%\n\n\nConvicted Offender\nrace\n80.50%\n\n\n\n\n\n⚠️ Warning: California race percentages don’t sum to 100%, indicating missing categories\n\n\n\n\nTexas: Calculate Percentages and Identify Missing Data\n\n# Texas only provided counts (and only female counts for gender)\ntexas_data = foia_raw[foia_raw['state'] == 'Texas'].copy()\n\n# First, check if race counts sum to totals\ntexas_totals = texas_data[texas_data['variable_category'] == 'total'].set_index('offender_type')['value_numeric'].to_dict()\ntexas_race_sums = texas_data[texas_data['variable_category'] == 'race'].groupby('offender_type')['value_numeric'].sum()\n\ndisplay(Markdown(\"**Texas: Missing race data (Unknown category)**\"))\nmissing_data = []\nfor offender_type, total in texas_totals.items():\n    if offender_type in texas_race_sums.index:\n        race_sum = texas_race_sums[offender_type]\n        missing = total - race_sum\n        missing_data.append({\n            'Offender Type': offender_type,\n            'Total': f\"{int(total):,}\",\n            'Race Sum': f\"{int(race_sum):,}\",\n            'Missing': f\"{int(missing):,}\",\n            'Missing %': f\"{missing/total * 100:.2f}%\"\n        })\n\nmissing_df = pd.DataFrame(missing_data)\ndisplay(HTML(missing_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Calculate percentages for available data\ntexas_with_pct = texas_data[\n    (texas_data['value_type'] == 'count') & \n    (texas_data['variable_category'] != 'total')\n].copy()\n\ntexas_with_pct['total'] = texas_with_pct['offender_type'].map(texas_totals)\ntexas_with_pct['percentage'] = round(texas_with_pct['value_numeric'] / texas_with_pct['total'] * 100, 2)\n\ndisplay(Markdown(\"\\n**Texas: Gender data (only female counts provided)**\"))\ngender_df = texas_with_pct[texas_with_pct['variable_category'] == 'gender'][\n    ['offender_type', 'variable_detailed', 'value_numeric', 'percentage']\n].copy()\ngender_df.columns = ['Offender Type', 'Gender', 'Count', 'Percentage']\ngender_df['Count'] = gender_df['Count'].apply(lambda x: f\"{x:,.0f}\")\ngender_df['Percentage'] = gender_df['Percentage'].apply(lambda x: f\"{x:.2f}%\")\ndisplay(HTML(gender_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Calculate male counts\ndisplay(Markdown(\"\\n**Calculated Male/Other counts:**\"))\nmale_data = []\nfor offender_type in ['Offenders', 'Arrestee']:\n    total = texas_totals[offender_type]\n    female = texas_with_pct[(texas_with_pct['offender_type'] == offender_type) & \n                           (texas_with_pct['variable_category'] == 'gender')]['value_numeric'].iloc[0]\n    male = total - female\n    male_data.append({\n        'Offender Type': offender_type,\n        'Male/Other Count': f\"{int(male):,}\",\n        'Male/Other %': f\"{male/total * 100:.2f}%\"\n    })\nmale_df = pd.DataFrame(male_data)\ndisplay(HTML(male_df.to_html(index=False, classes='table table-striped')))\n\ndisplay(Markdown(\"\\n⚠️ **Note**: Texas only reported female counts. Male/Other counts must be calculated by subtraction.\"))\n\nTexas: Missing race data (Unknown category)\n\n\n\n\n\nOffender Type\nTotal\nRace Sum\nMissing\nMissing %\n\n\n\n\nOffenders\n845,322\n845,293\n29\n0.00%\n\n\nArrestee\n73,631\n71,470\n2,161\n2.93%\n\n\n\n\n\nTexas: Gender data (only female counts provided)\n\n\n\n\n\nOffender Type\nGender\nCount\nPercentage\n\n\n\n\nOffenders\nFemale\n121,434\n14.37%\n\n\nArrestee\nFemale\n18,721\n25.43%\n\n\n\n\n\nCalculated Male/Other counts:\n\n\n\n\n\nOffender Type\nMale/Other Count\nMale/Other %\n\n\n\n\nOffenders\n723,888\n85.63%\n\n\nArrestee\n54,910\n74.57%\n\n\n\n\n\n⚠️ Note: Texas only reported female counts. Male/Other counts must be calculated by subtraction.\n\n\n\n\n\nStep 3: States with Only Percentages\nIndiana requires special handling as they only provided percentages for demographics.\n\nIndiana: Verify Percentages Sum to 100%\n\n# Indiana only provided percentages for demographics\nindiana_pct = foia_raw[\n    (foia_raw['state'] == 'Indiana') & \n    (foia_raw['value_type'] == 'percentage')\n].copy()\n\n# Check if percentages sum to 100%\nindiana_pct_check = indiana_pct.groupby('variable_category').agg({\n    'value_numeric': 'sum',\n    'variable_detailed': lambda x: ', '.join(x)\n}).rename(columns={'value_numeric': 'total_percentage'})\n\ndisplay(Markdown(\"**Indiana: Checking if percentages sum to 100%**\"))\ncheck_df = indiana_pct_check.reset_index()\ncheck_df.columns = ['Category', 'Total %', 'Values']\ncheck_df['Total %'] = check_df['Total %'].apply(lambda x: f\"{x:.1f}%\")\ndisplay(HTML(check_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Get totals for calculating counts\nindiana_totals = foia_raw[\n    (foia_raw['state'] == 'Indiana') & \n    (foia_raw['variable_category'] == 'total')\n]\n\nconvicted = indiana_totals[indiana_totals['offender_type'] == 'Convicted Offender']['value_numeric'].iloc[0]\narrestee = indiana_totals[indiana_totals['offender_type'] == 'Arrestee']['value_numeric'].iloc[0]\ncombined = convicted + arrestee\n\ndisplay(Markdown(\"\\n**Indiana totals for calculating counts:**\"))\ntotals_data = [\n    ['Convicted', f\"{int(convicted):,}\"],\n    ['Arrestee', f\"{int(arrestee):,}\"],\n    ['Combined', f\"{int(combined):,}\"]\n]\ntotals_df = pd.DataFrame(totals_data, columns=['Type', 'Count'])\ndisplay(HTML(totals_df.to_html(index=False, classes='table table-striped')))\n\n# Calculate counts from percentages\ndisplay(Markdown(\"\\n**Calculated counts from percentages:**\"))\ncalculated_counts = []\nfor _, row in indiana_pct.iterrows():\n    count = round(combined * row['value_numeric'] / 100)\n    calculated_counts.append({\n        'Category': row['variable_category'],\n        'Detail': row['variable_detailed'],\n        'Percentage': f\"{row['value_numeric']}%\",\n        'Calculated Count': f\"{int(count):,}\"\n    })\ncalc_df = pd.DataFrame(calculated_counts)\ndisplay(HTML(calc_df.to_html(index=False, classes='table table-striped table-hover')))\n\ndisplay(Markdown(\"\\n⚠️ **Note**: Indiana's race percentages sum to 100.5% due to '&lt;1%' being interpreted as 0.5%\"))\n\nIndiana: Checking if percentages sum to 100%\n\n\n\n\n\nCategory\nTotal %\nValues\n\n\n\n\ngender\n100.0%\nFemale, Male\n\n\nrace\n100.5%\nCaucasian, Black, Hispanic, Other\n\n\n\n\n\nIndiana totals for calculating counts:\n\n\n\n\n\nType\nCount\n\n\n\n\nConvicted\n279,654\n\n\nArrestee\n21,087\n\n\nCombined\n300,741\n\n\n\n\n\nCalculated counts from percentages:\n\n\n\n\n\nCategory\nDetail\nPercentage\nCalculated Count\n\n\n\n\ngender\nFemale\n20.0%\n60,148\n\n\ngender\nMale\n80.0%\n240,593\n\n\nrace\nCaucasian\n70.0%\n210,519\n\n\nrace\nBlack\n26.0%\n78,193\n\n\nrace\nHispanic\n4.0%\n12,030\n\n\nrace\nOther\n0.5%\n1,504\n\n\n\n\n\n⚠️ Note: Indiana’s race percentages sum to 100.5% due to ‘&lt;1%’ being interpreted as 0.5%\n\n\n\n\n\nStep 4: Nevada Special Case\nNevada provides counts for all data but only some percentages.\n\n# Nevada has counts for everything, percentages for some\nnevada_data = foia_raw[foia_raw['state'] == 'Nevada'].copy()\n\nnevada_wide = nevada_data.pivot_table(\n    index=['offender_type', 'variable_category', 'variable_detailed'],\n    columns='value_type',\n    values='value_numeric',\n    aggfunc='first'\n).reset_index()\n\ndisplay(Markdown(\"**Nevada: Data with provided percentages**\"))\nwith_pct = nevada_wide[nevada_wide['percentage'].notna()].copy()\nsummary_df = with_pct[['offender_type', 'variable_category', 'variable_detailed', 'count', 'percentage']]\nsummary_df.columns = ['Offender Type', 'Category', 'Detail', 'Count', 'Percentage']\nsummary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\nsummary_df['Percentage'] = summary_df['Percentage'].apply(lambda x: f\"{x:.4f}%\")\ndisplay(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Calculate missing percentages\ntotal_flags = 344097\nnevada_calc = nevada_wide[nevada_wide['percentage'].isna() & nevada_wide['count'].notna()].copy()\nnevada_calc['calc_percentage'] = round(nevada_calc['count'] / total_flags * 100, 3)\n\ndisplay(Markdown(\"\\n**Nevada: Calculated percentages for missing data**\"))\ncalc_df = nevada_calc[['offender_type', 'variable_category', 'variable_detailed', 'count', 'calc_percentage']].copy()\ncalc_df.columns = ['Offender Type', 'Category', 'Detail', 'Count', 'Calculated %']\ncalc_df['Count'] = calc_df['Count'].apply(lambda x: f\"{x:,.0f}\")\ncalc_df['Calculated %'] = calc_df['Calculated %'].apply(lambda x: f\"{x:.3f}%\")\ndisplay(HTML(calc_df.to_html(index=False, classes='table table-striped table-hover')))\n\ndisplay(Markdown(\"\\n⚠️ **Note**: Nevada uses 'flags' instead of 'profiles' terminology\"))\n\nNevada: Data with provided percentages\n\n\n\n\n\nOffender Type\nCategory\nDetail\nCount\nPercentage\n\n\n\n\nArrested offender\ntotal\ntotal_profiles\n185,074\n53.7850%\n\n\nCombined\ngender\nFemale\n63,287\n18.3920%\n\n\nCombined\ngender\nMale\n280,738\n81.5870%\n\n\nCombined\ngender\nUnknown\n72\n0.0209%\n\n\nCombined\nrace\nAmerican Indian\n5,710\n1.6590%\n\n\nCombined\nrace\nAsian\n7,999\n2.3460%\n\n\nCombined\nrace\nBlack\n88,174\n25.6250%\n\n\nCombined\nrace\nUnknown\n3,491\n1.0150%\n\n\nCombined\nrace\nWhite\n238,723\n69.3770%\n\n\nConvicted offenders\ntotal\ntotal_profiles\n159,023\n46.2150%\n\n\n\n\n\nNevada: Calculated percentages for missing data\n\n\n\n\n\nOffender Type\nCategory\nDetail\nCount\nCalculated %\n\n\n\n\nAll\ntotal\ntotal_flags\n344,097\n100.000%\n\n\n\n\n\n⚠️ Note: Nevada uses ‘flags’ instead of ‘profiles’ terminology"
  },
  {
    "objectID": "analysis/foia_processing.html#summary-of-calculations-needed",
    "href": "analysis/foia_processing.html#summary-of-calculations-needed",
    "title": "FOIA Document OCR Processing",
    "section": "Summary of Calculations Needed",
    "text": "Summary of Calculations Needed\n\n1. Verification Results (States with Both)\n✅ All states with both counts and percentages show excellent agreement:\n\nFlorida: Perfect match - all differences are 0.00\nMaine: Only 0.5% difference for Male due to rounding\n\nSouth Dakota: Only 0.07% difference for Asian due to rounding\n\n\n\n2. States Requiring Percentage Calculations\n⚠️ Critical issues found:\n\nCalifornia:\n\nAll percentages need calculation\nRace percentages don’t sum to 100% (Convicted: 80.50%, Arrestee: 87.22%)\n\nTexas:\n\nAll percentages need calculation\nOnly female gender counts provided\nRace counts missing Unknown category\n\n\n\n\n3. States Requiring Count Calculations\n\nIndiana:\n\nGender percentages sum to 100% ✓\nRace percentages sum to 100.5% (due to “&lt;1%” → 0.5%)\nUse combined total of 300,741 for calculations\n\n\n\n\n4. Mixed Reporting\n\nNevada:\n\nHas counts for all data\nMissing some percentages (need calculation)\nUses “flags” terminology instead of “profiles”"
  },
  {
    "objectID": "analysis/foia_processing.html#data-quality-issues-identified",
    "href": "analysis/foia_processing.html#data-quality-issues-identified",
    "title": "FOIA Document OCR Processing",
    "section": "Data Quality Issues Identified",
    "text": "Data Quality Issues Identified\n\nTexas Gender Data: Only female counts provided\nTexas Race Data: Totals don’t match sum of races (missing Unknown)\nIndiana “Other” Race: Reported as “&lt;1%” - needs interpretation\nFlorida Rounding: Total percentages sum to 100.01%\nNevada Terminology: Uses “flags” instead of “profiles”\nSouth Dakota Complexity: Provides race×gender intersections others don’t"
  },
  {
    "objectID": "analysis/ndis_scraping.html",
    "href": "analysis/ndis_scraping.html",
    "title": "NDIS Database Analysis",
    "section": "",
    "text": "This analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025."
  },
  {
    "objectID": "analysis/ndis_scraping.html#introduction",
    "href": "analysis/ndis_scraping.html#introduction",
    "title": "NDIS Database Analysis",
    "section": "",
    "text": "This analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025."
  },
  {
    "objectID": "analysis/ndis_scraping.html#setup-and-configuration",
    "href": "analysis/ndis_scraping.html#setup-and-configuration",
    "title": "NDIS Database Analysis",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\n\nInstalling beautifulsoup4...\nRequirement already satisfied: beautifulsoup4 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (4.13.4)\nRequirement already satisfied: soupsieve&gt;1.2 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (4.14.1)\n\n\n\nfrom pathlib import Path\nimport re, json, requests, time\nfrom datetime import datetime\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Configuration\n# Path setup - using current directory as base\n# This assumes you run the notebook from the project root (PODFRIDGE-Databases)\nBASE_DIR = Path(\"..\")  # Current working directory\nHTML_DIR = BASE_DIR / \"raw\" / \"wayback_html\"\nMETA_DIR = BASE_DIR / \"raw\" / \"wayback_meta\"\nOUTPUT_DIR = BASE_DIR / \"output\" / \"ndis\"\n\n# Create directories if they don't exist\nHTML_DIR.mkdir(parents=True, exist_ok=True)\nMETA_DIR.mkdir(parents=True, exist_ok=True)\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Jurisdiction name standardization mapping\nJURISDICTION_NAME_MAP = {\n    'D.C./FBI Lab': 'DC/FBI Lab',\n    'US Army': 'U.S. Army'\n}\n\n# Known data typos to fix\nKNOWN_TYPOS = [\n    {\n        'timestamp': '20250105164014',\n        'jurisdiction': 'California', \n        'field': 'investigations_aided',\n        'wrong_value': '1304657',  # How it parses\n        'correct_value': '130465'   # What it should be\n    },\n    {\n        'timestamp': '20250116205311',\n        'jurisdiction': 'California',\n        'field': 'investigations_aided', \n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    }\n]\n\nprint(f\"Working directory: {BASE_DIR.resolve()}\")\nprint(f\"HTML directory: {HTML_DIR}\")\nprint(f\"Meta directory: {META_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\n\nWorking directory: /Users/tlasisi/GitHub/PODFRIDGE-Databases\nHTML directory: ../raw/wayback_html\nMeta directory: ../raw/wayback_meta\nOutput directory: ../output/ndis"
  },
  {
    "objectID": "analysis/ndis_scraping.html#wayback-machine-functions",
    "href": "analysis/ndis_scraping.html#wayback-machine-functions",
    "title": "NDIS Database Analysis",
    "section": "Wayback Machine Functions",
    "text": "Wayback Machine Functions\n\ndef make_request_with_retry(params, max_retries=3, initial_delay=5):\n    \"\"\"Make a request with exponential backoff retry logic\"\"\"\n    base = \"https://web.archive.org/cdx/search/cdx\"\n    \n    for attempt in range(max_retries):\n        try:\n            r = requests.get(base, params=params, timeout=30)\n            if r.status_code == 200:\n                return r\n            elif r.status_code == 429:  # Rate limited\n                wait_time = initial_delay * (2 ** attempt)\n                print(f\"    Rate limited. Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                return r\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except Exception as e:\n            print(f\"    Unexpected error: {e}\")\n            return None\n    return None"
  },
  {
    "objectID": "analysis/ndis_scraping.html#search-for-all-ndis-snapshots",
    "href": "analysis/ndis_scraping.html#search-for-all-ndis-snapshots",
    "title": "NDIS Database Analysis",
    "section": "Search for All NDIS Snapshots",
    "text": "Search for All NDIS Snapshots\n\n\nShow search function code\ndef search_all_ndis_snapshots():\n    \"\"\"Search for NDIS snapshots across all known URL variations\"\"\"\n    \n    # Search for both http and https variants\n    protocols = [\"http://\", \"https://\"]\n    subdomains = [\"www\", \"le\", \"*\"]  # Known subdomains plus wildcard\n    \n    all_rows = []\n    seen_timestamps = set()\n    \n    # First, try broad searches with protocol wildcards\n    print(\"Starting wildcard searches...\")\n    for protocol in protocols:\n        for subdomain in subdomains:\n            pattern = f\"{protocol}{subdomain}.fbi.gov/*ndis-statistics*\"\n            print(f\"\\nSearching: {pattern}\")\n            \n            params = {\n                \"url\":         pattern,\n                \"matchType\":   \"wildcard\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) &gt; 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests to avoid rate limiting\n            time.sleep(2)\n    \n    # Also search your specific known URLs with both protocols\n    known_paths = [\n        \"www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n        \"www.fbi.gov/about-us/laboratory/biometric-analysis/codis/ndis-statistics\", \n        \"www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n        \"le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\",\n    ]\n    \n    print(\"\\n\\nStarting exact URL searches...\")\n    for path in known_paths:\n        for protocol in protocols:\n            url = f\"{protocol}{path}\"\n            print(f\"\\nSearching: {url}\")\n            \n            params = {\n                \"url\":         url,\n                \"matchType\":   \"exact\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) &gt; 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests\n            time.sleep(2)\n    \n    # Create DataFrame\n    snap_df = (pd.DataFrame(\n                    all_rows,\n                    columns=[\"timestamp\", \"original\", \"mimetype\", \"status\"])\n               .sort_values(\"timestamp\")\n               .reset_index(drop=True))\n    \n    return snap_df\n\n# Check if we already have snapshot data or need to search\nsnapshot_csv = META_DIR / 'snapshots_found.csv'\nif snapshot_csv.exists():\n    print(\"Loading existing snapshot list...\")\n    snap_df = pd.read_csv(snapshot_csv)\n    print(f\"Loaded {len(snap_df)} snapshots\")\nelse:\n    print(\"Searching for all NDIS snapshots...\")\n    snap_df = search_all_ndis_snapshots()\n    if len(snap_df) &gt; 0:\n        snap_df.to_csv(snapshot_csv, index=False)\n        print(f\"\\nSaved {len(snap_df)} snapshots to {snapshot_csv}\")\n\nif len(snap_df) &gt; 0:\n    print(f\"\\nTotal unique snapshots found: {len(snap_df):,}\")\n    print(f\"Unique URLs found: {snap_df['original'].nunique()}\")\n    print(\"\\nUnique URL patterns found:\")\n    for url in sorted(snap_df['original'].unique()):\n        print(f\"  {url}\")\n\n\nLoading existing snapshot list...\nLoaded 317 snapshots\n\nTotal unique snapshots found: 317\nUnique URLs found: 8\n\nUnique URL patterns found:\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics/\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics/\n  https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics/\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics//"
  },
  {
    "objectID": "analysis/ndis_scraping.html#download-functions",
    "href": "analysis/ndis_scraping.html#download-functions",
    "title": "NDIS Database Analysis",
    "section": "Download Functions",
    "text": "Download Functions\n\ndef download_with_retry(url, max_retries=3, initial_delay=5, consecutive_failures=0):\n    \"\"\"Download with adaptive retry logic based on consecutive failures\"\"\"\n    if consecutive_failures &gt; 0:\n        extra_wait = consecutive_failures * 10\n        print(f\"\\n    Adding {extra_wait}s cooldown due to {consecutive_failures} consecutive failures...\")\n        time.sleep(extra_wait)\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            return response, True\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"\\n    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:\n                wait_time = initial_delay * (2 ** attempt) * 2\n                print(f\"\\n    Rate limited (429). Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"\\n    HTTP Error: {e}\")\n                return None, False\n        except Exception as e:\n            print(f\"\\n    Unexpected error: {e}\")\n            return None, False\n    return None, False\n\ndef download_missing_snapshots(snap_df, output_folder):\n    \"\"\"Download HTML snapshots with resume capability and detailed logging\"\"\"\n    \n    # Create run-specific log file\n    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_log_file = META_DIR / f\"download_log_{run_timestamp}.txt\"\n    \n    # Check what we already have\n    existing_files = list(output_folder.glob(\"*.html\"))\n    existing_timestamps = {f.stem for f in existing_files}\n    print(f\"\\nFiles already downloaded: {len(existing_files)}\")\n    \n    # Check what needs to be downloaded\n    to_download = []\n    for _, row in snap_df.iterrows():\n        timestamp = row['timestamp']\n        url = row['original']\n        filename = output_folder / f\"{timestamp}.html\"\n        \n        if timestamp not in existing_timestamps and not filename.exists():\n            to_download.append((timestamp, url, filename))\n    \n    print(f\"Files to download: {len(to_download)}\")\n    \n    # Initialize log file\n    with open(run_log_file, \"w\") as log:\n        log.write(f\"Download run started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Total snapshots in list: {len(snap_df)}\\n\")\n        log.write(f\"Already downloaded: {len(existing_files)}\\n\")\n        log.write(f\"To download: {len(to_download)}\\n\")\n        log.write(f\"{'='*60}\\n\\n\")\n    \n    if len(to_download) == 0:\n        print(\"\\n✓ All files already downloaded! Nothing to do.\")\n        with open(run_log_file, \"a\") as log:\n            log.write(\"All files already downloaded. No action needed.\\n\")\n        return\n    \n    # Download configuration\n    BATCH_SIZE = 15\n    PAUSE_BETWEEN_DOWNLOADS = 3\n    PAUSE_BETWEEN_BATCHES = 45\n    PAUSE_AFTER_FAILURE = 60\n    \n    # Track statistics\n    successful_downloads = 0\n    failed_downloads = []\n    consecutive_failures = 0\n    \n    # Download in batches\n    for i in range(0, len(to_download), BATCH_SIZE):\n        batch = to_download[i:i + BATCH_SIZE]\n        batch_num = (i // BATCH_SIZE) + 1\n        total_batches = (len(to_download) + BATCH_SIZE - 1) // BATCH_SIZE\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Batch {batch_num}/{total_batches} ({len(batch)} files)\")\n        print(f\"Overall progress: {len(existing_timestamps) + successful_downloads}/{len(snap_df)} total files\")\n        print(f\"{'='*60}\")\n        \n        with open(run_log_file, \"a\") as log:\n            log.write(f\"\\nBatch {batch_num}/{total_batches} started at {datetime.now().strftime('%H:%M:%S')}\\n\")\n        \n        for j, (timestamp, url, filename) in enumerate(batch, 1):\n            # Double-check file doesn't exist\n            if filename.exists():\n                print(f\"\\n[{j}/{len(batch)}] {timestamp} - Already exists, skipping...\")\n                with open(run_log_file, \"a\") as log:\n                    log.write(f\"{timestamp}  ⚡ already exists\\n\")\n                continue\n                \n            wayback_url = f\"https://web.archive.org/web/{timestamp}/{url}\"\n            \n            print(f\"\\n[{j}/{len(batch)}] Downloading {timestamp}...\", end=\"\")\n            \n            response, success = download_with_retry(wayback_url, consecutive_failures=consecutive_failures)\n            \n            if response and response.status_code == 200:\n                try:\n                    with open(filename, 'w', encoding='utf-8') as f:\n                        f.write(response.text)\n                    \n                    print(\" ✓ Success\")\n                    successful_downloads += 1\n                    consecutive_failures = 0\n                    \n                    with open(run_log_file, \"a\") as log:\n                        log.write(f\"{timestamp}  ✓ downloaded\\n\")\n                    \n                except Exception as e:\n                    print(f\" ✗ Error saving file: {e}\")\n                    failed_downloads.append((timestamp, url, str(e)))\n                    consecutive_failures += 1\n                    \n                    with open(run_log_file, \"a\") as log:\n                        log.write(f\"{timestamp}  ✗ failed: {str(e)}\\n\")\n            else:\n                print(\" ✗ Failed after retries\")\n                failed_downloads.append((timestamp, url, \"Download failed\"))\n                consecutive_failures += 1\n                \n                with open(run_log_file, \"a\") as log:\n                    log.write(f\"{timestamp}  ✗ failed: Download failed after retries\\n\")\n                \n                if j &lt; len(batch):\n                    print(f\"    Taking {PAUSE_AFTER_FAILURE}s break after failure...\")\n                    time.sleep(PAUSE_AFTER_FAILURE)\n                    continue\n            \n            if j &lt; len(batch) and consecutive_failures == 0:\n                print(f\"    Waiting {PAUSE_BETWEEN_DOWNLOADS} seconds...\")\n                time.sleep(PAUSE_BETWEEN_DOWNLOADS)\n        \n        if i + BATCH_SIZE &lt; len(to_download):\n            print(f\"\\nBatch complete. Pausing {PAUSE_BETWEEN_BATCHES} seconds...\")\n            print(f\"This session: {successful_downloads} downloaded, {len(failed_downloads)} failed\")\n            time.sleep(PAUSE_BETWEEN_BATCHES)\n    \n    # Final summary\n    print(f\"\\n{'='*60}\")\n    print(f\"Download session complete!\")\n    print(f\"  Successfully downloaded: {successful_downloads}\")\n    print(f\"  Failed downloads: {len(failed_downloads)}\")\n    \n    # Write final summary to log\n    with open(run_log_file, \"a\") as log:\n        log.write(f\"\\n{'='*60}\\n\")\n        log.write(f\"Download run completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Successfully downloaded: {successful_downloads}\\n\")\n        log.write(f\"Failed downloads: {len(failed_downloads)}\\n\")\n        \n        if failed_downloads:\n            log.write(f\"\\nFailed downloads detail:\\n\")\n            for timestamp, url, error in failed_downloads:\n                log.write(f\"  {timestamp}: {error}\\n\")\n    \n    if failed_downloads:\n        print(f\"\\nFailed downloads:\")\n        for timestamp, url, error in failed_downloads[:10]:\n            print(f\"  {timestamp}: {error}\")\n        if len(failed_downloads) &gt; 10:\n            print(f\"  ... and {len(failed_downloads) - 10} more\")\n    \n    print(f\"\\nDownload log saved to: {run_log_file}\")\n\n# Download missing files\nif len(snap_df) &gt; 0:\n    download_missing_snapshots(snap_df, HTML_DIR)\n\n\nFiles already downloaded: 0\nFiles to download: 317\n\n============================================================\nBatch 1/22 (15 files)\nOverall progress: 0/317 total files\n============================================================\n\n[1/15] Downloading 20101014043819... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20110111093835... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20110127075531... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20110410202836... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20110706142451... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20110903022829... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20111001205511... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20111022093959... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20111026175039... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20111101090945... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20111115114507... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20111202050758... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20120101203804... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20120111115907... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20120508173804... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 15 downloaded, 0 failed\n\n============================================================\nBatch 2/22 (15 files)\nOverall progress: 15/317 total files\n============================================================\n\n[1/15] Downloading 20120701052745... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20120815160113... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20120911092525... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20120913074430... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20120913074444... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20120915124215...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[7/15] Downloading 20120921122419...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20120927161952... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20120927162304... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20120928031850... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20121006223803... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20121014054004... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20121014145914... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20121017230003... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20121018124013... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 29 downloaded, 1 failed\n\n============================================================\nBatch 3/22 (15 files)\nOverall progress: 29/317 total files\n============================================================\n\n[1/15] Downloading 20121021061338... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20121022162330... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20121023200759... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20121025072144... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20121027093545... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20121028031145... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20121030024545... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20121031002501... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20121103063643... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20121105112218... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20121114072458... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20160730053232...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[13/15] Downloading 20160826152101...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20161014043057... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20170106073100... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 43 downloaded, 2 failed\n\n============================================================\nBatch 4/22 (15 files)\nOverall progress: 43/317 total files\n============================================================\n\n[1/15] Downloading 20170127174455... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20170129013226... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20170227173146... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20170409202627... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20170505054222... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20170513040512... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20170516144007... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20170516201425... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20170518003707... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20170520050254... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20170621200054... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20170710141223... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20170925144132... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20170930134422... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20171014140750... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 58 downloaded, 2 failed\n\n============================================================\nBatch 5/22 (15 files)\nOverall progress: 58/317 total files\n============================================================\n\n[1/15] Downloading 20171025074255... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20171206151623... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20171214175911...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[4/15] Downloading 20180211052927...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20180314184913... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20180425125459... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20180428000233... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20180517024114... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20180608053326... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20180707231136... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20180710171340... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20180827122745... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20180911202332... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20180913062030... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20181016070747... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 72 downloaded, 3 failed\n\n============================================================\nBatch 6/22 (15 files)\nOverall progress: 72/317 total files\n============================================================\n\n[1/15] Downloading 20181017073224... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20181018155249... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20181019162711... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20181020170549... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20181021184026... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20181022182033... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20181024094434... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20181027194438... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20181104153147...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[10/15] Downloading 20181116183506...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20181130044044... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20181204032834... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20181220214643... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20181230180748... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20190103203642... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 86 downloaded, 4 failed\n\n============================================================\nBatch 7/22 (15 files)\nOverall progress: 86/317 total files\n============================================================\n\n[1/15] Downloading 20190129222014... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20190215164131... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20190302060036... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20190302211500... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20190305145401... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20190307110347... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20190320233159... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20190322204155... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20190323202517... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20190324201718... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20190326161430... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20190401203737... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20190412150804... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20190502135053... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20190502231236...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n\nBatch complete. Pausing 45 seconds...\nThis session: 100 downloaded, 5 failed\n\n============================================================\nBatch 8/22 (15 files)\nOverall progress: 100/317 total files\n============================================================\n\n[1/15] Downloading 20190510235359...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20190510235400... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20190511000325... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20190523123747... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20190611144717... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20190612165511... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20190613161343...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[8/15] Downloading 20190614180425...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20190620210900... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20190626135247... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20190702201315... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20190703224220... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20190810091618... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20190816134908... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20190824202250... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 114 downloaded, 6 failed\n\n============================================================\nBatch 9/22 (15 files)\nOverall progress: 114/317 total files\n============================================================\n\n[1/15] Downloading 20190825210357... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20190929200543... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20190929200638... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20191004193052... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20191005212555... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20191006085602... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20191011161318... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20191017000804... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20191019061318... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20191026043249... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20191027055331... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20191030211212... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20191102010133...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[14/15] Downloading 20191104050315...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20191107032513... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 128 downloaded, 7 failed\n\n============================================================\nBatch 10/22 (15 files)\nOverall progress: 128/317 total files\n============================================================\n\n[1/15] Downloading 20191108023243... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20191117165200... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20191124095229... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20191217191331... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20191226185928... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20200117180042... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20200301051858... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200307031008... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20200326154938... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20200401175110... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20200401211339... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20200405080000... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20200414121931... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20200415182137... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20200429225129... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 143 downloaded, 7 failed\n\n============================================================\nBatch 11/22 (15 files)\nOverall progress: 143/317 total files\n============================================================\n\n[1/15] Downloading 20200510221207... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20200511224952... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20200519201942... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20200607224349...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[5/15] Downloading 20200620221947...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20200719134420... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20200728162952... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200802095215... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20200803110741... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20200806102056... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20200806191421... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20200809030342... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20200814161013... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20200816121500... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20200818151622... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 157 downloaded, 8 failed\n\n============================================================\nBatch 12/22 (15 files)\nOverall progress: 157/317 total files\n============================================================\n\n[1/15] Downloading 20200820052431... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20200826044232... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20200914021204... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20200916105338... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20200916211529... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20200917051636... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20200918184343... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200920212212... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20200926224215... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20201002213332...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[11/15] Downloading 20201016205200...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20201018092433... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20201020010327... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20201028215925... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20201029194830... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 171 downloaded, 9 failed\n\n============================================================\nBatch 13/22 (15 files)\nOverall progress: 171/317 total files\n============================================================\n\n[1/15] Downloading 20201111223430... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20201112011539... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20201125102405... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20201126071015... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20201203022058... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20201203072310... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20201204071640... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20201209084343... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20201218220958... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20201224155501... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20201225185038... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20201226161923... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20201227181400... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20201228193055... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20201229213443... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 186 downloaded, 9 failed\n\n============================================================\nBatch 14/22 (15 files)\nOverall progress: 186/317 total files\n============================================================\n\n[1/15] Downloading 20201230185607...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[2/15] Downloading 20210101071729...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20210103113801... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20210104124131... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20210117123428... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20210117154805... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20210118215353... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20210119080323... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20210124090728... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20210125004521... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210126040048... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210129170244... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20210131205415... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20210206102132... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20210212004658... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 200 downloaded, 10 failed\n\n============================================================\nBatch 15/22 (15 files)\nOverall progress: 200/317 total files\n============================================================\n\n[1/15] Downloading 20210214004627... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20210303055250... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20210305012011... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20210308040136... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20210308111758... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20210309005158... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20210311023951...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[8/15] Downloading 20210318000609...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20210324050729... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20210409020505... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210414043038... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210428153014... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20210507090954... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20210507090955... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20210514075457... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 214 downloaded, 11 failed\n\n============================================================\nBatch 16/22 (15 files)\nOverall progress: 214/317 total files\n============================================================\n\n[1/15] Downloading 20210527100801... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20210619090441... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20210703080605... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20210703105804... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20210706213905... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20210711064442... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20210713143621... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20210718105628... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20210808190600... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20210809035809... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210905221144... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210908064238... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20210924184245...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[14/15] Downloading 20210926213344...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20211010060654... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 228 downloaded, 12 failed\n\n============================================================\nBatch 17/22 (15 files)\nOverall progress: 228/317 total files\n============================================================\n\n[1/15] Downloading 20211010060655... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20211010203923... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20211019180134... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20211021235339... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20211025021035... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20211104193210... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20211114092857... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20211121045733... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20211128164432... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20211205045644... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20211205200142... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20211217120358... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20211224185611... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20220125103659... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20220130221713... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 243 downloaded, 12 failed\n\n============================================================\nBatch 18/22 (15 files)\nOverall progress: 243/317 total files\n============================================================\n\n[1/15] Downloading 20220209033036... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20220210054625... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20220214082051... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20220226123842...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[5/15] Downloading 20220305152131...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20220307213357... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20220311083835... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20220321052126... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20220326170646... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20220328083342... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20220329024143... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20220331230634... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20220402203124... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20220416015545... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20220423192010... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 257 downloaded, 13 failed\n\n============================================================\nBatch 19/22 (15 files)\nOverall progress: 257/317 total files\n============================================================\n\n[1/15] Downloading 20220519223937... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20220601085211... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20220629123237... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20230411172744... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20230515144637... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20230528103842... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20230713202131... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20230713202137... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20230829204608... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20230829213846...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[11/15] Downloading 20230831093549...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20230901125409... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20230902061832... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20230913085200... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20231025142202... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 271 downloaded, 14 failed\n\n============================================================\nBatch 20/22 (15 files)\nOverall progress: 271/317 total files\n============================================================\n\n[1/15] Downloading 20231025183800... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20231031115239... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20231102072222... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20231113033840... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20231127144926... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20231130100758... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20240111202156... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20240116143636... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20240203212847... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20240317130138... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20240413070959... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20240415150311... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20240713143931... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20240714204728... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20240822224345... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 286 downloaded, 14 failed\n\n============================================================\nBatch 21/22 (15 files)\nOverall progress: 286/317 total files\n============================================================\n\n[1/15] Downloading 20240824025139...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[2/15] Downloading 20240826235428...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20240927093855... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20241014183409... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20241128025327... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20250105164014... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20250116205311... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20250117084527... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20250201152305... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20250305200402... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20250307115942... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20250329050408... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20250523182825... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20250603075541... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20250606041947... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 300 downloaded, 15 failed\n\n============================================================\nBatch 22/22 (2 files)\nOverall progress: 300/317 total files\n============================================================\n\n[1/2] Downloading 20250606050213... ✓ Success\n    Waiting 3 seconds...\n\n[2/2] Downloading 20250629171550... ✓ Success\n\n============================================================\nDownload session complete!\n  Successfully downloaded: 302\n  Failed downloads: 15\n\nFailed downloads:\n  20120915124215: Download failed\n  20160730053232: Download failed\n  20171214175911: Download failed\n  20181104153147: Download failed\n  20190502231236: Download failed\n  20190613161343: Download failed\n  20191102010133: Download failed\n  20200607224349: Download failed\n  20201002213332: Download failed\n  20201230185607: Download failed\n  ... and 5 more\n\nDownload log saved to: ../raw/wayback_meta/download_log_20250721_190902.txt"
  },
  {
    "objectID": "analysis/ndis_scraping.html#download-status-check-functions",
    "href": "analysis/ndis_scraping.html#download-status-check-functions",
    "title": "NDIS Database Analysis",
    "section": "Download Status Check Functions",
    "text": "Download Status Check Functions\n\ndef get_latest_log():\n    \"\"\"Find and read the most recent download log\"\"\"\n    log_files = sorted(META_DIR.glob(\"download_log_*.txt\"))\n    if not log_files:\n        print(\"No download logs found.\")\n        return None\n    \n    latest_log = log_files[-1]\n    print(f\"Latest log: {latest_log.name}\")\n    return latest_log\n\ndef analyze_download_status():\n    \"\"\"Analyze the current download status and latest run results\"\"\"\n    \n    # Check what we have\n    html_files = list(HTML_DIR.glob(\"*.html\"))\n    downloaded_timestamps = {f.stem for f in html_files}\n    \n    # Check what we should have\n    snapshot_csv = META_DIR / 'snapshots_found.csv'\n    if snapshot_csv.exists():\n        snap_df = pd.read_csv(snapshot_csv)\n        expected_timestamps = set(snap_df['timestamp'].astype(str))\n    else:\n        print(\"No snapshot list found. Run search first.\")\n        return None\n    \n    # Calculate missing\n    missing_timestamps = expected_timestamps - downloaded_timestamps\n    \n    # Parse latest log for failures\n    latest_log = get_latest_log()\n    failed_in_last_run = []\n    \n    if latest_log:\n        with open(latest_log, 'r') as f:\n            for line in f:\n                if '✗ failed:' in line:\n                    timestamp = line.split()[0]\n                    if timestamp.isdigit() and len(timestamp) == 14:\n                        failed_in_last_run.append(timestamp)\n    \n    # Create summary\n    print(f\"\\n{'='*60}\")\n    print(\"DOWNLOAD STATUS SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"Expected snapshots: {len(expected_timestamps)}\")\n    print(f\"Downloaded: {len(downloaded_timestamps)} ({len(downloaded_timestamps)/len(expected_timestamps)*100:.1f}%)\")\n    print(f\"Missing: {len(missing_timestamps)}\")\n    \n    if latest_log:\n        print(f\"\\nLatest run ({latest_log.name}):\")\n        print(f\"  Failed downloads: {len(failed_in_last_run)}\")\n        if failed_in_last_run:\n            print(f\"  Failed timestamps: {', '.join(failed_in_last_run[:5])}\")\n            if len(failed_in_last_run) &gt; 5:\n                print(f\"  ... and {len(failed_in_last_run) - 5} more\")\n    \n    print(f\"\\nTotal still needed: {len(missing_timestamps)}\")\n    \n    return {\n        'missing': missing_timestamps,\n        'failed_last_run': failed_in_last_run,\n        'downloaded': downloaded_timestamps,\n        'expected': expected_timestamps\n    }\n\ndef create_retry_list(status_info, retry_only_failed=True):\n    \"\"\"Create a list of files to retry downloading\"\"\"\n    if not status_info:\n        return None\n    \n    if retry_only_failed and status_info['failed_last_run']:\n        retry_timestamps = set(status_info['failed_last_run'])\n        print(f\"\\nWill retry {len(retry_timestamps)} failed downloads from last run\")\n    else:\n        retry_timestamps = status_info['missing']\n        print(f\"\\nWill retry all {len(retry_timestamps)} missing files\")\n    \n    snapshot_csv = META_DIR / 'snapshots_found.csv'\n    snap_df = pd.read_csv(snapshot_csv)\n    retry_df = snap_df[snap_df['timestamp'].astype(str).isin(retry_timestamps)]\n    \n    return retry_df"
  },
  {
    "objectID": "analysis/ndis_scraping.html#parser-functions",
    "href": "analysis/ndis_scraping.html#parser-functions",
    "title": "NDIS Database Analysis",
    "section": "Parser Functions",
    "text": "Parser Functions\n\ndef clean_jurisdiction_name(name):\n    \"\"\"Clean up jurisdiction names by removing common prefixes\"\"\"\n    name = re.sub(r'^.*?Back to top\\s*', '', name)\n    name = re.sub(r'^.*?Tables by NDIS Participant\\s*', '', name)\n    name = re.sub(r'^.*?ation\\.\\s*', '', name)\n    name = name.strip()\n    return name\n\ndef standardize_jurisdiction_name(name):\n    \"\"\"Standardize jurisdiction names to handle variations\"\"\"\n    name = clean_jurisdiction_name(name)\n    if name in JURISDICTION_NAME_MAP:\n        return JURISDICTION_NAME_MAP[name]\n    return name\n\ndef extract_data_date(html_content):\n    \"\"\"Extract the 'Statistics as of' date from HTML content\"\"\"\n    match = re.search(r'Statistics as of (\\w+ \\d{4})', html_content, re.IGNORECASE)\n    if match:\n        date_str = match.group(1)\n        try:\n            # Convert \"October 2024\" to datetime\n            return datetime.strptime(date_str, \"%B %Y\")\n        except:\n            pass\n    return None\n\ndef parse_ndis_snapshot(html_file):\n    \"\"\"Parse a single NDIS snapshot file\"\"\"\n    timestamp = html_file.stem\n    year = int(timestamp[:4])\n    \n    html_content = html_file.read_text('utf-8', errors='ignore')\n    soup = BeautifulSoup(html_content, 'lxml')\n    text = soup.get_text(' ', strip=True)\n    \n    # Extract the \"as of\" date\n    data_date = extract_data_date(html_content)\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    records = []\n    \n    # Pattern for 2010 (no arrestee data)\n    if year &lt;= 2010:\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Samples\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': '0',\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    else:\n        # Pattern for 2011+ (includes arrestee data)\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Arrestee\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Profiles\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, arrestee, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': arrestee.replace(',', ''),\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    \n    return records"
  },
  {
    "objectID": "analysis/ndis_scraping.html#process-all-snapshots",
    "href": "analysis/ndis_scraping.html#process-all-snapshots",
    "title": "NDIS Database Analysis",
    "section": "Process All Snapshots",
    "text": "Process All Snapshots\n\ndef process_all_snapshots():\n    \"\"\"Parse all downloaded snapshots and create datasets\"\"\"\n    print(\"Processing all snapshots...\")\n    \n    all_records = []\n    html_files = sorted(HTML_DIR.glob(\"*.html\"))\n    \n    for html_file in tqdm(html_files, desc=\"Parsing HTML files\"):\n        try:\n            records = parse_ndis_snapshot(html_file)\n            all_records.extend(records)\n        except Exception as e:\n            print(f\"Error parsing {html_file.name}: {e}\")\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(all_records)\n    \n    # Convert numeric fields\n    numeric_fields = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n    for field in numeric_fields:\n        df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0).astype(int)\n    \n    # Add datetime columns\n    df['capture_datetime'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S')\n    df['capture_date'] = df['capture_datetime'].dt.date\n    \n    # Sort by timestamp and jurisdiction\n    df = df.sort_values(['timestamp', 'jurisdiction'])\n    \n    return df\n\n# Process all files\ndf_raw = process_all_snapshots()\nprint(f\"\\nProcessed {len(df_raw)} total records\")\nprint(f\"Unique jurisdictions: {df_raw['jurisdiction'].nunique()}\")\nprint(f\"Date range: {df_raw['capture_datetime'].min()} to {df_raw['capture_datetime'].max()}\")\n\nProcessing all snapshots...\n\n\n\n\n\n\nProcessed 15320 total records\nUnique jurisdictions: 54\nDate range: 2010-10-14 04:38:19 to 2025-06-29 17:15:50"
  },
  {
    "objectID": "analysis/ndis_scraping.html#check-download-completeness",
    "href": "analysis/ndis_scraping.html#check-download-completeness",
    "title": "NDIS Database Analysis",
    "section": "Check Download Completeness",
    "text": "Check Download Completeness\n\n# Check if we have all expected files\nprint(\"\\nChecking download completeness...\")\nstatus = analyze_download_status()\n\n# Automatically retry if there are failures\nif status and status['failed_last_run'] and len(status['failed_last_run']) &gt; 0:\n    print(f\"\\n⚠️  Found {len(status['failed_last_run'])} failed downloads from last run. Retrying...\")\n    retry_df = create_retry_list(status, retry_only_failed=True)\n    if retry_df is not None and len(retry_df) &gt; 0:\n        download_missing_snapshots(retry_df, HTML_DIR)\n        \n        # Re-check status after retry\n        print(\"\\nRechecking status after retry...\")\n        status = analyze_download_status()\nelif status and status['missing']:\n    print(f\"\\n⚠️  {len(status['missing'])} files are missing but weren't from a failed run.\")\n    print(\"These may be new snapshots. Run download cell manually if needed.\")\nelse:\n    print(\"\\n✅ All files successfully downloaded!\")\n\n\nChecking download completeness...\nLatest log: download_log_20250721_190902.txt\n\n============================================================\nDOWNLOAD STATUS SUMMARY\n============================================================\nExpected snapshots: 317\nDownloaded: 302 (95.3%)\nMissing: 15\n\nLatest run (download_log_20250721_190902.txt):\n  Failed downloads: 15\n  Failed timestamps: 20120915124215, 20160730053232, 20171214175911, 20181104153147, 20190502231236\n  ... and 10 more\n\nTotal still needed: 15\n\n⚠️  Found 15 failed downloads from last run. Retrying...\n\nWill retry 15 failed downloads from last run\n\nFiles already downloaded: 302\nFiles to download: 15\n\n============================================================\nBatch 1/1 (15 files)\nOverall progress: 302/15 total files\n============================================================\n\n[1/15] Downloading 20120915124215... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20160730053232... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20171214175911... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20181104153147... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20190502231236...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[6/15] Downloading 20190613161343...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20191102010133... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200607224349... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20201002213332... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20201230185607... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210311023951... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210924184245... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20220226123842... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20230829213846... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20240824025139... ✓ Success\n\n============================================================\nDownload session complete!\n  Successfully downloaded: 14\n  Failed downloads: 1\n\nFailed downloads:\n  20190502231236: Download failed\n\nDownload log saved to: ../raw/wayback_meta/download_log_20250721_200952.txt\n\nRechecking status after retry...\nLatest log: download_log_20250721_200952.txt\n\n============================================================\nDOWNLOAD STATUS SUMMARY\n============================================================\nExpected snapshots: 317\nDownloaded: 316 (99.7%)\nMissing: 1\n\nLatest run (download_log_20250721_200952.txt):\n  Failed downloads: 1\n  Failed timestamps: 20190502231236\n\nTotal still needed: 1"
  },
  {
    "objectID": "analysis/ndis_scraping.html#apply-typo-fixes",
    "href": "analysis/ndis_scraping.html#apply-typo-fixes",
    "title": "NDIS Database Analysis",
    "section": "Apply Typo Fixes",
    "text": "Apply Typo Fixes\n\ndef apply_typo_fixes(df):\n    \"\"\"Apply known typo corrections\"\"\"\n    df_fixed = df.copy()\n    \n    for typo in KNOWN_TYPOS:\n        mask = (\n            (df_fixed['timestamp'] == typo['timestamp']) & \n            (df_fixed['jurisdiction'] == typo['jurisdiction'])\n        )\n        if mask.any():\n            df_fixed.loc[mask, typo['field']] = int(typo['correct_value'])\n            print(f\"Fixed typo: {typo['jurisdiction']} on {typo['timestamp'][:8]} - \"\n                  f\"{typo['field']} from {typo['wrong_value']} to {typo['correct_value']}\")\n    \n    return df_fixed\n\n# Apply fixes\ndf_fixed = apply_typo_fixes(df_raw)\n\nFixed typo: California on 20250105 - investigations_aided from 1304657 to 130465\nFixed typo: California on 20250116 - investigations_aided from 1304657 to 130465"
  },
  {
    "objectID": "analysis/ndis_scraping.html#save-datasets",
    "href": "analysis/ndis_scraping.html#save-datasets",
    "title": "NDIS Database Analysis",
    "section": "Save Datasets",
    "text": "Save Datasets\n\n# Save datasets\ndf_raw.to_csv(OUTPUT_DIR / 'ndis_data_raw.csv', index=False)\ndf_fixed.to_csv(OUTPUT_DIR / 'ndis_data_fixed.csv', index=False)\nprint(f\"\\nSaved raw data to: {OUTPUT_DIR / 'ndis_data_raw.csv'}\")\nprint(f\"Saved fixed data to: {OUTPUT_DIR / 'ndis_data_fixed.csv'}\")\n\n\nSaved raw data to: ../output/ndis/ndis_data_raw.csv\nSaved fixed data to: ../output/ndis/ndis_data_fixed.csv"
  },
  {
    "objectID": "analysis/ndis_scraping.html#summary-statistics",
    "href": "analysis/ndis_scraping.html#summary-statistics",
    "title": "NDIS Database Analysis",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n# Calculate summary statistics\nlatest_data = df_fixed[df_fixed['capture_datetime'] == df_fixed['capture_datetime'].max()]\nlatest_data = latest_data[latest_data['jurisdiction'] != 'D.C./Metro PD']\n\nprint(\"\\nLatest Statistics Summary:\")\nprint(f\"  As of: {latest_data['capture_datetime'].iloc[0]}\")\nprint(f\"  Data from: {latest_data['data_as_of_date'].iloc[0] if latest_data['data_as_of_date'].iloc[0] else 'Unknown'}\")\nprint(f\"  Jurisdictions reporting: {len(latest_data)}\")\nprint(f\"  Total offender profiles: {latest_data['offender_profiles'].sum():,}\")\nprint(f\"  Total arrestee profiles: {latest_data['arrestee'].sum():,}\")\nprint(f\"  Total forensic profiles: {latest_data['forensic_profiles'].sum():,}\")\nprint(f\"  Total investigations aided: {latest_data['investigations_aided'].sum():,}\")\n\n\nLatest Statistics Summary:\n  As of: 2025-06-29 17:15:50\n  Data from: 2025-04-01 00:00:00\n  Jurisdictions reporting: 53\n  Total offender profiles: 18,431,162\n  Total arrestee profiles: 5,879,537\n  Total forensic profiles: 1,405,917\n  Total investigations aided: 730,426"
  },
  {
    "objectID": "analysis/ndis_scraping.html#visualizations",
    "href": "analysis/ndis_scraping.html#visualizations",
    "title": "NDIS Database Analysis",
    "section": "Visualizations",
    "text": "Visualizations\n\ndef create_visualizations(df_raw, df_fixed):\n    \"\"\"Create comprehensive visualizations\"\"\"\n    # Exclude D.C./Metro PD from visualizations\n    df_raw_viz = df_raw[df_raw['jurisdiction'] != 'D.C./Metro PD']\n    df_fixed_viz = df_fixed[df_fixed['jurisdiction'] != 'D.C./Metro PD']\n    \n    # Create figure with subplots\n    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n    \n    # 1. Jurisdictions reporting over time\n    for ax, (df, title_suffix) in zip([axes[0,0], axes[0,1]], \n                                      [(df_raw_viz, 'Raw'), (df_fixed_viz, 'Fixed')]):\n        jurisdictions_per_date = df.groupby('capture_datetime')['jurisdiction'].nunique()\n        ax.plot(jurisdictions_per_date.index, jurisdictions_per_date.values, 'b-', linewidth=2)\n        ax.set_title(f'Jurisdictions Reporting ({title_suffix})')\n        ax.set_ylabel('Number of Jurisdictions')\n        ax.grid(True, alpha=0.3)\n        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n    \n    # 2. Total investigations aided\n    for ax, (df, title_suffix) in zip([axes[1,0], axes[1,1]], \n                                      [(df_raw_viz, 'Raw with Typo'), (df_fixed_viz, 'Fixed')]):\n        total_inv = df.groupby('capture_datetime')['investigations_aided'].sum()\n        ax.plot(total_inv.index, total_inv.values / 1e3, 'purple', linewidth=2)\n        ax.set_title(f'Total Investigations Aided ({title_suffix})')\n        ax.set_ylabel('Thousands of Investigations')\n        ax.grid(True, alpha=0.3)\n        \n        # Highlight the typo in raw data\n        if 'Raw' in title_suffix:\n            typo_dates = df[(df['jurisdiction'] == 'California') & \n                          (df['investigations_aided'] &gt; 1000000)]['capture_datetime']\n            for date in typo_dates:\n                ax.axvline(x=date, color='red', linestyle='--', alpha=0.5)\n                ax.text(date, ax.get_ylim()[1]*0.9, 'Typo', rotation=90, \n                       verticalalignment='bottom', color='red')\n    \n    # 3. Data lag analysis\n    ax = axes[2, 0]\n    df_with_lag = df_fixed_viz[df_fixed_viz['data_as_of_date'].notna()].copy()\n    df_with_lag['data_lag_days'] = (df_with_lag['capture_datetime'] - df_with_lag['data_as_of_date']).dt.days\n    \n    avg_lag = df_with_lag.groupby('capture_datetime')['data_lag_days'].mean()\n    ax.plot(avg_lag.index, avg_lag.values, 'orange', linewidth=2)\n    ax.set_title('Average Data Lag (Capture Date vs \"As Of\" Date)')\n    ax.set_ylabel('Days')\n    ax.grid(True, alpha=0.3)\n    \n    # 4. California investigations over time (showing typo fix)\n    ax = axes[2, 1]\n    cal_raw = df_raw_viz[df_raw_viz['jurisdiction'] == 'California']\n    cal_fixed = df_fixed_viz[df_fixed_viz['jurisdiction'] == 'California']\n    \n    ax.plot(cal_raw['capture_datetime'], cal_raw['investigations_aided'], \n            'r-', label='Raw (with typo)', linewidth=2, alpha=0.7)\n    ax.plot(cal_fixed['capture_datetime'], cal_fixed['investigations_aided'], \n            'g-', label='Fixed', linewidth=2)\n    ax.set_title('California Investigations Aided: Raw vs Fixed')\n    ax.set_ylabel('Investigations Aided')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / 'ndis_analysis_complete.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Create visualizations\nprint(\"\\nCreating visualizations...\")\ncreate_visualizations(df_raw, df_fixed)\nprint(\"\\nProcessing complete!\")\n\n\nCreating visualizations...\n\n\n\n\n\n\n\n\n\n\nProcessing complete!"
  },
  {
    "objectID": "analysis/foia_processing.html#data-quality-issues-summary",
    "href": "analysis/foia_processing.html#data-quality-issues-summary",
    "title": "FOIA Document OCR Processing",
    "section": "Data Quality Issues Summary",
    "text": "Data Quality Issues Summary\n\n\n\n\n\n\n\n \nState\nIssue Type\nSeverity\nDescription\n\n\n\n\n0\nTexas\nGender\nCritical\nOnly female counts provided; male counts must be calculated\n\n\n1\nTexas\nRace\nCritical\nTotals don't match sum; 29 Offenders and 2,161 Arrestees missing\n\n\n2\nCalifornia\nRace\nMajor\nConvicted: 80.50% total; Arrestee: 87.22% total\n\n\n3\nIndiana\nRace\nMinor\n'&lt;1%' for Other race requires interpretation\n\n\n4\nFlorida\nGender\nMinor\nPercentages sum to 100.01% (rounding)\n\n\n5\nNevada\nTerminology\nNote\nUses 'flags' instead of 'profiles'\n\n\n6\nSouth Dakota\nComplexity\nNote\nProvides race×gender intersections (unique among states)"
  }
]