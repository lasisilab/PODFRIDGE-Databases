---
title: "NDIS Database Analysis"
subtitle: "Parsing FBI National DNA Index System Statistics from Wayback Machine"
author: "Tina Lasisi | Edited: João P. Donadio"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
execute:
  echo: true
  warning: false
  freeze: auto
---

## Pipeline Overview

1. [Introduction](#introduction)
    - [Objectives](#objectives)

2. [Setup & Configuration](#setup-configuration)  
    - [System Requirements](#system-requirements)  
    - [Project Structure](#project-structure)  

3. [Wayback Machine Snapshot Search](#WMSS)
    - [Scraping Method](#scrap-method)
    - [Technical Implementation](#technical-impl)
    - [Core Search Implementation](#core-search)
    - [Search Execution](#execution-search)

4. [Snapshot Downloader](#downloaderSnap)
    - [Download Methods](#download-method)
    - [Download Execution](#download-exec)
    - [Download Validation](#download-validation)

5. [Data Extraction](#extraction-pipe)
    - [Extraction Overview](#extraction-overview)
    - [Core Parser Functions](#parser-functions)
    - [Era-Specific Parsers](#era-parsers)
    - [Output Schema](#output-schema)
    - [Batch Processing](#batch-processing)
    - [Export & Validation](#exp-valid)

6. [Dataset Validation](#validation)
    - [Cross-language Validation](#cross-valid)
    - [Data Import and Validation](#import-valid)

7. [Dataset Cleaning](#datacleaning)
    - [New Columns](#new-columns)
    - [Jurisdiction Name Reconciliation](#jurisdic_names)
    - [Checking Data Growth](#data-growth)
    - [California Typo](#california-typo)
    - [DC/FBI Lab Typo](#dc-lab-typo)
    - [Oklahoma Typo](#oklahoma-typo)
    - [Export Cleaned Dataset](#export-dataset)

8. [Summary Statistics](#summarystats)
    
9. [Data Visualization](#visualiz)
    - [Time-Series Analysis](#time_series)
    - [Geospatial Mapping of Jurisdiction Participation](#geomap_juris)
    - [Interactive Table of Profile Growth](#interactive-explore)

10. [Conclusion](#conclusion)

## Introduction {#introduction}

The National DNA Index System (NDIS) is the central database that allows accredited forensic laboratories across the United States to electronically exchange and compare DNA profiles. Maintained by the FBI as part of CODIS (Combined DNA Index System), NDIS tracks the accumulation of DNA records contributed by federal, state, and local laboratories.

This project focuses on systematically reconstructing the growth and evolution of NDIS by parsing historical statistics published on the FBI’s website and preserved in the Internet Archive’s Wayback Machine. These snapshots contain tables reporting the number of DNA profiles stored in NDIS (offender, arrestee, forensic), as well as information on laboratory participation across jurisdictions.

### Objectives {#objectives}

1.  Develop a reproducible pipeline to extract NDIS statistics from archived FBI webpages in the Wayback Machine.

2.  Identify and correct inconsistencies and data quality issues across historical snapshots.

3.  Document the expansion of DNA profiles (offender, arrestee, forensic) over time.

4.  Analyze patterns of state and federal participation in NDIS.

## Setup and Configuration {#setup-configuration}

### System Requirements {#system-requirements}

**Required Packages:**

- Core: requests, beautifulsoup4, and lxml (scraping/parsing).

- Data/Visualization: pandas and tqdm (progress tracking).

```{python}
#| label: setup
#| warning: false
#| code-fold: true
#| code-summary: "Show Configuration code"
#| results: hide

import sys
import subprocess
import importlib

required_packages = [
    'requests',         # API/HTTP
    'beautifulsoup4',   # HTML parsing
    'lxml',             # Faster parsing (optional but recommended)
    'pandas',           # Data handling
    'tqdm',              # Progress bars
    'hashlib',
    'collections',
    'pathlib',
    'datetime',
    'os'
]

for package in required_packages:
    try:
        importlib.import_module(package)
        print(f"✓ {package} already installed")
    except ImportError:
        print(f"Installing {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
```

### Project Structure {#project-structure}

**Main Configurations:**

- Directory paths for raw HTML (wayback_html), metadata (wayback_meta), and outputs (ndis).

- Standardization mappings for jurisdiction names and known data typos.

```{python}
#| label: config
#| code-fold: true
#| code-summary: "Show Configuration code"
#| results: hold

from pathlib import Path
import re, json, requests, time, hashlib
from datetime import datetime
import pandas as pd
from bs4 import BeautifulSoup
from tqdm.auto import tqdm
from datetime import datetime
from collections import defaultdict
import os

# Configuration
BASE_DIR = Path("..")  # Project root directory
HTML_DIR = BASE_DIR / "raw" / "wayback_html"    # Storage for downloaded HTML
META_DIR = BASE_DIR / "raw" / "wayback_meta"    # Metadata storage
OUTPUT_DIR = BASE_DIR / "output" / "ndis"       # Processed data output

NDIS_SNAPSHOTS_DIR = HTML_DIR / "raw" / "ndis_snapshots"
NDIS_SNAPSHOTS_DIR.mkdir(parents=True, exist_ok=True)

# Create directory structure
for directory in [HTML_DIR, META_DIR, OUTPUT_DIR]:
    directory.mkdir(parents=True, exist_ok=True)
```

```{python}
#| echo: false

print(f"Project directories initialized:")
print(f"  - Working directory: {BASE_DIR.resolve()}")
print(f"  - HTML storage: {HTML_DIR}")
print(f"  - Metadata directory: {META_DIR}")
print(f"  - Output directory: {OUTPUT_DIR}")
```

## Wayback Machine Snapshot Search {#WMSS}

A function was developed to systematically search the Internet Archive's Wayback Machine for all preserved snapshots of FBI NDIS statistics pages using a comprehensive multi-phase approach.

### Scraping Method {#scrap-method}

1. **Multi-Phase Search Strategy**:

-   First searches for snapshots from the pre-2007 era using state-specific URLs

-   Then targets consolidated pages from post-2007 periods

-   Handles both HTTP and HTTPS protocol variants

2. **Intelligent Filtering**:

-   Only returns successful captures (HTTP 200)

-   Excludes non-HTML content (PDFs, images)

-   Limits to two snapshots per year per URL to manage data volume

-   Uses pagination with 5,000-result batches for comprehensive coverage

3.  **Robust Error Handling**:

-   Automatic retries with exponential backoff (1s → 2s → 4s delays)

-   Deduplicates results by timestamp

-   Failed requests are tracked and retried with increased retry attempts

-   Preserves complete error context for troubleshooting

### Technical Implementation {#technical-impl}

1.  API Request

2.  Converts JSON responses to clean DataFrame

3.  Maintains lookup of seen timestamps to prevent duplicates

4.  Sorts chronologically (oldest → newest)

### Core Search Implementation {#core-search}

- **make_request_with_retry()**: Implements exponential backoff (1s → 2s → 4s delays) for fault-tolerant API requests with configurable retry attempts.

- **should_keep_snapshot()**: Manages yearly snapshot limits (3 in different months) to prevent data overload while maintaining temporal coverage.

```{python}
#| label: snapshot-search1
#| echo: true
#| code-fold: true
#| code-summary: "Show search helpers function code"

def make_request_with_retry(params, max_retries=3, initial_delay=1):
    API_URL = "https://web.archive.org/cdx/search/cdx"
    delay = initial_delay
    for attempt in range(max_retries):
        try:
            resp = requests.get(API_URL, params=params, timeout=30)
            resp.raise_for_status()
            print(f"✓ Successful request for {params['url']} (offset: {params.get('offset', 0)})")
            return resp
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                print(f"✗ Final attempt failed for {params['url']} (offset: {params.get('offset', 0)}): {str(e)}")
                return None
            print(f"! Attempt {attempt+1} failed for {params['url']}, retrying in {delay} seconds...")
            time.sleep(delay)
            delay *= 2

def should_keep_snapshot(timestamp, url, yearly_snapshot_counts, max_per_year=3):
    """Check if we should keep the snapshot based on yearly limits and month uniqueness"""
    year = timestamp[:4]   # YYYY format
    month = timestamp[4:6] # MM format
    key = f"{url}_{year}"

    # Initialize if not present
    if key not in yearly_snapshot_counts:
        yearly_snapshot_counts[key] = {"count": 0, "months": set()}

    # Check yearly limit AND month uniqueness
    if (yearly_snapshot_counts[key]["count"] < max_per_year 
        and month not in yearly_snapshot_counts[key]["months"]):
        yearly_snapshot_counts[key]["count"] += 1
        yearly_snapshot_counts[key]["months"].add(month)
        return True
    
    return False
```

#### Pre-2007 (Clickmap Era)

![Pre-2007 Table Format](images/snapshot_2001.png)

- Each state/agency has its own page (e.g., ne.htm for Nebraska, dc.htm for DC/FBI Lab).

- The search iterates through the known two-letter codes and queries Wayback for each URL individually.

- **search_pre2007_snapshots()**: Searches individual state-specific pages using 50+ state codes (al.htm, ak.htm, etc.).

```{python}
#| label: snapshot-search2
#| echo: true
#| code-fold: true
#| code-summary: "Show pre-2007 search function code"

def search_pre2007_snapshots():
    state_codes = ["al", "ak", "az", "ar", "ca", "co", "ct", "de", "dc",
                   "fl", "ga", "hi", "id", "il", "in", "ia", "ks", "ky",
                   "la", "me", "md", "ma", "mi", "mn", "ms", "mo", "mt",
                   "ne", "nv", "nh", "nj", "nm", "ny", "nc", "nd", "oh",
                   "ok", "or", "pa", "pr", "ri", "sc", "sd", "tn", "tx",
                   "army", "ut", "vt", "va", "wa", "wv", "wi", "wy"]
    all_rows = []
    seen_timestamps = set()
    yearly_snapshot_counts = {} 
    total_saved = 0
    total_duplicates = 0
    failed_codes = []
    
    print(f"Starting pre-2007 snapshot search for {len(state_codes)} state codes at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # First pass - try all state codes
    for i, code in enumerate(state_codes, 1):
        url = f"http://www.fbi.gov/hq/lab/codis/{code}.htm"
        offset = 0
        state_snapshots = 0
        state_duplicates = 0
        has_more_results = True
        state_failed = False
        
        print(f"\n[{i}/{len(state_codes)}] Searching for {code.upper()} snapshots...")
        
        while has_more_results:
            params = {
                "url": url,
                "matchType": "exact",
                "output": "json",
                "fl": "timestamp,original,mimetype,statuscode",
                "filter": ["statuscode:200", "mimetype:text/html"],
                "limit": "5000",
                "offset": str(offset)
            }
            resp = make_request_with_retry(params)
            if not resp: 
                state_failed = True
                break
                
            data = resp.json()
            
            if len(data) <= 1:
                print(f"  No more results for {code.upper()} at offset {offset}")
                has_more_results = False
                break
                
            new_snapshots = 0
            for row in data[1:]:
                timestamp = row[0]
                if timestamp not in seen_timestamps:
                    if should_keep_snapshot(timestamp, url, yearly_snapshot_counts, max_per_year=2):
                        all_rows.append(row)
                        seen_timestamps.add(timestamp)
                        new_snapshots += 1
                        state_snapshots += 1
                        total_saved += 1
                    else:
                        state_duplicates += 1
                        total_duplicates += 1
            
            if new_snapshots > 0:
                print(f"  → Saved {new_snapshots} new snapshots for {code.upper()} (offset: {offset})")
            
            # Check if we've reached the end of results
            if len(data) < 5001:
                print(f"  Reached end of results for {code.upper()} at offset {offset}")
                has_more_results = False
            else:
                offset += 5000
                time.sleep(1)
        
        if state_failed:
            failed_codes.append(code)
            print(f"✗ State {code.upper()} failed, will retry later")
        elif state_snapshots > 0:
            print(f"✓ Found {state_snapshots} total snapshots for {code.upper()} ({state_duplicates} duplicates skipped)")
        else:
            print(f"✗ No snapshots found for {code.upper()}")
    
    # Retry failed codes
    if failed_codes:
        print(f"\nRetrying {len(failed_codes)} failed state codes...")
        for code in failed_codes:
            url = f"http://www.fbi.gov/hq/lab/codis/{code}.htm"
            offset = 0
            state_snapshots = 0
            state_duplicates = 0
            has_more_results = True
            
            print(f"\nRetrying {code.upper()}...")
            
            while has_more_results:
                params = {
                    "url": url,
                    "matchType": "exact",
                    "output": "json",
                    "fl": "timestamp,original,mimetype,statuscode",
                    "filter": ["statuscode:200", "mimetype:text/html"],
                    "limit": "5000",
                    "offset": str(offset)
                }
                resp = make_request_with_retry(params, max_retries=5, initial_delay=2)
                if not resp: 
                    print(f"✗ Final failure for {code.upper()}")
                    break
                    
                data = resp.json()
                
                if len(data) <= 1:
                    print(f"  No more results for {code.upper()} at offset {offset}")
                    has_more_results = False
                    break
                    
                new_snapshots = 0
                for row in data[1:]:
                    timestamp = row[0]
                    if timestamp not in seen_timestamps:
                        if should_keep_snapshot(timestamp, url, yearly_snapshot_counts, max_per_year=2):
                            all_rows.append(row)
                            seen_timestamps.add(timestamp)
                            new_snapshots += 1
                            state_snapshots += 1
                            total_saved += 1
                        else:
                            state_duplicates += 1
                            total_duplicates += 1
                
                if new_snapshots > 0:
                    print(f"  → Saved {new_snapshots} new snapshots for {code.upper()} (offset: {offset})")
                
                if len(data) < 5001:
                    print(f"  Reached end of results for {code.upper()} at offset {offset}")
                    has_more_results = False
                else:
                    offset += 5000
                    time.sleep(1)
            
            if state_snapshots > 0:
                print(f"✓ Found {state_snapshots} total snapshots for {code.upper()} on retry ({state_duplicates} duplicates skipped)")
            else:
                print(f"✗ Still no snapshots found for {code.upper()} after retry")
    
    print(f"\nPre-2007 search completed. Total snapshots saved: {total_saved}, Total duplicates found: {total_duplicates}")
    return pd.DataFrame(all_rows, columns=["timestamp","original","mimetype","status"]), total_duplicates
```

#### Post-2007 (Consolidated Pages)

![2008 Table Format](images/snapshot_2008.png)

![2025 Table Format](images/snapshot_2025.png)

- All state/agency records are on a single page per snapshot.

- Searches use the known consolidated URL patterns per era.

- **search_post2007_snapshots()**: Searches consolidated pages across 5 historical URL patterns with both HTTP and HTTPS variants.

```{python}
#| label: snapshot-search3
#| echo: true
#| code-fold: true
#| code-summary: "Show search function code"


def search_post2007_snapshots():
    urls = [
        "https://www.fbi.gov/hq/lab/codis/stats.htm",
        "https://www.fbi.gov/about-us/lab/codis/ndis-statistics",
        "https://www.fbi.gov/about-us/lab/biometric-analysis/codis/ndis-statistics",
        "https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics",
        "https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics"
    ]
    
    all_rows = []
    seen_timestamps = set()
    yearly_snapshot_counts = {}  # Track counts per year per URL
    protocols = ["http://", "https://"]
    total_saved = 0
    total_duplicates = 0
    failed_urls = []  # Track failed URLs for retry
    
    print(f"Starting post-2007 snapshot search for {len(urls)} URLs at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # First pass - try all URLs
    for i, base_url in enumerate(urls, 1):
        url_snapshots = 0
        url_duplicates = 0
        url_failed = False
        
        for protocol in protocols:
            current_url = base_url.replace("https://","").replace("http://","")
            full_url = f"{protocol}{current_url}"
            offset = 0
            has_more_results = True
            
            print(f"\n[{i}/{len(urls)}] Searching for {full_url} snapshots...")
            
            while has_more_results:
                params = {
                    "url": full_url,
                    "matchType": "exact",
                    "output": "json",
                    "fl": "timestamp,original,mimetype,statuscode",
                    "filter": ["statuscode:200", "mimetype:text/html"],
                    "limit": "5000",
                    "offset": str(offset)
                }
                resp = make_request_with_retry(params, max_retries=3)
                if not resp: 
                    url_failed = True
                    break
                    
                data = resp.json()
                
                # Check if we have results (header + at least one row)
                if len(data) <= 1:
                    print(f"  No more results for {full_url} at offset {offset}")
                    has_more_results = False
                    break
                    
                new_snapshots = 0
                for row in data[1:]:
                    timestamp = row[0]
                    if timestamp not in seen_timestamps:
                        # Check if we've reached the yearly limit for this URL
                        if should_keep_snapshot(timestamp, full_url, yearly_snapshot_counts, max_per_year=2):
                            all_rows.append(row)
                            seen_timestamps.add(timestamp)
                            new_snapshots += 1
                            url_snapshots += 1
                            total_saved += 1
                        else:
                            url_duplicates += 1
                            total_duplicates += 1
                            # Removed the "Yearly limit reached" message as requested
                
                if new_snapshots > 0:
                    print(f"  → Saved {new_snapshots} new snapshots for {full_url} (offset: {offset})")
                
                # Check if we've reached the end of results
                if len(data) < 5001:  # Fewer results than requested limit
                    print(f"  Reached end of results for {full_url} at offset {offset}")
                    has_more_results = False
                else:
                    offset += 5000
                    time.sleep(1)  # Reduced delay for faster processing
        
        if url_failed:
            failed_urls.append(base_url)
            print(f"✗ URL {base_url} failed, will retry later")
        elif url_snapshots > 0:
            print(f"✓ Found {url_snapshots} total snapshots for {base_url} ({url_duplicates} duplicates skipped)")
        else:
            print(f"✗ No snapshots found for {base_url}")
    
    # Retry failed URLs
    if failed_urls:
        print(f"\nRetrying {len(failed_urls)} failed URLs...")
        for base_url in failed_urls:
            url_snapshots = 0
            url_duplicates = 0
            
            for protocol in protocols:
                current_url = base_url.replace("https://","").replace("http://","")
                full_url = f"{protocol}{current_url}"
                offset = 0
                has_more_results = True
                
                print(f"\nRetrying {full_url}...")
                
                while has_more_results:
                    params = {
                        "url": full_url,
                        "matchType": "exact",
                        "output": "json",
                        "fl": "timestamp,original,mimetype,statuscode",
                        "filter": ["statuscode:200", "mimetype:text/html"],
                        "limit": "5000",
                        "offset": str(offset)
                    }
                    resp = make_request_with_retry(params, max_retries=5, initial_delay=2)
                    if not resp: 
                        print(f"✗ Final failure for {full_url}")
                        break
                        
                    data = resp.json()
                    
                    if len(data) <= 1:
                        print(f"  No more results for {full_url} at offset {offset}")
                        has_more_results = False
                        break
                        
                    new_snapshots = 0
                    for row in data[1:]:
                        timestamp = row[0]
                        if timestamp not in seen_timestamps:
                            if should_keep_snapshot(timestamp, full_url, yearly_snapshot_counts, max_per_year=2):
                                all_rows.append(row)
                                seen_timestamps.add(timestamp)
                                new_snapshots += 1
                                url_snapshots += 1
                                total_saved += 1
                            else:
                                url_duplicates += 1
                                total_duplicates += 1
                    
                    if new_snapshots > 0:
                        print(f"  → Saved {new_snapshots} new snapshots for {full_url} (offset: {offset})")
                    
                    if len(data) < 5001:
                        print(f"  Reached end of results for {full_url} at offset {offset}")
                        has_more_results = False
                    else:
                        offset += 5000
                        time.sleep(1)
            
            if url_snapshots > 0:
                print(f"✓ Found {url_snapshots} total snapshots for {base_url} on retry ({url_duplicates} duplicates skipped)")
            else:
                print(f"✗ Still no snapshots found for {base_url} after retry")
    
    print(f"\nPost-2007 search completed. Total snapshots saved: {total_saved}, Total duplicates found: {total_duplicates}")
    return pd.DataFrame(all_rows, columns=["timestamp","original","mimetype","status"]), total_duplicates
```

### Search Execution {#execution-search}

- Calls both pre-2007 and post-2007 search functions to retrieve comprehensive NDIS records.

- Combines results and removes duplicates based on timestamps.

- Stores technical details (timestamps, URL variants, duplicate counts) in structured JSON metadata.

- Provides detailed logging and progress tracking throughout the search process.

```{python}  
#| label: execute-search
#| echo: true
#| code-fold: true
#| code-summary: "Show search code"
#| results: hide

# Execute searches
pre2007_df, pre2007_duplicates = search_pre2007_snapshots()

post2007_df, post2007_duplicates = search_post2007_snapshots()

# Combine all snapshots
snap_df = pd.concat([pre2007_df, post2007_df]).drop_duplicates("timestamp").sort_values("timestamp").reset_index(drop=True)
total_duplicates = pre2007_duplicates + post2007_duplicates

# Save search metadata
search_meta = {
    "search_performed": datetime.now().isoformat(),
    "total_snapshots": len(snap_df),
    "total_duplicates_found": total_duplicates,
    "max_snapshots_per_year": 3,
    "time_span": {
        "first": snap_df["timestamp"].min(),
        "last": snap_df["timestamp"].max()
    },
    "url_variants": snap_df["original"].nunique()
}

with open(META_DIR / "search_metadata.json", "w") as f:
    json.dump(search_meta, f, indent=2)
```

```{python}
#| echo: false

# Print summary
unique_urls = snap_df['original'].unique()
formatted_urls = "\n  ".join(sorted(unique_urls))
print(f"""
Search Results Summary
=====================
Loaded {len(snap_df)} unique snapshots (max 3 per year per URL)
Total duplicates skipped: {total_duplicates}
Time coverage:
{snap_df['timestamp'].min()}
to
{snap_df['timestamp'].max()}
Unique URL patterns found: {snap_df['original'].nunique()}
Output saved to: {META_DIR.resolve()}/search_metadata.json
""")
```

## Snapshot Downloader {#downloaderSnap}

This system provides a robust method for downloading historical webpage snapshots from the Internet Archive's Wayback Machine, specifically designed for the FBI NDIS statistics pages.

### Download Methods {#download-method}

The download system implements a sequential approach optimized for reliability and respectful API usage:

- **Resilient Downloading**: Automatic retries with exponential backoff (2s → 4s → 8s → 16s → 32s delays) and extended 60-second timeouts for reliable network handling

- **Smart File Management**: Context-aware naming scheme using timestamp + state/scope identifier (e.g., 20040312_ne.html for pre-2007, 20150621_ndis.html for post-2007)

- **Duplicate Prevention**: Automatically skips already downloaded files to prevent redundant operations

- **Progress Tracking**: Real-time download status with completion counters and detailed success/failure reporting

- **Rate Limiting**: 1.5-second delays between requests to avoid overloading the Wayback Machine servers

```{python}
#| label: download-helper-codes
#| echo: true
#| code-fold: true
#| code-summary: "Show downloader helpers code"

def download_with_retry(url, save_path, max_retries=5, initial_delay=2):
    """
    Download an archived snapshot with retries and exponential backoff.
    """
    delay = initial_delay
    for attempt in range(max_retries):
        try:
            resp = requests.get(url, timeout=60)
            resp.raise_for_status()
            # Save to disk
            with open(save_path, "wb") as f:
                f.write(resp.content)
            print(f"✓ Downloaded: {save_path}")
            return True
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                print(f"✗ Final attempt failed for {url}: {str(e)}")
                return False
            print(f"! Attempt {attempt+1} failed, retrying in {delay}s...")
            time.sleep(delay)
            delay *= 2
    return False


def snapshot_to_filepath(row):
    """
    Map a snapshot record to a local filename.
    Format: {timestamp}_{state_or_scope}.html
    """
    ts = row["timestamp"]
    original = row["original"]
    
    # derive name from FBI URL pattern
    if "/codis/" in original and original.endswith(".htm"):
        # Pre-2007: use last part (state code or army/dc)
        suffix = Path(original).stem
    else:
        # Post-2007: consolidated page, use 'ndis'
        suffix = "ndis"
    
    save_dir = HTML_DIR / "raw" / "ndis_snapshots"
    save_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
    
    return save_dir / f"{ts}_{suffix}.html"
```

### Download Execution {#download-exec}

The download execution phase performs bulk retrieval of historical NDIS snapshots with comprehensive error handling:

- **Sequential Processing**: Iterates through snapshot DataFrame chronologically, processing each file individually for maximum reliability

- **URL Construction**: Uses identity flag (id_) in archive URLs to retrieve unmodified original content: https://web.archive.org/web/{timestamp}id_/{original}

- **Binary Preservation**: Saves files as binary content to maintain original encoding and prevent character corruption

- **Comprehensive Logging**: Provides real-time progress updates with attempt counters and final success/failure statistics

- **Flexible Limiting**: Optional download limits for testing or partial processing

```{python}
#| label: execute-download
#| echo: true
#| code-fold: true
#| code-summary: "Show download execution code"
#| results: hide

def download_snapshots(snap_df, limit=None):
    """
    Iterate over snapshot DataFrame and download archived HTML pages.
    """
    total = len(snap_df) if limit is None else min(limit, len(snap_df))
    print(f"Starting download of {total} snapshots at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    successes, failures = 0, 0
    for i, row in enumerate(snap_df.head(total).itertuples(index=False), 1):
        ts, original, _, _ = row
        save_path = snapshot_to_filepath(row._asdict())
        
        if save_path.exists():
            print(f"- [{i}/{total}] Already exists: {save_path}")
            successes += 1
            continue
        
        archive_url = f"https://web.archive.org/web/{ts}id_/{original}"
        print(f"- [{i}/{total}] Downloading {archive_url}")
        
        if download_with_retry(archive_url, save_path):
            successes += 1
        else:
            failures += 1
        
        time.sleep(1.5)  # polite delay
    
    print(f"\nDownload completed. Success: {successes}, Failures: {failures}")
    return successes, failures


successes, failures = download_snapshots(snap_df, limit=None)
```

### Download Validation {#download-validation}

Post-download validation ensures data integrity and identifies potential issues:

- **File Existence Verification**: Checks that all expected files were successfully downloaded to the target directory

- **Content Quality Assessment**: Validates HTML content by examining file headers for proper HTML tags

- **Error Categorization**: Separates missing files from corrupted/non-HTML files for targeted remediation

- **Metadata Generation**: Creates JSON validation reports with detailed statistics and file counts

- **Actionable Reporting**: Provides clear feedback on download success rates and files requiring attention

```{python}
#| label: verify-downloads
#| echo: true
#| code-fold: true
#| code-summary: "Show download validation code"
#| results: hold
#| output: false

def validate_downloads(snap_df):
    """
    Validate downloaded files exist and are HTML-like.
    """
    missing, bad_html = [], []
    
    for row in snap_df.itertuples(index=False):
        save_path = snapshot_to_filepath(row._asdict())
        if not save_path.exists():
            missing.append(save_path)
            continue
        try:
            with open(save_path, "rb") as f:
                start = f.read(200).lower()
            if b"<html" not in start:
                bad_html.append(save_path)
        except Exception:
            bad_html.append(save_path)
    
    print(f"Validation results → Missing: {len(missing)}, Bad HTML: {len(bad_html)}")
    return missing, bad_html

# Run validation
missing, bad_html = validate_downloads(snap_df)

# Save validation summary
validation_meta = {
    "validation_performed": datetime.now().isoformat(),
    "missing_files": len(missing),
    "bad_html_files": len(bad_html),
    "total_snapshots": len(snap_df),
    "successful_downloads": successes
}
with open(META_DIR / "validation_metadata.json", "w") as f:
    json.dump(validation_meta, f, indent=2)

```

```{python}
#| echo: false
# Save and print report
report_path = META_DIR / f"validation_metadata.json"
with open(report_path, 'w') as f:
    json.dump(validation_meta, f, indent=2)

print(f"\n{'='*60}")
print("DOWNLOAD VALIDATION REPORT")
print(f"{'='*60}")
print(f"  Missing files: {validation_meta['missing_files']}/{validation_meta['total_snapshots']}")
print(f"  Bad HTML files: {validation_meta['bad_html_files']}/{validation_meta['total_snapshots']}")
print(f"  Successful downloads: {validation_meta['successful_downloads']}/{validation_meta['total_snapshots']} ({validation_meta['successful_downloads']/validation_meta['total_snapshots']*100:.1f}%)")

print(f"\nFull report: {report_path}")

```

## Data Extraction {#extraction-pipe}

The data extraction pipeline converts downloaded HTML snapshots into structured tabular data, handling the evolution of FBI NDIS reporting formats across different time periods.

### Extraction Overview {#extraction-overview}
The extraction system processes three distinct eras of NDIS reporting:

1. Pre-2007 Era: Basic statistics without date metadata or arrestee data

2. 2007-2011 Era: Includes "as of" dates but no arrestee profiles

3. Post-2012 Era: Complete format with all profile types and consistent dating

**Key Features:**

- **Era-Aware Processing:** Automatically routes files to appropriate parsers based on timestamp

- **Metadata Recovery:** Extracts report dates from "as of" statements when available

- **Complete Traceability:** Links each record to its source HTML file and original URL

- **Robust Error Handling:** Processes files individually to prevent single failures from stopping the entire batch

### Core Parser Functions {#parser-functions}
Essential text processing utilities for NDIS data extraction:

- **HTML Cleaning**: Removes navigation, scripts, and styling elements to focus on data content

- **Date Extraction**: Identifies and parses "as of" dates using multiple pattern variations

- **Text Normalization**: Standardizes whitespace and jurisdiction name formatting

- **Encoding Handling**: Manages various character encodings found in historical snapshots

```{python}
#| label: helper-functions
#| echo: true
#| code-fold: true
#| code-summary: "Show setup and normalization functions code"

def extract_ndis_metadata(html_content):
    """
    Extract key metadata from NDIS HTML content including report dates
    
    Returns:
    --------
    dict:
        - report_month: Month from "as of" statement (None if not found)
        - report_year: Year from "as of" statement (None if not found)  
        - clean_text: Normalized text content
    """
    
    # Multiple patterns to catch different "as of" formats
    date_patterns = [
        r'[Aa]s of ([A-Za-z]+) (\d{4})',           # "as of November 2008"
        r'[Aa]s of ([A-Za-z]+) (\d{1,2}), (\d{4})', # "as of November 15, 2008"
        r'Statistics as of ([A-Za-z]+) (\d{4})',    # "Statistics as of November 2008"
        r'Statistics as of ([A-Za-z]+) (\d{1,2}), (\d{4})' # "Statistics as of November 15, 2008"
    ]
    
    report_month = None
    report_year = None
    
    # Find first occurrence of any date pattern
    for pattern in date_patterns:
        date_match = re.search(pattern, html_content)
        if date_match:
            month_str = date_match.group(1)
            if len(date_match.groups()) == 2:  # Month + Year only
                year_str = date_match.group(2)
            else:  # Month + Day + Year
                year_str = date_match.group(3)
            
            # Convert month name to number
            try:
                month_num = pd.to_datetime(f"{month_str} 1, 2000").month
                report_month = month_num
                report_year = int(year_str)
                break
            except:
                continue
    
    # Clean HTML and normalize text
    soup = BeautifulSoup(html_content, 'lxml')
    
    # Remove scripts, styles, and navigation elements
    for element in soup(['script', 'style', 'nav', 'header', 'footer']):
        element.decompose()
    
    # Get clean text with normalized whitespace
    clean_text = re.sub(r'\s+', ' ', soup.get_text(' ', strip=True))
    
    return {
        'report_month': report_month,
        'report_year': report_year, 
        'clean_text': clean_text
    }

def standardize_jurisdiction_name(name):
    """
    Clean and standardize jurisdiction names for consistency
    """
    if not name:
        return name
        
    # Remove common prefixes and suffixes
    name = re.sub(r'^.*?(Back to top|Tables by NDIS Participant|ation\.)\s*', 
                 '', name, flags=re.I).strip()
    
    # Standardize known variants
    replacements = {
        'D.C./FBI Lab': 'DC/FBI Lab',
        'D.C./Metro PD': 'DC/Metro PD', 
        'US Army': 'U.S. Army',
        'D.C.': 'DC'
    }
    
    for old, new in replacements.items():
        name = name.replace(old, new)
    
    return name.strip()

def extract_original_url_from_filename(html_file):
    """
    Reconstruct original URL from filename and timestamp
    """
    filename = html_file.name
    timestamp = filename.split('_')[0]  # Get timestamp part
    
    # Determine URL pattern based on filename suffix
    if filename.endswith('_ndis.html'):
        # Post-2007 consolidated format - use most common URL pattern
        return "https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics"
    else:
        # Pre-2007 state-specific format
        state_code = filename.split('_')[1].replace('.html', '')
        return f"http://www.fbi.gov/hq/lab/codis/{state_code}.htm"
```

### Era-Specific Parsers {#era-parsers}

Time-period-adapted parsing logic that accounts for format evolution:

**Pre-2007 Parser:**

- Extracts basic statistics from state-specific pages

- Uses timestamp-derived year (no report dates available)

- Sets arrestee counts to 0 (not reported in this era)

- Handles missing NDIS labs and investigations data

**2008-2011 Parser:**

- Processes consolidated pages with "Back to top" section dividers

- Extracts month and year from report dates

- Handles missing arrestee data (sets to 0)

- Multiple pattern matching for jurisdiction identification

**2012-2016 Parser:**

- First era with arrestee data extraction

- Processes consolidated pages with "Back to top" section dividers

- Multiple pattern matching for jurisdiction identification

- Complete jurisdiction coverage with standardized names

**Post-2017 Parser:**

- Modern format with consistent structure and all fields

- Robust regex pattern for reliable extraction

- Full feature extraction including arrestee profiles

- Complete jurisdiction coverage with standardized names

```{python}
#| label: parser-functions
#| echo: true
#| code-fold: true
#| code-summary: "Show parser functions code"

def parse_pre2007_ndis(text, timestamp, html_file, report_month=None, report_year=None):
    """
    Parse NDIS snapshots from 2001-2007 era (state-specific pages)
    HTML has table structure with "Statistical Information" and "Total" columns
    """
    records = []
    
    # Clean up the text for better matching
    text = re.sub(r'\s+', ' ', text)
    
    # Pattern for the state name (appears before "Statistical Information")
    state_pattern = r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+Statistical\s+Information'
    state_match = re.search(state_pattern, text)
    
    if state_match:
        jurisdiction = standardize_jurisdiction_name(state_match.group(1))
        
        # Extract individual values with more flexible patterns
        offender_match = re.search(r'Offender\s+Profiles?\s+([\d,]+)', text, re.IGNORECASE)
        forensic_match = re.search(r'Forensic\s+(?:Samples?|Profiles?)\s+([\d,]+)', text, re.IGNORECASE)
        ndis_labs_match = re.search(r'NDIS\s+Participating\s+Labs?\s+(\d+)', text, re.IGNORECASE)
        investigations_match = re.search(r'Investigations?\s+Aided\s+([\d,]+)', text, re.IGNORECASE)
        
        if offender_match and forensic_match:
            records.append({
                'timestamp': timestamp,
                'report_month': None,  # Not available pre-2007
                'report_year': None,   # Not available pre-2007
                'jurisdiction': jurisdiction,
                'offender_profiles': int(offender_match.group(1).replace(',', '')),
                'arrestee': 0,  # Not reported pre-2007
                'forensic_profiles': int(forensic_match.group(1).replace(',', '')),
                'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,
                'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0
            })
    
    return records


def parse_2008_2011_ndis(text, timestamp, html_file, report_month=None, report_year=None):
    """
    Parse NDIS snapshots from 2008-2011 era
    Consolidated page with state anchors and "Back to top" links
    No arrestee data in this period
    """
    records = []
    
    # Clean up the text
    text = re.sub(r'\s+', ' ', text)
    
    # Split by "Back to top" to isolate each state section
    sections = re.split(r'Back\s+to\s+top', text, flags=re.IGNORECASE)
    
    for section in sections:
        # Look for state name pattern (appears as anchor or bold text)
        # Try multiple patterns to catch different HTML formats
        jurisdiction = None
        
        # Pattern 1: <a name="State"></a><strong>State</strong>
        state_match = re.search(r'<a\s+name="([^"]+)"[^>]*>.*?(?:<strong>|<b>)\s*([A-Z][^<]+?)(?:</strong>|</b>)', section, re.IGNORECASE)
        if state_match:
            jurisdiction = state_match.group(2).strip()
        
        # Pattern 2: Just the state name in bold/strong tags before "Statistical Information"
        if not jurisdiction:
            state_match = re.search(r'(?:<strong>|<b>)\s*([A-Z][^<]+?)(?:</strong>|</b>).*?Statistical\s+Information', section, re.IGNORECASE)
            if state_match:
                jurisdiction = state_match.group(1).strip()
        
        # Pattern 3: State name without tags before "Statistical Information"
        if not jurisdiction:
            state_match = re.search(r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+Statistical\s+Information', section)
            if state_match:
                jurisdiction = state_match.group(1).strip()
        
        if jurisdiction:
            jurisdiction = standardize_jurisdiction_name(jurisdiction)
            
            # Extract values
            offender_match = re.search(r'Offender\s+Profiles?\s+([\d,]+)', section, re.IGNORECASE)
            forensic_match = re.search(r'Forensic\s+(?:Samples?|Profiles?)\s+([\d,]+)', section, re.IGNORECASE)
            ndis_labs_match = re.search(r'NDIS\s+Participating\s+Labs?\s+(\d+)', section, re.IGNORECASE)
            investigations_match = re.search(r'Investigations?\s+Aided\s+([\d,]+)', section, re.IGNORECASE)
            
            if offender_match and forensic_match:
                records.append({
                    'timestamp': timestamp,
                    'report_month': report_month,
                    'report_year': report_year,
                    'jurisdiction': jurisdiction,
                    'offender_profiles': int(offender_match.group(1).replace(',', '')),
                    'arrestee': 0,  # Not reported 2008-2011
                    'forensic_profiles': int(forensic_match.group(1).replace(',', '')),
                    'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,
                    'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0
                })
    
    return records


def parse_2012_2016_ndis(text, timestamp, html_file, report_month=None, report_year=None):
    """
    Parse NDIS snapshots from 2012-2016 era
    Includes arrestee data for the first time
    """
    records = []
    
    # Clean up the text
    text = re.sub(r'\s+', ' ', text)
    
    # Split by "Back to top" to isolate each state section
    sections = re.split(r'Back\s+to\s+top', text, flags=re.IGNORECASE)
    
    for section in sections:
        jurisdiction = None
        
        # Look for state name patterns
        # Pattern 1: <a name="State"></a><b>State</b>
        state_match = re.search(r'<a\s+name="([^"]+)"[^>]*>.*?<b>([^<]+?)</b>', section, re.IGNORECASE)
        if state_match:
            jurisdiction = state_match.group(2).strip()
        
        # Pattern 2: Just bold state name
        if not jurisdiction:
            state_match = re.search(r'<b>([A-Z][^<]+?)</b>.*?Statistical\s+Information', section, re.IGNORECASE)
            if state_match:
                jurisdiction = state_match.group(1).strip()
        
        # Pattern 3: State name without tags
        if not jurisdiction:
            state_match = re.search(r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+Statistical\s+Information', section)
            if state_match:
                jurisdiction = state_match.group(1).strip()
        
        if jurisdiction:
            jurisdiction = standardize_jurisdiction_name(jurisdiction)
            
            # Extract values INCLUDING arrestee which appears starting 2012
            offender_match = re.search(r'Offender\s+Profiles?\s+([\d,]+)', section, re.IGNORECASE)
            arrestee_match = re.search(r'Arrestee\s+([\d,]+)', section, re.IGNORECASE)
            forensic_match = re.search(r'Forensic\s+Profiles?\s+([\d,]+)', section, re.IGNORECASE)
            ndis_labs_match = re.search(r'NDIS\s+Participating\s+Labs?\s+(\d+)', section, re.IGNORECASE)
            investigations_match = re.search(r'Investigations?\s+Aided\s+([\d,]+)', section, re.IGNORECASE)
            
            if offender_match and forensic_match:
                records.append({
                    'timestamp': timestamp,
                    'report_month': report_month,
                    'report_year': report_year,
                    'jurisdiction': jurisdiction,
                    'offender_profiles': int(offender_match.group(1).replace(',', '')),
                    'arrestee': int(arrestee_match.group(1).replace(',', '')) if arrestee_match else 0,
                    'forensic_profiles': int(forensic_match.group(1).replace(',', '')),
                    'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,
                    'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0
                })
    
    return records


def parse_post2017_ndis(text, timestamp, html_file, report_month=None, report_year=None):
    """
    Parse NDIS snapshots from 2017+ era
    Modern format with consistent structure and all fields
    Keep using your existing working pattern for this era
    """
    records = []
    
    # This is your existing working pattern - don't change it
    pattern = re.compile(
        r'([A-Z][\w\s\.\-\'\/&\(\)]+?)Statistical Information'
        r'.*?Offender Profiles\s+([\d,]+)'
        r'.*?Arrestee\s+([\d,]+)'
        r'.*?Forensic Profiles\s+([\d,]+)'
        r'.*?NDIS Participating Labs\s+(\d+)'
        r'.*?Investigations Aided\s+([\d,]+)',
        re.IGNORECASE | re.DOTALL
    )
    
    for match in pattern.finditer(text):
        records.append({
            'timestamp': timestamp,
            'report_month': report_month,
            'report_year': report_year,
            'jurisdiction': standardize_jurisdiction_name(match.group(1)),
            'offender_profiles': int(match.group(2).replace(',', '')),
            'arrestee': int(match.group(3).replace(',', '')),
            'forensic_profiles': int(match.group(4).replace(',', '')),
            'ndis_labs': int(match.group(5)),
            'investigations_aided': int(match.group(6).replace(',', ''))
        })
    
    return records
```

### Output Schema {#output-schema}

Each extracted record contains the following standardized fields:

| Field | Description | Availability |
| :--- | :--- | :--- |
| `timestamp` | Wayback capture timestamp (YYYYMMDDHHMMSS) | All eras |
| `report_month` | Month from "as of" statement | 2007+ only |
| `report_year` | Year from "as of" statement | 2007+ only |
| `jurisdiction` | Standardized state/agency name | All eras |
| `offender_profiles` | DNA profiles from convicted offenders | All eras |
| `arrestee` | DNA profiles from arrestees | 2012+ only |
| `forensic_profiles` | Crime scene DNA profiles | All eras |
| `ndis_labs` | Number of participating laboratories | All eras |
| `investigations_aided` | Cases assisted by DNA matches | All eras |

### Batch Processing {#batch-processing}

The complete extraction workflow:

**File Discovery**

- Scans download directory for HTML files

- Sorts chronologically for consistent processing

- Tracks progress with detailed logging

**Individual File Processing**

- Reads HTML content with encoding fallback

- Extracts metadata and cleans content

- Routes to era-appropriate parser based on timestamp

- Captures source file information for traceability

**Data Consolidation**

- Combines all records into single DataFrame

- Adds derived timestamp columns (capture_date, year)

- Validates data integrity and completeness

- Sorts by capture date and jurisdiction for consistency

```{python}
#| label: batch-processing
#| echo: true
#| code-fold: true
#| code-summary: "Show batch processing function code"

def process_ndis_snapshot(html_file):
    """
    Convert single NDIS HTML file to structured data
    Routes to appropriate parser based on timestamp year
    """
    try:
        # Read HTML content with encoding fallback
        try:
            content = html_file.read_text(encoding='utf-8')
        except UnicodeDecodeError:
            content = html_file.read_text(encoding='latin-1', errors='ignore')
        
        # Extract metadata
        metadata = extract_ndis_metadata(content)
        timestamp = html_file.stem.split('_')[0]  # Get timestamp from filename
        year = int(timestamp[:4])
        
        # Route to appropriate parser based on timestamp year with MORE GRANULAR RANGES
        if year <= 2007:
            return parse_pre2007_ndis(
                metadata['clean_text'], timestamp, html_file,
                metadata['report_month'], metadata['report_year']
            )
        elif year <= 2011:  # Changed from 2012 to 2011
            return parse_2008_2011_ndis(  # Renamed function
                metadata['clean_text'], timestamp, html_file,
                metadata['report_month'], metadata['report_year']
            )
        elif year <= 2016:  # New range for 2012-2016
            return parse_2012_2016_ndis(  # New function for this period
                metadata['clean_text'], timestamp, html_file,
                metadata['report_month'], metadata['report_year']
            )
        else:  # 2017 and later
            return parse_post2017_ndis(  
                metadata['clean_text'], timestamp, html_file,
                metadata['report_month'], metadata['report_year']
            )
            
    except Exception as e:
        print(f"Error processing {html_file.name}: {str(e)}")
        return []

def process_all_snapshots():
    """
    Process all downloaded snapshots into a single DataFrame
    
    Returns:
    --------
    pd.DataFrame
        Combined dataset with all snapshots
    """
    all_records = []
    html_files = sorted(NDIS_SNAPSHOTS_DIR.glob("*.html"))
    
    print(f"Processing {len(html_files)} HTML snapshots...")
    
    successful_files = 0
    failed_files = 0
    
    for html_file in tqdm(html_files, desc="Extracting NDIS data", mininterval=3600): 
        records = process_ndis_snapshot(html_file)
        if records:
            all_records.extend(records)
            successful_files += 1
        else:
            failed_files += 1
    
    print(f"Processing complete: {successful_files} successful, {failed_files} failed")
    
    if not all_records:
        print("Warning: No records extracted!")
        return pd.DataFrame()
    
    df = pd.DataFrame(all_records)
    
    df['capture_date'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S')
    df['capture_year'] = df['capture_date'].dt.year
    
    return df.sort_values(['capture_date', 'jurisdiction']).reset_index(drop=True)

```

### Export & Validation {#exp-valid}

Structured output generation with comprehensive quality control:

**Validation Checks:**

- Verifies all required columns are present

- Checks for null values in critical fields

- Validates numeric ranges (non-negative counts)

- Confirms timestamp format consistency

- Ensures jurisdiction names contain valid characters

**Export Features**

- Saves as UTF-8 encoded CSV for maximum compatibility

- Generates timestamped filenames for version control

- Creates metadata summary with file statistics

- Performs round-trip validation to confirm data integrity

**Quality Metrics**

- Records total file count and processing success rate

- Tracks temporal coverage (earliest to latest snapshots)

- Documents jurisdiction coverage across time periods

- Reports data completeness by era and field

```{python}
#| label: export-data
#| echo: true
#| code-fold: true
#| code-summary: "Show execution function code"

def export_ndis_data(df, output_dir=OUTPUT_DIR):
    """
    Export processed NDIS data with comprehensive metadata
    """
    if df.empty:
        print("Warning: DataFrame is empty, skipping export")
        return None
    
    # Generate output filename
    export_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = output_dir / f"ndis_data_raw.csv"
    
    # Export main dataset
    df.to_csv(csv_path, index=False, encoding='utf-8')
    
    # Calculate export metadata
    file_size_mb = csv_path.stat().st_size / (1024 * 1024)
    
    export_metadata = {
        'export_timestamp': export_timestamp,
        'export_path': str(csv_path.resolve()),
        'record_count': len(df),
        'file_size_mb': round(file_size_mb, 2),
        'unique_snapshots': df['timestamp'].nunique(),
        'unique_jurisdictions': df['jurisdiction'].nunique(),
        'date_coverage': {
            'earliest_capture': df['capture_date'].min().isoformat(),
            'latest_capture': df['capture_date'].max().isoformat(),
            'span_years': df['capture_year'].max() - df['capture_year'].min() + 1
        },
        'data_completeness': {
            'with_report_dates': len(df[df['report_year'].notna()]),
            'with_arrestee_data': len(df[df['arrestee'] > 0]),
            'total_investigations_aided': int(df['investigations_aided'].sum())
        }
    }
    
    # Save metadata
    metadata_path = output_dir / f"ndis_export_metadata_{export_timestamp}.json"
    import json
    with open(metadata_path, 'w') as f:
        json.dump(export_metadata, f, indent=2, default=str)
    
    print(f"✓ Data exported: {csv_path}")
    print(f"✓ Metadata saved: {metadata_path}")
    print(f"✓ Export summary: {len(df):,} records, {file_size_mb:.1f} MB")
    
    return export_metadata
```

**Data integrity validation**

- **Schema Checking:** Ensures proper field types and formats.

- **Null Validation:** Confirms mandatory fields are populated.

- **Value Sanity Checks:** Verifies non-negative numbers.

```{python}
#| label: verify-export
#| echo: true
#| code-fold: true
#| code-summary: "Show validation function code"

def validate_extracted_data(df):
    """
    Comprehensive validation of extracted NDIS data
    """
    print("Validating extracted data...")
    
    # Required columns check
    required_cols = [
        'timestamp', 'jurisdiction',
        'offender_profiles', 'arrestee', 'forensic_profiles', 
        'ndis_labs', 'investigations_aided'
    ]
    
    validation_results = {}
    
    # Check column presence
    missing_cols = set(required_cols) - set(df.columns)
    validation_results['missing_columns'] = list(missing_cols)
    
    if missing_cols:
        print(f"✗ Missing required columns: {missing_cols}")
        return validation_results
    
    # Data quality checks
    validation_results['total_records'] = len(df)
    validation_results['unique_timestamps'] = df['timestamp'].nunique()
    validation_results['unique_jurisdictions'] = df['jurisdiction'].nunique()
    validation_results['date_range'] = {
        'earliest': df['capture_date'].min().strftime('%Y-%m-%d'),
        'latest': df['capture_date'].max().strftime('%Y-%m-%d')
    }
    
    # Null value checks
    critical_nulls = df[['jurisdiction', 'offender_profiles', 'forensic_profiles']].isnull().sum()
    validation_results['critical_nulls'] = critical_nulls.to_dict()
    
    # Value range checks
    numeric_cols = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']
    negative_values = {}
    for col in numeric_cols:
        negative_count = (df[col] < 0).sum()
        if negative_count > 0:
            negative_values[col] = negative_count
    validation_results['negative_values'] = negative_values
    
    # Era-specific validation
    pre2007_count = len(df[df['capture_year'] < 2007])
    arrestee_pre2012 = len(df[(df['capture_year'] < 2012) & (df['arrestee'] > 0)])
    
    validation_results['era_checks'] = {
        'pre2007_records': pre2007_count,
        'arrestee_before_2012': arrestee_pre2012  # Should be 0
    }
    
    # Print summary
    print(f"✓ Total records: {validation_results['total_records']:,}")
    print(f"✓ Unique snapshots: {validation_results['unique_timestamps']}")
    print(f"✓ Unique jurisdictions: {validation_results['unique_jurisdictions']}")
    print(f"✓ Date range: {validation_results['date_range']['earliest']} to {validation_results['date_range']['latest']}")
    
    if validation_results['critical_nulls']:
        print(f"! Critical null values found: {validation_results['critical_nulls']}")
    
    if validation_results['negative_values']:
        print(f"! Negative values found: {validation_results['negative_values']}")
    
    if validation_results['era_checks']['arrestee_before_2012'] > 0:
        print(f"! Data integrity issue: {validation_results['era_checks']['arrestee_before_2012']} arrestee records found before 2012")
    
    return validation_results
```

#### Extraction Execution

```{python}
#| label: extract-execution
#| echo: true
#| code-fold: true
#| code-summary: "Show main execution code"

if __name__ == "__main__":
    # Process all snapshots
    ndis_data = process_all_snapshots()
    
    if not ndis_data.empty:
        # Validate data quality
        validation_results = validate_extracted_data(ndis_data)
        
        # Export if validation passes
        export_metadata = export_ndis_data(ndis_data)
        print("\n" + "="*50)
        print("EXTRACTION COMPLETE")
        print("="*50)
        print(f"Records extracted: {len(ndis_data):,}")
        print(f"Time span: {ndis_data['capture_year'].min()}-{ndis_data['capture_year'].max()}")
        print(f"Jurisdictions: {ndis_data['jurisdiction'].nunique()}")
    else:
        print("No data extracted - check HTML files and parsing logic")
```

## Dataset Validation {#validation}

### Cross-language Validation {#cross-valid}

This section prepares the environment for cross-language analysis by:

1. Ensuring all required R packages are available

2. Loading the NDIS dataset with proper type specifications

3. Providing basic data validation checks

```{r}
#| label: packages-setup
#| echo: true
#| code-fold: true
#| code-summary: "Show setup code"

# List of required packages
required_packages <- c(
  "tidyverse",    # Data manipulation and visualization
  "lubridate",    # Date-time manipulation
  "DT",           # Interactive tables
  "plotly",       # Interactive visualizations
  "leaflet",      # Geospatial mapping
  "kableExtra",   # Enhanced table formatting
  "scales",       # Axis scaling and formatting
  "dlookr",       # Data validation and diagnostics
  "gt",           # Table generation
  "assertr",      # Data validation and assertions
  "flextable",    # Enhanced table visualization
  "ggridges",     # Ridge plots
  "here",         # File path management
  "patchwork",    # Data visualization  
  "scales"        # Plot aesthetics
  )

# Function to install missing packages
install_missing <- function(packages) {
  for (pkg in packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      message(paste("Installing missing package:", pkg))
      install.packages(pkg, dependencies = TRUE)
    }
  }
}

# Install any missing packages
install_missing(required_packages)

# Load all packages
suppressPackageStartupMessages({
  library(tidyverse)
  library(lubridate)
  library(DT)
  library(plotly)
  library(leaflet)
  library(kableExtra)
  library(scales)
  library(dlookr)
  library(gt)
  library(assertr)
  library(flextable)
  library(ggridges)
  library(here)
  library(patchwork)
  library(scales)
})

# Verify all packages loaded successfully
loaded_packages <- sapply(required_packages, require, character.only = TRUE)

if (all(loaded_packages)) {
  message("All packages loaded successfully!")
} else {
  warning("The following packages failed to load: ", 
          paste(names(loaded_packages)[!loaded_packages], collapse = ", "))
}
```

### Data Import and Validation {#import-valid}

```{r}
#| label: csv-reading
#| echo: true
#| code-fold: true
#| code-summary: "Show data import code"

# Define expected column structure
expected_cols <- cols(
  timestamp = col_character(),
  report_month = col_character(),
  report_year = col_character(),
  jurisdiction = col_character(),
  offender_profiles = col_double(),
  arrestee = col_double(),
  forensic_profiles = col_double(),
  ndis_labs = col_double(),
  investigations_aided = col_double()
)

# Read data with validation
ndis_data <- read_csv(
  here::here("output", "ndis", "ndis_data_raw.csv"),
  col_types = expected_cols
)

```

```{r}
#| echo: false

enhanced_glimpse <- function(df) {
  glimpse_data <- data.frame(
    Column = names(df),
    Type = sapply(df, function(x) paste(class(x), collapse = ", ")),
    Rows = nrow(df),
    Missing = sapply(df, function(x) sum(is.na(x))),
    Unique = sapply(df, function(x) length(unique(x))),
    First_Values = sapply(df, function(x) {
      if(is.numeric(x)) {
        paste(round(head(x, 3), 2), collapse = ", ")
      } else {
        paste(encodeString(head(as.character(x), 3)), collapse = ", ")
      }
    })
  )
  
  ft <- flextable(glimpse_data) %>%
    theme_zebra() %>%
    set_caption(paste("Enhanced Data Glimpse:", deparse(substitute(df)))) %>%
    autofit() %>%
    align(align = "left", part = "all") %>%
    colformat_num(j = c("Rows", "Missing", "Unique"), big.mark = "") %>%
    bg(j = "Missing", bg = function(x) ifelse(x > 0, "#FFF3CD", "transparent")) %>%
    bg(j = "Unique", bg = function(x) ifelse(x == 1, "#FFF3CD", "transparent")) %>%
    add_footer_lines(paste("Data frame dimensions:", nrow(df), "rows ×", ncol(df), "columns")) %>%
    fontsize(size = 10, part = "all") %>%
    set_table_properties(layout = "autofit", width = 1)
  
  return(ft)
}

enhanced_glimpse(ndis_data)

```

## Dataset Cleaning {#datacleaning}

### New columns {#new-columns}
    - `total_profiles`: Sum of all profile types
    - `profiles_per_lab`: Average profiles per lab

```{r}
#| label: clean-types
#| echo: true
#| code-fold: true
#| code-summary: "Show cleaning code"

ndis_clean <- ndis_data %>%
  filter(ndis_labs > 0) %>%
  mutate(
    timestamp = as_datetime(timestamp, format = "%Y%m%d%H%M%S"),
    capture_month = month(timestamp),
    total_profiles = offender_profiles + arrestee + forensic_profiles,
    profiles_per_lab = total_profiles / ndis_labs
  ) %>%
  # Fill NAs in report_month and report_year
  mutate(
    report_month = ifelse(is.na(report_month), capture_month, report_month),
    report_year = ifelse(is.na(report_year), capture_year, report_year),
    report_year = as.integer(report_year),
    report_month = as.integer(report_month),
    report_year = as.factor(report_year),
    report_month = as.factor(report_month)
  ) %>%
  select(-capture_date, -capture_year, -capture_month) %>%
  select(
    timestamp, report_month, report_year, jurisdiction,
    offender_profiles, arrestee, forensic_profiles, total_profiles, ndis_labs, profiles_per_lab,
    investigations_aided, everything()
  ) %>%
  arrange(jurisdiction, timestamp)

```

### Jurisdiction Name Reconciliation {#jurisdic_names}

This section reconciles jurisdiction names between the NDIS dataset and a reference dataset of US jurisdictions, ensuring consistency for analysis.

Before:

```{r}
#| echo: false

levels(as.factor(ndis_data$jurisdiction))
```


```{r}
#| label: jurisdiction-clean
#| echo: true
#| code-fold: true
#| code-summary: "Show cleaning code (jurisdiction)"
#| results: hide

# Clean jurisdiction names with Alabama-specific patterns
ndis_clean <- ndis_clean %>%
  mutate(
    jurisdiction = case_when(
      # Standard state names
      str_detect(jurisdiction, "Alabama$|Alabama Stats") ~ "Alabama",
      str_detect(jurisdiction, "Alaska$|Alaska Stats") ~ "Alaska",
      str_detect(jurisdiction, "Arizona$|Arizona Stats") ~ "Arizona",
      str_detect(jurisdiction, "Arkansas$|Arkansas Stats") ~ "Arkansas",
      str_detect(jurisdiction, "California$|California Stats") ~ "California",
      str_detect(jurisdiction, "Colorado$|Colorado Stats") ~ "Colorado",
      str_detect(jurisdiction, "Connecticut$|Connecticut Stats") ~ "Connecticut",
      str_detect(jurisdiction, "Delaware$|Delaware Stats") ~ "Delaware",
      str_detect(jurisdiction, "Florida$|Florida Stats") ~ "Florida",
      str_detect(jurisdiction, "Georgia$|Georgia Stats") ~ "Georgia",
      str_detect(jurisdiction, "Hawaii$|Hawaii Stats") ~ "Hawaii",
      str_detect(jurisdiction, "Idaho$|Idaho Stats") ~ "Idaho",
      str_detect(jurisdiction, "Illinois$|Illinois Stats") ~ "Illinois",
      str_detect(jurisdiction, "Indiana$|Indiana Stats") ~ "Indiana",
      str_detect(jurisdiction, "Iowa$|Iowa Stats") ~ "Iowa",
      str_detect(jurisdiction, "Kansas$|Kansas Stats") ~ "Kansas",
      str_detect(jurisdiction, "Kentucky$|Kentucky Stats") ~ "Kentucky",
      str_detect(jurisdiction, "Louisiana$|Louisiana Stats") ~ "Louisiana",
      str_detect(jurisdiction, "Maine$|Maine Stats") ~ "Maine",
      str_detect(jurisdiction, "Maryland$|Maryland Stats") ~ "Maryland",
      str_detect(jurisdiction, "Massachusetts$|Massachusetts Stats") ~ "Massachusetts",
      str_detect(jurisdiction, "Michigan$|Michigan Stats") ~ "Michigan",
      str_detect(jurisdiction, "Minnesota$|Minnesota Stats") ~ "Minnesota",
      str_detect(jurisdiction, "Mississippi$|Mississippi Stats") ~ "Mississippi",
      str_detect(jurisdiction, "Missouri$|Missouri Stats") ~ "Missouri",
      str_detect(jurisdiction, "Montana$|Montana Stats") ~ "Montana",
      str_detect(jurisdiction, "Nebraska$|Nebraska Stats") ~ "Nebraska",
      str_detect(jurisdiction, "Nevada$|Nevada Stats") ~ "Nevada",
      str_detect(jurisdiction, "New Hampshire$|New Hampshire Stats") ~ "New Hampshire",
      str_detect(jurisdiction, "New Jersey$|New Jersey Stats") ~ "New Jersey",
      str_detect(jurisdiction, "New Mexico$|New Mexico Stats|Mexico Stats") ~ "New Mexico",
      str_detect(jurisdiction, "New York$|New York Stats") ~ "New York",
      str_detect(jurisdiction, "North Carolina$|North Carolina Stats") ~ "North Carolina",
      str_detect(jurisdiction, "North Dakota$|North Dakota Stats") ~ "North Dakota",
      str_detect(jurisdiction, "Ohio$|Ohio Stats") ~ "Ohio",
      str_detect(jurisdiction, "Oklahoma$|Oklahoma Stats") ~ "Oklahoma",
      str_detect(jurisdiction, "Oregon$|Oregon Stats") ~ "Oregon",
      str_detect(jurisdiction, "Pennsylvania$|Pennsylvania Stats") ~ "Pennsylvania",
      str_detect(jurisdiction, "Rhode Island$|Rhode Island Stats") ~ "Rhode Island",
      str_detect(jurisdiction, "South Carolina$|South Carolina Stats") ~ "South Carolina",
      str_detect(jurisdiction, "South Dakota$|South Dakota Stats") ~ "South Dakota",
      str_detect(jurisdiction, "Tennessee$|Tennessee Stats") ~ "Tennessee",
      str_detect(jurisdiction, "Texas$|Texas Stats") ~ "Texas",
      str_detect(jurisdiction, "Utah$|Utah Stats") ~ "Utah",
      str_detect(jurisdiction, "Vermont$|Vermont Stats") ~ "Vermont",
      str_detect(jurisdiction, "West Virginia$|West Virginia Stats") ~ "West Virginia",
      str_detect(jurisdiction, "Virginia$|Virginia Stats") ~ "Virginia",
      str_detect(jurisdiction, "Washington$|Washington State Stats") ~ "Washington",
      str_detect(jurisdiction, "Wisconsin$|Wisconsin Stats") ~ "Wisconsin",
      str_detect(jurisdiction, "Wyoming$|Wyoming Stats") ~ "Wyoming",
      
      # Special jurisdictions
      str_detect(jurisdiction, "DC/FBI|Washington DC Stats") ~ "DC/FBI Lab",
      str_detect(jurisdiction, "DC/Metro") ~ "DC/Metro PD",
      str_detect(jurisdiction, "U.S. Army$|U.S. Army Stats") ~ "U.S. Army",
            str_detect(jurisdiction, "Puerto Rico$|Puerto Rico Stats") ~ "Puerto Rico",
      
      # Handle DC explicitly
      str_detect(jurisdiction, "^DC$") ~ "DC/Metro PD",
      
      # Handle miscellaneous cases
      str_detect(jurisdiction, "Tables by NDIS Participant") ~ "Alabama", # Default to Alabama
      str_detect(jurisdiction, "investigations. Select a state") ~ NA_character_, # Remove these
      
      TRUE ~ jurisdiction
    ),
    
    # Clean up any remaining whitespace
    jurisdiction = str_trim(jurisdiction)
     ) %>%
  
  # Filter out NA jurisdictions
  filter(!is.na(jurisdiction)) %>%
  
  # Convert to factor with the 54 levels you want
  mutate(
    jurisdiction = factor(jurisdiction,
                         levels = c(sort(state.name), "Puerto Rico", "DC/FBI Lab", "DC/Metro PD", "U.S. Army")))

ndis_clean <- ndis_clean %>%
  filter(!is.na(jurisdiction))

```

After:
```{r}
#| label: jurisdiction-clean2
#| echo: false
#| eval: true

levels(ndis_clean$jurisdiction)

```

### Checking Data Growth {#data-growth}

Verifying if investigations aided by the CODIS database increases over time (as expected).


```{r}
#| label: growth-clean
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "Show cleaning code (data growth)"

# Data prep

growth_data <- ndis_clean %>%
  mutate(year = report_year) %>%
  group_by(jurisdiction, year) %>%
  summarise(max_investigations = max(investigations_aided, na.rm = TRUE),
    .groups = 'drop')

# Static ggplot for the national total
national_trend <- growth_data %>%
  group_by(year) %>%
  summarise(national_total = sum(max_investigations, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = national_total)) +
  geom_line(linewidth = 1.5, color = "darkblue", alpha = 0.8) +
  geom_point(size = 2, color = "darkblue") +
  labs(
    title = "National Total: Investigations Aided by DNA Matches",
    x = "Year",
    y = "Total Investigations Aided (Sum of Jurisdiction Maxima)",
    caption = "Source: FBI CODIS Monthly Statistics"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma)

national_trend

# Interactive plot for jurisdiction-level exploration
interactive_plot <- growth_data %>%
  ggplot(aes(x = year, y = max_investigations, group = jurisdiction, color = jurisdiction,
             text = paste("Jurisdiction:", jurisdiction,
                          "<br>Year:", year,
                          "<br>Investigations Aided:", scales::comma(max_investigations)))) +
  geom_line(alpha = 0.7) +
  geom_point(size = 0.8, alpha = 0.8) +
  labs(
    title = "Investigations Aided by Jurisdiction",
    x = "Year",
    y = "Max Investigations Aided Reported",
    color = "Jurisdiction"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

ggplotly(interactive_plot, tooltip = "text") %>%
  layout(hoverlabel = list(bgcolor = "white")) %>%
  config(displayModeBar = TRUE)

```

### California Typo {#california-typo}

- Identified an incorrect value in Investigations Aided for California in 2024 (an outlier > 1,000,000).

- Extracted the affected row to verify the issue.

- Removed the erroneous entry by filtering out values greater than 1,000,000 in investigations_aided.

```{r}
#| label: california-typo
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "Show typo resolution code"

california_2024 <- ndis_clean %>%
  filter(jurisdiction == "California" & report_year == "2024") %>%
  select(report_year, report_month, jurisdiction, investigations_aided)

flextable(california_2024) %>%
  theme_zebra() %>%
  autofit()

ndis_clean <- ndis_clean %>%
  filter(!(investigations_aided > 1000000))

```


### DC/FBI Lab Typo {#dc-lab-typo}

While exploring the data interactively, an outlier was identified in Offender Profiles for DC/FBI Lab in 2024.

```{r}
#| label: dc-typo
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "Show typo resolution code"

growth_data <- ndis_clean %>%
  mutate(year = report_year) %>%
  group_by(jurisdiction, year) %>%
  summarise(max_offenders = max(offender_profiles, na.rm = TRUE),
    .groups = 'drop')

# Interactive plot for jurisdiction-level exploration
interactive_plot <- growth_data %>%
  ggplot(aes(x = year, y = max_offenders, group = jurisdiction, color = jurisdiction,
             text = paste("Jurisdiction:", jurisdiction,
                          "<br>Year:", year,
                          "<br>Max Offenders:", scales::comma(max_offenders)))) +
  geom_line(alpha = 0.7) +
  geom_point(size = 0.8, alpha = 0.8) +
  labs(
    title = "Offender profiles by Jurisdiction",
    x = "Year",
    y = "Max Offender profiles Reported",
    color = "Jurisdiction"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

ggplotly(interactive_plot, tooltip = "text") %>%
  layout(hoverlabel = list(bgcolor = "white")) %>%
  config(displayModeBar = TRUE)

typo <- ndis_clean %>%
  filter(jurisdiction == "DC/FBI Lab" & as.numeric(as.character(report_year)) %in% 2023:2025) %>%
  select(report_year, report_month, jurisdiction, offender_profiles)

flextable(typo) %>%
  theme_zebra() %>%
  autofit()

```

- Manual verification showed a typo in the NDIS system.

- To resolve this, the dataset was manually corrected to reflect the verified values.

```{r}
#| label: dcfbi-typo-fix
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "Show typo correction code"

# Incorrect system values
system_values <- data.frame(
  Jurisdiction = "DC/FBI Lab",
  Period = "As of October 2024",
  Offender_Profiles = 940870,
  Arrestee_Profiles = 510516,
  Forensic_Profiles = 11188,
  NDIS_Labs = 2,
  Investigations_Aided = 1495
)

# Corrected values (manually verified)
corrected_values <- data.frame(
  Jurisdiction = "DC/FBI Lab",
  Period = "As of February 2024",
  Offender_Profiles = 2539381,
  Arrestee_Profiles = 494664,
  Forensic_Profiles = 10774,
  NDIS_Labs = 2,
  Investigations_Aided = 2984
)

# Display both tables for transparency
ft1 <- flextable(system_values) %>% 
  set_caption("System Reported Values (Typo)") %>% 
  theme_zebra() %>% 
  autofit()

ft2 <- flextable(corrected_values) %>% 
  set_caption("Manually Verified Correct Values") %>% 
  theme_zebra() %>% 
  autofit()

ft1
ft2

# Update ndis_clean with corrected values
ndis_clean <- ndis_clean %>%
  mutate(
    report_month = ifelse(jurisdiction == "DC/FBI Lab" & report_year == 2024, 2, report_month),
    offender_profiles = ifelse(jurisdiction == "DC/FBI Lab" & report_year == 2024, 2539381, offender_profiles),
    arrestee = ifelse(jurisdiction == "DC/FBI Lab" & report_year == 2024, 494664, arrestee),
    forensic_profiles = ifelse(jurisdiction == "DC/FBI Lab" & report_year == 2024, 10774, forensic_profiles),
    investigations_aided = ifelse(jurisdiction == "DC/FBI Lab" & report_year == 2024, 2984, investigations_aided)
  )

```

### Oklahoma Typo {#oklahoma-typo}

While exploring the data interactively, an outlier was identified in the dataset for Oklahoma in 2008.

- Extracted the affected row to verify the issue.

- Removed the erroneous entry by filtering out values greater than 10 ndis_labs specifically for Oklahoma jurisdiction in 2008.

```{r}
#| label: oklahoma-typo
#| echo: true
#| code-fold: true
#| code-summary: "Show Oklahoma typo resolution code"

growth_data <- ndis_clean %>%
  mutate(year = report_year) %>%
  group_by(jurisdiction, year) %>%
  summarise(max_labs = max(ndis_labs, na.rm = TRUE),
    .groups = 'drop')

# Interactive plot for jurisdiction-level exploration
interactive_plot <- growth_data %>%
  ggplot(aes(x = year, y = max_labs, group = jurisdiction, color = jurisdiction,
             text = paste("Jurisdiction:", jurisdiction,
                          "<br>Year:", year,
                          "<br>Max Labs:", scales::comma(max_labs)))) +
  geom_line(alpha = 0.7) +
  geom_point(size = 0.8, alpha = 0.8) +
  labs(
    title = "Labs Participating by Jurisdiction",
    x = "Year",
    y = "Max Labs Reported",
    color = "Jurisdiction"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

ggplotly(interactive_plot, tooltip = "text") %>%
  layout(hoverlabel = list(bgcolor = "white")) %>%
  config(displayModeBar = TRUE)

typo <- ndis_clean %>%
  filter(jurisdiction == "Oklahoma" & as.numeric(as.character(report_year)) %in% 2008) %>%
  select(report_year, report_month, jurisdiction, total_profiles, ndis_labs) %>%
  arrange(report_month)

flextable(typo) %>%
  theme_zebra() %>%
  autofit()

# Filter out erroneous Oklahoma 2008 NDIS labs entry
ndis_clean <- ndis_clean %>%
  filter(!(jurisdiction == "Oklahoma" & 
           report_year == 2008 & 
           ndis_labs > 10))
```

### Export Cleaned Dataset {#export-dataset}

After cleaning and processing the NDIS data, the final dataset is exported as a CSV file for further analysis or sharing. The file is saved to the `data/v1.0/` directory to maintain an organized workflow.

**Output:** `ndis_data_v1.0.csv`

```{r}
#| label: export-cleaned
#| echo: false
#| eval: true

enhanced_glimpse(ndis_clean)
```


```{r}
output_path <- here("data", "v1.0", "ndis_data_v1.0.csv")

write.csv(ndis_clean, file = output_path, row.names = FALSE)
```

## Summary Statistics {#summarystats}

Basic descriptive statistics to understand the scope and characteristics of the NDIS data.

```{r}
#| label: stats-overview
#| echo: true
#| code-fold: true
#| code-summary: "Show summary statistics code"

# Summary statistics table
ndis_summary <- ndis_clean %>% 
  group_by(report_year) %>% 
  summarise(
    jurisdictions = n_distinct(jurisdiction),
    offender = max(offender_profiles),
    arrestee = max(arrestee),
    forensic = max(forensic_profiles),
    total_profiles = max(offender + arrestee + forensic),
    .groups = 'drop'
  ) %>%
  arrange(report_year)

# Print summary table
kable(ndis_summary, caption = "Annual Summary Statistics") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

## Data Visualization {#visualiz}

### Time-Series Analysis {#time_series}

Visualization of the growth of different profile types (offender, arrestee, forensic) over time, with separate trends for each jurisdiction and smoothed aggregate trends.

#### NDIS Profile Growth for all Jurisdictions by Year
```{r}
#| label: ts-growth
#| echo: true
#| code-fold: true
#| code-summary: "Show time-series analysis code"

# Aggregate by year and profile type
yearly_growth <- ndis_clean %>%
  mutate(year = year(timestamp)) %>%
  group_by(year) %>%
  summarise(
    Offender = max(offender_profiles, na.rm = TRUE),
    Arrestee = max(arrestee, na.rm = TRUE),
    Forensic = max(forensic_profiles, na.rm = TRUE)
  ) %>%
  pivot_longer(
    cols = -year,
    names_to = "profile_type",
    values_to = "count"
  ) %>%
  filter(count > 0, !is.na(count))

# Clean interactive plot
plot_ly(yearly_growth) %>%
  add_trace(
    x = ~year, 
    y = ~count,
    color = ~profile_type,
    colors = c("#1f77b4", "#ff7f0e", "#2ca02c"),
    type = "scatter",
    mode = "lines+markers",
    hoverinfo = "text",
    text = ~paste(
      "</br>Year:", year,
      "</br>Profile Type:", profile_type,
      "</br>Count:", format(count, big.mark = ",")
    ),
    line = list(width = 3),
    marker = list(size = 6)
  ) %>%
  layout(
    title = "",
    xaxis = list(
      title = "Year", 
      dtick = 1,
      showline = TRUE,
      mirror = TRUE,
      showgrid = FALSE
    ),
    yaxis = list(
      title = "Total Profiles",
      type = "log",
      tickformat = ",.0f",
      showline = TRUE,
      mirror = TRUE,
      showgrid = FALSE
    ),
    hovermode = "x unified",
    legend = list(
      orientation = "h",
      x = 0.5,
      y = 1.1,
      xanchor = "center"
    ),
    margin = list(t = 50, b = 50),
    showlegend = TRUE
  ) %>%
  config(displayModeBar = TRUE)
```

#### Offender Profiles Growth by Jurisdiction and Year

```{r}
#| label: offenders_plot
#| echo: true
#| code-fold: true
#| code-summary: "Show plot code"

growth_data <- ndis_clean %>%
  mutate(year = report_year) %>%
  group_by(jurisdiction, year) %>%
  summarise(max_offenders = max(offender_profiles, na.rm = TRUE),
    .groups = 'drop')

# Interactive plot for jurisdiction-level exploration
interactive_plot <- growth_data %>%
  ggplot(aes(x = year, y = max_offenders, group = jurisdiction, color = jurisdiction,
             text = paste("Jurisdiction:", jurisdiction,
                          "<br>Year:", year,
                          "<br>Max Offenders:", scales::comma(max_offenders)))) +
  geom_line(alpha = 0.7) +
  geom_point(size = 0.8, alpha = 0.8) +
  labs(
    title = "Offender profiles by Jurisdiction",
    x = "Year",
    y = "Max Offender profiles Reported",
    color = "Jurisdiction"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

ggplotly(interactive_plot, tooltip = "text") %>%
  layout(hoverlabel = list(bgcolor = "white")) %>%
  config(displayModeBar = TRUE)
```

#### Arrestee Profiles Growth by Jurisdiction and Year

```{r}
#| label: arrestee_plot
#| echo: true
#| code-fold: true
#| code-summary: "Show plot code"

growth_data <- ndis_clean %>%
  mutate(year = report_year) %>%
  group_by(jurisdiction, year) %>%
  summarise(max_arrestee = max(arrestee, na.rm = TRUE),
    .groups = 'drop')

# Interactive plot for jurisdiction-level exploration
interactive_plot <- growth_data %>%
  ggplot(aes(x = year, y = max_arrestee, group = jurisdiction, color = jurisdiction,
             text = paste("Jurisdiction:", jurisdiction,
                          "<br>Year:", year,
                          "<br>Max Arrestees:", scales::comma(max_arrestee)))) +
  geom_line(alpha = 0.7) +
  geom_point(size = 0.8, alpha = 0.8) +
  labs(
    title = "Arrestee profiles by Jurisdiction",
    x = "Year",
    y = "Max Arrestee profiles Reported",
    color = "Jurisdiction"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

ggplotly(interactive_plot, tooltip = "text") %>%
  layout(hoverlabel = list(bgcolor = "white")) %>%
  config(displayModeBar = TRUE)
```

#### Forensic Profiles Growth by Jurisdiction and Year

```{r}
#| label: forensic_plot
#| echo: true
#| code-fold: true
#| code-summary: "Show plot code"

growth_data <- ndis_clean %>%
  mutate(year = report_year) %>%
  group_by(jurisdiction, year) %>%
  summarise(max_forensic = max(forensic_profiles, na.rm = TRUE),
    .groups = 'drop')

# Interactive plot for jurisdiction-level exploration
interactive_plot <- growth_data %>%
  ggplot(aes(x = year, y = max_forensic, group = jurisdiction, color = jurisdiction,
             text = paste("Jurisdiction:", jurisdiction,
                          "<br>Year:", year,
                          "<br>Max Forensics:", scales::comma(max_forensic)))) +
  geom_line(alpha = 0.7) +
  geom_point(size = 0.8, alpha = 0.8) +
  labs(
    title = "Forensic profiles by Jurisdiction",
    x = "Year",
    y = "Max Forensic profiles Reported",
    color = "Jurisdiction"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

ggplotly(interactive_plot, tooltip = "text") %>%
  layout(hoverlabel = list(bgcolor = "white")) %>%
  config(displayModeBar = TRUE)
```

#### Investigations Aided Growth by Jurisdiction and Year

```{r}
#| label: investigations_plot
#| echo: true
#| code-fold: true
#| code-summary: "Show plot code"

growth_data <- ndis_clean %>%
  mutate(year = report_year) %>%
  group_by(jurisdiction, year) %>%
  summarise(max_investigations = max(investigations_aided, na.rm = TRUE),
    .groups = 'drop')

# Interactive plot for jurisdiction-level exploration
interactive_plot <- growth_data %>%
  ggplot(aes(x = year, y = max_investigations, group = jurisdiction, color = jurisdiction,
             text = paste("Jurisdiction:", jurisdiction,
                          "<br>Year:", year,
                          "<br>Max Investigations:", scales::comma(max_investigations)))) +
  geom_line(alpha = 0.7) +
  geom_point(size = 0.8, alpha = 0.8) +
  labs(
    title = "Investigations Aided by Jurisdiction",
    x = "Year",
    y = "Max Investigations Aided Reported",
    color = "Jurisdiction"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

ggplotly(interactive_plot, tooltip = "text") %>%
  layout(hoverlabel = list(bgcolor = "white")) %>%
  config(displayModeBar = TRUE)
```

#### Participating Laboratories Growth by Jurisdiction and Year

```{r}
#| label: labs_plot
#| echo: true
#| code-fold: true
#| code-summary: "Show plot code"

growth_data <- ndis_clean %>%
  mutate(year = report_year) %>%
  group_by(jurisdiction, year) %>%
  summarise(max_labs = max(ndis_labs, na.rm = TRUE),
    .groups = 'drop')

# Interactive plot for jurisdiction-level exploration
interactive_plot <- growth_data %>%
  ggplot(aes(x = year, y = max_labs, group = jurisdiction, color = jurisdiction,
             text = paste("Jurisdiction:", jurisdiction,
                          "<br>Year:", year,
                          "<br>Max Labs:", scales::comma(max_labs)))) +
  geom_line(alpha = 0.7) +
  geom_point(size = 0.8, alpha = 0.8) +
  labs(
    title = "Labs Participating by Jurisdiction",
    x = "Year",
    y = "Max Labs Reported",
    color = "Jurisdiction"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

ggplotly(interactive_plot, tooltip = "text") %>%
  layout(hoverlabel = list(bgcolor = "white")) %>%
  config(displayModeBar = TRUE)
```

### Geospatial Mapping of Jurisdiction Participation {#geomap_juris}

```{r}
#| label: geo-map
#| echo: true
#| code-fold: true
#| code-summary: "Show jurisdiction mapping analysis code"

#| label: geo-map
#| echo: true
#| code-fold: true
#| code-summary: "Show jurisdiction mapping analysis code"

jurisdiction_coords <- tibble::tribble(
  ~jurisdiction_std,        ~lat,     ~lng,
  "Alabama",         32.8067,  -86.7911,
  "Alaska",         66.1605, -153.3691,
  "Arizona",        33.7298, -111.4312,
  "Arkansas",       34.9697,  -92.3731,
  "California",     36.1162, -119.6816,
  "Colorado",       39.0598, -105.3111,
  "Connecticut",    41.5978,  -72.7554,
  "Delaware",       39.3185,  -75.5071,
  "DC/FBI Lab",     38.9072,  -77.0369,
  "DC/Metro PD",    39.9072,  -77.0369,
  "Florida",        27.7663,  -81.6868,
  "Georgia",        33.0406,  -83.6431,
  "Hawaii",         21.3068, -157.7912,
  "Idaho",          44.2405, -114.4788,
  "Illinois",       40.3495,  -88.9861,
  "Indiana",        39.8494,  -86.2583,
  "Iowa",           42.0115,  -93.2105,
  "Kansas",         38.5266,  -96.7265,
  "Kentucky",       37.6681,  -84.6701,
  "Louisiana",      31.1695,  -91.8678,
  "Maine",          44.6939,  -69.3819,
  "Maryland",       39.0639,  -76.8021,
  "Massachusetts",  42.2302,  -71.5301,
  "Michigan",       43.3266,  -84.5361,
  "Minnesota",      45.6945,  -93.9002,
  "Mississippi",    32.7416,  -89.6787,
  "Missouri",       38.4561,  -92.2884,
  "Montana",        46.9219, -110.4544,
  "Nebraska",       41.1254,  -98.2681,
  "Nevada",         38.3135, -117.0554,
  "New Hampshire",  43.4525,  -71.5639,
  "New Jersey",     40.2989,  -74.5210,
  "New Mexico",     34.8405, -106.2485,
  "New York",       42.1657,  -74.9481,
  "North Carolina", 35.6301,  -79.8064,
  "North Dakota",   47.5289,  -99.7840,
  "Ohio",           40.3888,  -82.7649,
  "Oklahoma",       35.5653,  -96.9289,
  "Oregon",         44.5720, -122.0709,
  "Pennsylvania",   40.5908,  -77.2098,
  "Rhode Island",   41.6809,  -71.5118,
  "South Carolina", 33.8569,  -80.9450,
  "South Dakota",   44.2998,  -99.4388,
  "Tennessee",      35.7478,  -86.6923,
  "Texas",          31.0545,  -97.5635,
  "Utah",           40.1500, -111.8624,
  "Vermont",        44.0459,  -72.7107,
  "Virginia",       37.7693,  -78.1700,
  "Washington",     47.4009, -121.4905,
  "West Virginia",  38.4912,  -80.9545,
  "Wisconsin",      44.2685,  -89.6165,
  "Wyoming",        42.7560, -107.3025,
  "US Army",        40.9072,  -77.0369,
  "Puerto Rico",    18.2208,  -66.5901
)

state_abbs <- tibble::tibble(
  state = tolower(c(
    "Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut",
    "Delaware", "Florida","Georgia","Hawaii","Idaho","Illinois",
    "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts",
    "Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada",
    "New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota",
    "Ohio","Oklahoma","Oregon","Pennsylvania","Rhode Island","South Carolina",
    "South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington",
    "West Virginia","Wisconsin","Wyoming",
    "DC/Metro PD",
    "DC/FBI Lab",
    "Puerto Rico",
    "US Army"
  )),
  abb = c(
    "AL","AK","AZ","AR","CA","CO","CT",
    "DE","FL","GA","HI","ID","IL",
    "IN","IA","KS","KY","LA","ME","MD","MA",
    "MI","MN","MS","MO","MT","NE","NV",
    "NH","NJ","NM","NY","NC","ND",
    "OH","OK","OR","PA","RI","SC",
    "SD","TN","TX","UT","VT","VA","WA",
    "WV","WI","WY",
    "DC",
    "FBI",
    "PR",
    "US"
  )
)

map_data <- ndis_clean %>%
  left_join(jurisdiction_coords, by = c("jurisdiction" = "jurisdiction_std"))

set.seed(123) # for reproducibility

map_data <- map_data %>%
  group_by(lat, lng) %>%
  mutate(
    n = n(),
    offset_needed = n > 1,
    lat_offset = ifelse(offset_needed, runif(1, -0.5, 0.5), 0),
    lng_offset = ifelse(offset_needed, runif(1, -0.5, 0.5), 0),
    lat_adj = lat + lat_offset,
    lng_adj = lng + lng_offset
  ) %>%
  ungroup()

jurisdiction_summary <- map_data %>%
  filter(report_year == 2024) %>%
  mutate(jurisdiction = tolower(trimws(jurisdiction))) %>% 
  group_by(jurisdiction) %>%
  summarise(
    offender = if (all(is.na(offender_profiles))) 0 else max(offender_profiles, na.rm = TRUE),
    arrestee = if (all(is.na(arrestee))) 0 else max(arrestee, na.rm = TRUE),
    forensic = if (all(is.na(forensic_profiles))) 0 else max(forensic_profiles, na.rm = TRUE),
    total_profiles = sum(c(offender, arrestee, forensic), na.rm = TRUE),
    lat_adj = first(lat_adj),
    lng_adj = first(lng_adj)
  ) %>%
  left_join(state_abbs, by = c("jurisdiction" = "state")) %>%
  filter(!is.na(lat_adj) & !is.na(lng_adj))

pal <- colorNumeric(palette = "Blues", domain = jurisdiction_summary$total_profiles)

leaflet() %>%
  addTiles() %>%
  addLabelOnlyMarkers(
    data = jurisdiction_summary,
    lng = ~lng_adj,
    lat = ~lat_adj,
    label = ~abb,
    labelOptions = labelOptions(
      noHide = TRUE,
      direction = "center",
      textOnly = FALSE,
      style = list(
        "background" = "white",
        "border" = "2px solid #1a5276",
        "border-radius" = "3px",
        "padding" = "2px 4px",
        "font-weight" = "bold",
        "font-size" = "10px",
        "color" = "#1a5276",
        "box-shadow" = "2px 2px 4px rgba(0,0,0,0.3)"
      )
    )
  ) %>%
  addCircleMarkers(
    data = jurisdiction_summary,
    lng = ~lng_adj,
    lat = ~lat_adj,
    stroke = TRUE,
    weight = 1,
    popup = ~paste0(
      "<div style='font-size:12px'>",
      "<b>", tools::toTitleCase(jurisdiction), " (", abb, ")</b><br>",
      "2025 Data<br>",
      "Total: ", format(total_profiles, big.mark = ","), "<br>",
      "Offender: ", format(offender, big.mark = ","), "<br>",
      "Arrestee: ", format(arrestee, big.mark = ","), "<br>",
      "Forensic: ", format(forensic, big.mark = ","),
      "</div>"
    )
  ) %>%
  addControl(
    html = "<div style='background:white;padding:5px;border:2px solid #1a5276;border-radius:3px;font-weight:bold;'>NDIS 2025 State Participation</div>",
    position = "topright"
  ) %>%
  setView(lng = -98.5833, lat = 39.8333, zoom = 4)

```

### Interactive Table of Profile Growth {#interactive-explore}

Interactive table for readers to explore the underlying data with filtering and export capabilities.

```{r}
#| label: interactive-table
#| echo: true
#| code-fold: true
#| code-summary: "Show interactive table code"

summary_table <- ndis_clean %>%
  group_by(jurisdiction, report_year) %>%
  summarise(
    offender_profiles = max(offender_profiles, na.rm = TRUE),
    arrestee = max(arrestee, na.rm = TRUE),
    forensic_profiles = max(forensic_profiles, na.rm = TRUE),
    total_profiles = max(total_profiles, na.rm = TRUE),
    investigations_aided = max(investigations_aided, na.rm = TRUE),
    ndis_labs = max(ndis_labs, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(jurisdiction, report_year)

# Count the number of numeric columns (excluding the first 2 grouping columns)
numeric_cols_start <- 3
numeric_cols_end <- ncol(summary_table)  

# Interactive table
datatable(
  summary_table,
  extensions = c('Buttons', 'ColReorder', 'Scroller'),
  options = list(
    dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel', 'colvis'),
    scrollX = TRUE,
    scrollY = "600px",
    scroller = TRUE,
    pageLength = 20,
    columnDefs = list(
      list(className = 'dt-right', targets = (numeric_cols_start-1):(numeric_cols_end-1))
    )
  ),
  rownames = FALSE,
  filter = 'top'
)

```

## Conclusion {#conclusion}

This project analyzed the growth and evolution of the National DNA Index System (NDIS) by parsing historical snapshots of FBI statistics from the Wayback Machine.

**Key Findings**

1.  **Exponential Growth of DNA Profiles**

    -   The NDIS database experienced significant expansion from 2002 to 2025.

2.  **Jurisdictional Participation**

    -   All 54 U.S. jurisdictions (states and territories) contributed to NDIS, with **California, Texas, and Florida** being the largest contributors.

    -   The number of participating labs increased over time, indicating broader adoption of DNA forensic capabilities.

3.  **Policy and Technological Impact**

    -   The growth in forensic profiles suggests advancements in DNA technology and its integration into law enforcement workflows.