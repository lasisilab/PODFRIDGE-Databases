{
  "hash": "c297a34227e883846ae1f628b1217ba9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NDIS Database Analysis\"\nsubtitle: \"Parsing FBI National DNA Index System Statistics from Wayback Machine\"\nauthor: \"Tina Lasisi\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    toc: true\n    toc-depth: 3\nexecute:\n  echo: true\n  warning: false\n  freeze: auto  # prevents re-execution unless code changes\n---\n\n## Introduction\n\nThis analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025.\n\n## Setup and Configuration\n\n::: {#1b7866b5 .cell execution_count=1}\n\n::: {.cell-output .cell-output-stdout}\n```\nInstalling beautifulsoup4...\nRequirement already satisfied: beautifulsoup4 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (4.13.4)\nRequirement already satisfied: soupsieve>1.2 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (4.14.1)\n```\n:::\n:::\n\n\n::: {#1b23d621 .cell execution_count=2}\n``` {.python .cell-code}\nfrom pathlib import Path\nimport re, json, requests, time\nfrom datetime import datetime\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Configuration\n# Path setup - using current directory as base\n# This assumes you run the notebook from the project root (PODFRIDGE-Databases)\nBASE_DIR = Path(\"..\")  # Current working directory\nHTML_DIR = BASE_DIR / \"raw\" / \"wayback_html\"\nMETA_DIR = BASE_DIR / \"raw\" / \"wayback_meta\"\nOUTPUT_DIR = BASE_DIR / \"output\" / \"ndis\"\n\n# Create directories if they don't exist\nHTML_DIR.mkdir(parents=True, exist_ok=True)\nMETA_DIR.mkdir(parents=True, exist_ok=True)\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Jurisdiction name standardization mapping\nJURISDICTION_NAME_MAP = {\n    'D.C./FBI Lab': 'DC/FBI Lab',\n    'US Army': 'U.S. Army'\n}\n\n# Known data typos to fix\nKNOWN_TYPOS = [\n    {\n        'timestamp': '20250105164014',\n        'jurisdiction': 'California', \n        'field': 'investigations_aided',\n        'wrong_value': '1304657',  # How it parses\n        'correct_value': '130465'   # What it should be\n    },\n    {\n        'timestamp': '20250116205311',\n        'jurisdiction': 'California',\n        'field': 'investigations_aided', \n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    }\n]\n\nprint(f\"Working directory: {BASE_DIR.resolve()}\")\nprint(f\"HTML directory: {HTML_DIR}\")\nprint(f\"Meta directory: {META_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWorking directory: /Users/tlasisi/GitHub/PODFRIDGE-Databases\nHTML directory: ../raw/wayback_html\nMeta directory: ../raw/wayback_meta\nOutput directory: ../output/ndis\n```\n:::\n:::\n\n\n## Wayback Machine Functions\n\n::: {#f4582912 .cell execution_count=3}\n``` {.python .cell-code}\ndef make_request_with_retry(params, max_retries=3, initial_delay=5):\n    \"\"\"Make a request with exponential backoff retry logic\"\"\"\n    base = \"https://web.archive.org/cdx/search/cdx\"\n    \n    for attempt in range(max_retries):\n        try:\n            r = requests.get(base, params=params, timeout=30)\n            if r.status_code == 200:\n                return r\n            elif r.status_code == 429:  # Rate limited\n                wait_time = initial_delay * (2 ** attempt)\n                print(f\"    Rate limited. Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                return r\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except Exception as e:\n            print(f\"    Unexpected error: {e}\")\n            return None\n    return None\n```\n:::\n\n\n## Search for All NDIS Snapshots\n\n::: {#a1d6d9a5 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show search function code\"}\ndef search_all_ndis_snapshots():\n    \"\"\"Search for NDIS snapshots across all known URL variations\"\"\"\n    \n    # Search for both http and https variants\n    protocols = [\"http://\", \"https://\"]\n    subdomains = [\"www\", \"le\", \"*\"]  # Known subdomains plus wildcard\n    \n    all_rows = []\n    seen_timestamps = set()\n    \n    # First, try broad searches with protocol wildcards\n    print(\"Starting wildcard searches...\")\n    for protocol in protocols:\n        for subdomain in subdomains:\n            pattern = f\"{protocol}{subdomain}.fbi.gov/*ndis-statistics*\"\n            print(f\"\\nSearching: {pattern}\")\n            \n            params = {\n                \"url\":         pattern,\n                \"matchType\":   \"wildcard\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) > 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests to avoid rate limiting\n            time.sleep(2)\n    \n    # Also search your specific known URLs with both protocols\n    known_paths = [\n        \"www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n        \"www.fbi.gov/about-us/laboratory/biometric-analysis/codis/ndis-statistics\", \n        \"www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n        \"le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\",\n    ]\n    \n    print(\"\\n\\nStarting exact URL searches...\")\n    for path in known_paths:\n        for protocol in protocols:\n            url = f\"{protocol}{path}\"\n            print(f\"\\nSearching: {url}\")\n            \n            params = {\n                \"url\":         url,\n                \"matchType\":   \"exact\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) > 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests\n            time.sleep(2)\n    \n    # Create DataFrame\n    snap_df = (pd.DataFrame(\n                    all_rows,\n                    columns=[\"timestamp\", \"original\", \"mimetype\", \"status\"])\n               .sort_values(\"timestamp\")\n               .reset_index(drop=True))\n    \n    return snap_df\n\n# Check if we already have snapshot data or need to search\nsnapshot_csv = META_DIR / 'snapshots_found.csv'\nif snapshot_csv.exists():\n    print(\"Loading existing snapshot list...\")\n    snap_df = pd.read_csv(snapshot_csv)\n    print(f\"Loaded {len(snap_df)} snapshots\")\nelse:\n    print(\"Searching for all NDIS snapshots...\")\n    snap_df = search_all_ndis_snapshots()\n    if len(snap_df) > 0:\n        snap_df.to_csv(snapshot_csv, index=False)\n        print(f\"\\nSaved {len(snap_df)} snapshots to {snapshot_csv}\")\n\nif len(snap_df) > 0:\n    print(f\"\\nTotal unique snapshots found: {len(snap_df):,}\")\n    print(f\"Unique URLs found: {snap_df['original'].nunique()}\")\n    print(\"\\nUnique URL patterns found:\")\n    for url in sorted(snap_df['original'].unique()):\n        print(f\"  {url}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoading existing snapshot list...\nLoaded 317 snapshots\n\nTotal unique snapshots found: 317\nUnique URLs found: 8\n\nUnique URL patterns found:\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics/\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics/\n  https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics/\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics//\n```\n:::\n:::\n\n\n## Download Functions\n\n::: {#55d326da .cell execution_count=5}\n``` {.python .cell-code}\ndef download_with_retry(url, max_retries=3, initial_delay=5, consecutive_failures=0):\n    \"\"\"Download with adaptive retry logic based on consecutive failures\"\"\"\n    if consecutive_failures > 0:\n        extra_wait = consecutive_failures * 10\n        print(f\"\\n    Adding {extra_wait}s cooldown due to {consecutive_failures} consecutive failures...\")\n        time.sleep(extra_wait)\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            return response, True\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"\\n    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:\n                wait_time = initial_delay * (2 ** attempt) * 2\n                print(f\"\\n    Rate limited (429). Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"\\n    HTTP Error: {e}\")\n                return None, False\n        except Exception as e:\n            print(f\"\\n    Unexpected error: {e}\")\n            return None, False\n    return None, False\n\ndef download_missing_snapshots(snap_df, output_folder):\n    \"\"\"Download HTML snapshots with resume capability and detailed logging\"\"\"\n    \n    # Create run-specific log file\n    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_log_file = META_DIR / f\"download_log_{run_timestamp}.txt\"\n    \n    # Check what we already have\n    existing_files = list(output_folder.glob(\"*.html\"))\n    existing_timestamps = {f.stem for f in existing_files}\n    print(f\"\\nFiles already downloaded: {len(existing_files)}\")\n    \n    # Check what needs to be downloaded\n    to_download = []\n    for _, row in snap_df.iterrows():\n        timestamp = row['timestamp']\n        url = row['original']\n        filename = output_folder / f\"{timestamp}.html\"\n        \n        if timestamp not in existing_timestamps and not filename.exists():\n            to_download.append((timestamp, url, filename))\n    \n    print(f\"Files to download: {len(to_download)}\")\n    \n    # Initialize log file\n    with open(run_log_file, \"w\") as log:\n        log.write(f\"Download run started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Total snapshots in list: {len(snap_df)}\\n\")\n        log.write(f\"Already downloaded: {len(existing_files)}\\n\")\n        log.write(f\"To download: {len(to_download)}\\n\")\n        log.write(f\"{'='*60}\\n\\n\")\n    \n    if len(to_download) == 0:\n        print(\"\\n✓ All files already downloaded! Nothing to do.\")\n        with open(run_log_file, \"a\") as log:\n            log.write(\"All files already downloaded. No action needed.\\n\")\n        return\n    \n    # Download configuration\n    BATCH_SIZE = 15\n    PAUSE_BETWEEN_DOWNLOADS = 3\n    PAUSE_BETWEEN_BATCHES = 45\n    PAUSE_AFTER_FAILURE = 60\n    \n    # Track statistics\n    successful_downloads = 0\n    failed_downloads = []\n    consecutive_failures = 0\n    \n    # Download in batches\n    for i in range(0, len(to_download), BATCH_SIZE):\n        batch = to_download[i:i + BATCH_SIZE]\n        batch_num = (i // BATCH_SIZE) + 1\n        total_batches = (len(to_download) + BATCH_SIZE - 1) // BATCH_SIZE\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Batch {batch_num}/{total_batches} ({len(batch)} files)\")\n        print(f\"Overall progress: {len(existing_timestamps) + successful_downloads}/{len(snap_df)} total files\")\n        print(f\"{'='*60}\")\n        \n        with open(run_log_file, \"a\") as log:\n            log.write(f\"\\nBatch {batch_num}/{total_batches} started at {datetime.now().strftime('%H:%M:%S')}\\n\")\n        \n        for j, (timestamp, url, filename) in enumerate(batch, 1):\n            # Double-check file doesn't exist\n            if filename.exists():\n                print(f\"\\n[{j}/{len(batch)}] {timestamp} - Already exists, skipping...\")\n                with open(run_log_file, \"a\") as log:\n                    log.write(f\"{timestamp}  ⚡ already exists\\n\")\n                continue\n                \n            wayback_url = f\"https://web.archive.org/web/{timestamp}/{url}\"\n            \n            print(f\"\\n[{j}/{len(batch)}] Downloading {timestamp}...\", end=\"\")\n            \n            response, success = download_with_retry(wayback_url, consecutive_failures=consecutive_failures)\n            \n            if response and response.status_code == 200:\n                try:\n                    with open(filename, 'w', encoding='utf-8') as f:\n                        f.write(response.text)\n                    \n                    print(\" ✓ Success\")\n                    successful_downloads += 1\n                    consecutive_failures = 0\n                    \n                    with open(run_log_file, \"a\") as log:\n                        log.write(f\"{timestamp}  ✓ downloaded\\n\")\n                    \n                except Exception as e:\n                    print(f\" ✗ Error saving file: {e}\")\n                    failed_downloads.append((timestamp, url, str(e)))\n                    consecutive_failures += 1\n                    \n                    with open(run_log_file, \"a\") as log:\n                        log.write(f\"{timestamp}  ✗ failed: {str(e)}\\n\")\n            else:\n                print(\" ✗ Failed after retries\")\n                failed_downloads.append((timestamp, url, \"Download failed\"))\n                consecutive_failures += 1\n                \n                with open(run_log_file, \"a\") as log:\n                    log.write(f\"{timestamp}  ✗ failed: Download failed after retries\\n\")\n                \n                if j < len(batch):\n                    print(f\"    Taking {PAUSE_AFTER_FAILURE}s break after failure...\")\n                    time.sleep(PAUSE_AFTER_FAILURE)\n                    continue\n            \n            if j < len(batch) and consecutive_failures == 0:\n                print(f\"    Waiting {PAUSE_BETWEEN_DOWNLOADS} seconds...\")\n                time.sleep(PAUSE_BETWEEN_DOWNLOADS)\n        \n        if i + BATCH_SIZE < len(to_download):\n            print(f\"\\nBatch complete. Pausing {PAUSE_BETWEEN_BATCHES} seconds...\")\n            print(f\"This session: {successful_downloads} downloaded, {len(failed_downloads)} failed\")\n            time.sleep(PAUSE_BETWEEN_BATCHES)\n    \n    # Final summary\n    print(f\"\\n{'='*60}\")\n    print(f\"Download session complete!\")\n    print(f\"  Successfully downloaded: {successful_downloads}\")\n    print(f\"  Failed downloads: {len(failed_downloads)}\")\n    \n    # Write final summary to log\n    with open(run_log_file, \"a\") as log:\n        log.write(f\"\\n{'='*60}\\n\")\n        log.write(f\"Download run completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Successfully downloaded: {successful_downloads}\\n\")\n        log.write(f\"Failed downloads: {len(failed_downloads)}\\n\")\n        \n        if failed_downloads:\n            log.write(f\"\\nFailed downloads detail:\\n\")\n            for timestamp, url, error in failed_downloads:\n                log.write(f\"  {timestamp}: {error}\\n\")\n    \n    if failed_downloads:\n        print(f\"\\nFailed downloads:\")\n        for timestamp, url, error in failed_downloads[:10]:\n            print(f\"  {timestamp}: {error}\")\n        if len(failed_downloads) > 10:\n            print(f\"  ... and {len(failed_downloads) - 10} more\")\n    \n    print(f\"\\nDownload log saved to: {run_log_file}\")\n\n# Download missing files\nif len(snap_df) > 0:\n    download_missing_snapshots(snap_df, HTML_DIR)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFiles already downloaded: 0\nFiles to download: 317\n\n============================================================\nBatch 1/22 (15 files)\nOverall progress: 0/317 total files\n============================================================\n\n[1/15] Downloading 20101014043819... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20110111093835... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20110127075531... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20110410202836... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20110706142451... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20110903022829... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20111001205511... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20111022093959... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20111026175039... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20111101090945... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20111115114507... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20111202050758... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20120101203804... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20120111115907... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20120508173804... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 15 downloaded, 0 failed\n\n============================================================\nBatch 2/22 (15 files)\nOverall progress: 15/317 total files\n============================================================\n\n[1/15] Downloading 20120701052745... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20120815160113... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20120911092525... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20120913074430... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20120913074444... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20120915124215...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[7/15] Downloading 20120921122419...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20120927161952... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20120927162304... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20120928031850... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20121006223803... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20121014054004... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20121014145914... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20121017230003... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20121018124013... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 29 downloaded, 1 failed\n\n============================================================\nBatch 3/22 (15 files)\nOverall progress: 29/317 total files\n============================================================\n\n[1/15] Downloading 20121021061338... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20121022162330... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20121023200759... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20121025072144... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20121027093545... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20121028031145... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20121030024545... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20121031002501... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20121103063643... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20121105112218... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20121114072458... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20160730053232...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[13/15] Downloading 20160826152101...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20161014043057... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20170106073100... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 43 downloaded, 2 failed\n\n============================================================\nBatch 4/22 (15 files)\nOverall progress: 43/317 total files\n============================================================\n\n[1/15] Downloading 20170127174455... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20170129013226... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20170227173146... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20170409202627... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20170505054222... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20170513040512... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20170516144007... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20170516201425... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20170518003707... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20170520050254... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20170621200054... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20170710141223... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20170925144132... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20170930134422... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20171014140750... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 58 downloaded, 2 failed\n\n============================================================\nBatch 5/22 (15 files)\nOverall progress: 58/317 total files\n============================================================\n\n[1/15] Downloading 20171025074255... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20171206151623... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20171214175911...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[4/15] Downloading 20180211052927...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20180314184913... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20180425125459... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20180428000233... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20180517024114... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20180608053326... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20180707231136... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20180710171340... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20180827122745... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20180911202332... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20180913062030... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20181016070747... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 72 downloaded, 3 failed\n\n============================================================\nBatch 6/22 (15 files)\nOverall progress: 72/317 total files\n============================================================\n\n[1/15] Downloading 20181017073224... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20181018155249... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20181019162711... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20181020170549... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20181021184026... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20181022182033... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20181024094434... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20181027194438... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20181104153147...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[10/15] Downloading 20181116183506...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20181130044044... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20181204032834... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20181220214643... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20181230180748... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20190103203642... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 86 downloaded, 4 failed\n\n============================================================\nBatch 7/22 (15 files)\nOverall progress: 86/317 total files\n============================================================\n\n[1/15] Downloading 20190129222014... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20190215164131... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20190302060036... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20190302211500... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20190305145401... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20190307110347... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20190320233159... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20190322204155... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20190323202517... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20190324201718... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20190326161430... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20190401203737... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20190412150804... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20190502135053... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20190502231236...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n\nBatch complete. Pausing 45 seconds...\nThis session: 100 downloaded, 5 failed\n\n============================================================\nBatch 8/22 (15 files)\nOverall progress: 100/317 total files\n============================================================\n\n[1/15] Downloading 20190510235359...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20190510235400... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20190511000325... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20190523123747... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20190611144717... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20190612165511... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20190613161343...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[8/15] Downloading 20190614180425...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20190620210900... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20190626135247... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20190702201315... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20190703224220... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20190810091618... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20190816134908... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20190824202250... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 114 downloaded, 6 failed\n\n============================================================\nBatch 9/22 (15 files)\nOverall progress: 114/317 total files\n============================================================\n\n[1/15] Downloading 20190825210357... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20190929200543... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20190929200638... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20191004193052... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20191005212555... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20191006085602... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20191011161318... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20191017000804... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20191019061318... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20191026043249... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20191027055331... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20191030211212... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20191102010133...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[14/15] Downloading 20191104050315...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20191107032513... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 128 downloaded, 7 failed\n\n============================================================\nBatch 10/22 (15 files)\nOverall progress: 128/317 total files\n============================================================\n\n[1/15] Downloading 20191108023243... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20191117165200... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20191124095229... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20191217191331... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20191226185928... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20200117180042... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20200301051858... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200307031008... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20200326154938... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20200401175110... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20200401211339... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20200405080000... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20200414121931... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20200415182137... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20200429225129... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 143 downloaded, 7 failed\n\n============================================================\nBatch 11/22 (15 files)\nOverall progress: 143/317 total files\n============================================================\n\n[1/15] Downloading 20200510221207... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20200511224952... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20200519201942... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20200607224349...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[5/15] Downloading 20200620221947...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20200719134420... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20200728162952... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200802095215... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20200803110741... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20200806102056... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20200806191421... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20200809030342... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20200814161013... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20200816121500... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20200818151622... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 157 downloaded, 8 failed\n\n============================================================\nBatch 12/22 (15 files)\nOverall progress: 157/317 total files\n============================================================\n\n[1/15] Downloading 20200820052431... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20200826044232... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20200914021204... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20200916105338... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20200916211529... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20200917051636... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20200918184343... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200920212212... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20200926224215... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20201002213332...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[11/15] Downloading 20201016205200...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20201018092433... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20201020010327... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20201028215925... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20201029194830... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 171 downloaded, 9 failed\n\n============================================================\nBatch 13/22 (15 files)\nOverall progress: 171/317 total files\n============================================================\n\n[1/15] Downloading 20201111223430... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20201112011539... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20201125102405... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20201126071015... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20201203022058... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20201203072310... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20201204071640... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20201209084343... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20201218220958... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20201224155501... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20201225185038... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20201226161923... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20201227181400... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20201228193055... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20201229213443... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 186 downloaded, 9 failed\n\n============================================================\nBatch 14/22 (15 files)\nOverall progress: 186/317 total files\n============================================================\n\n[1/15] Downloading 20201230185607...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[2/15] Downloading 20210101071729...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20210103113801... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20210104124131... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20210117123428... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20210117154805... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20210118215353... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20210119080323... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20210124090728... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20210125004521... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210126040048... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210129170244... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20210131205415... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20210206102132... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20210212004658... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 200 downloaded, 10 failed\n\n============================================================\nBatch 15/22 (15 files)\nOverall progress: 200/317 total files\n============================================================\n\n[1/15] Downloading 20210214004627... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20210303055250... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20210305012011... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20210308040136... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20210308111758... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20210309005158... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20210311023951...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[8/15] Downloading 20210318000609...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20210324050729... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20210409020505... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210414043038... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210428153014... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20210507090954... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20210507090955... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20210514075457... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 214 downloaded, 11 failed\n\n============================================================\nBatch 16/22 (15 files)\nOverall progress: 214/317 total files\n============================================================\n\n[1/15] Downloading 20210527100801... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20210619090441... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20210703080605... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20210703105804... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20210706213905... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20210711064442... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20210713143621... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20210718105628... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20210808190600... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20210809035809... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210905221144... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210908064238... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20210924184245...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[14/15] Downloading 20210926213344...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20211010060654... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 228 downloaded, 12 failed\n\n============================================================\nBatch 17/22 (15 files)\nOverall progress: 228/317 total files\n============================================================\n\n[1/15] Downloading 20211010060655... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20211010203923... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20211019180134... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20211021235339... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20211025021035... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20211104193210... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20211114092857... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20211121045733... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20211128164432... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20211205045644... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20211205200142... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20211217120358... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20211224185611... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20220125103659... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20220130221713... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 243 downloaded, 12 failed\n\n============================================================\nBatch 18/22 (15 files)\nOverall progress: 243/317 total files\n============================================================\n\n[1/15] Downloading 20220209033036... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20220210054625... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20220214082051... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20220226123842...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[5/15] Downloading 20220305152131...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20220307213357... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20220311083835... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20220321052126... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20220326170646... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20220328083342... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20220329024143... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20220331230634... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20220402203124... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20220416015545... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20220423192010... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 257 downloaded, 13 failed\n\n============================================================\nBatch 19/22 (15 files)\nOverall progress: 257/317 total files\n============================================================\n\n[1/15] Downloading 20220519223937... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20220601085211... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20220629123237... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20230411172744... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20230515144637... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20230528103842... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20230713202131... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20230713202137... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20230829204608... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20230829213846...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[11/15] Downloading 20230831093549...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20230901125409... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20230902061832... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20230913085200... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20231025142202... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 271 downloaded, 14 failed\n\n============================================================\nBatch 20/22 (15 files)\nOverall progress: 271/317 total files\n============================================================\n\n[1/15] Downloading 20231025183800... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20231031115239... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20231102072222... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20231113033840... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20231127144926... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20231130100758... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20240111202156... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20240116143636... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20240203212847... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20240317130138... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20240413070959... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20240415150311... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20240713143931... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20240714204728... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20240822224345... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 286 downloaded, 14 failed\n\n============================================================\nBatch 21/22 (15 files)\nOverall progress: 286/317 total files\n============================================================\n\n[1/15] Downloading 20240824025139...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[2/15] Downloading 20240826235428...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20240927093855... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20241014183409... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20241128025327... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20250105164014... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20250116205311... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20250117084527... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20250201152305... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20250305200402... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20250307115942... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20250329050408... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20250523182825... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20250603075541... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20250606041947... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 300 downloaded, 15 failed\n\n============================================================\nBatch 22/22 (2 files)\nOverall progress: 300/317 total files\n============================================================\n\n[1/2] Downloading 20250606050213... ✓ Success\n    Waiting 3 seconds...\n\n[2/2] Downloading 20250629171550... ✓ Success\n\n============================================================\nDownload session complete!\n  Successfully downloaded: 302\n  Failed downloads: 15\n\nFailed downloads:\n  20120915124215: Download failed\n  20160730053232: Download failed\n  20171214175911: Download failed\n  20181104153147: Download failed\n  20190502231236: Download failed\n  20190613161343: Download failed\n  20191102010133: Download failed\n  20200607224349: Download failed\n  20201002213332: Download failed\n  20201230185607: Download failed\n  ... and 5 more\n\nDownload log saved to: ../raw/wayback_meta/download_log_20250721_190902.txt\n```\n:::\n:::\n\n\n## Download Status Check Functions\n\n::: {#2d2d01ec .cell execution_count=6}\n``` {.python .cell-code}\ndef get_latest_log():\n    \"\"\"Find and read the most recent download log\"\"\"\n    log_files = sorted(META_DIR.glob(\"download_log_*.txt\"))\n    if not log_files:\n        print(\"No download logs found.\")\n        return None\n    \n    latest_log = log_files[-1]\n    print(f\"Latest log: {latest_log.name}\")\n    return latest_log\n\ndef analyze_download_status():\n    \"\"\"Analyze the current download status and latest run results\"\"\"\n    \n    # Check what we have\n    html_files = list(HTML_DIR.glob(\"*.html\"))\n    downloaded_timestamps = {f.stem for f in html_files}\n    \n    # Check what we should have\n    snapshot_csv = META_DIR / 'snapshots_found.csv'\n    if snapshot_csv.exists():\n        snap_df = pd.read_csv(snapshot_csv)\n        expected_timestamps = set(snap_df['timestamp'].astype(str))\n    else:\n        print(\"No snapshot list found. Run search first.\")\n        return None\n    \n    # Calculate missing\n    missing_timestamps = expected_timestamps - downloaded_timestamps\n    \n    # Parse latest log for failures\n    latest_log = get_latest_log()\n    failed_in_last_run = []\n    \n    if latest_log:\n        with open(latest_log, 'r') as f:\n            for line in f:\n                if '✗ failed:' in line:\n                    timestamp = line.split()[0]\n                    if timestamp.isdigit() and len(timestamp) == 14:\n                        failed_in_last_run.append(timestamp)\n    \n    # Create summary\n    print(f\"\\n{'='*60}\")\n    print(\"DOWNLOAD STATUS SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"Expected snapshots: {len(expected_timestamps)}\")\n    print(f\"Downloaded: {len(downloaded_timestamps)} ({len(downloaded_timestamps)/len(expected_timestamps)*100:.1f}%)\")\n    print(f\"Missing: {len(missing_timestamps)}\")\n    \n    if latest_log:\n        print(f\"\\nLatest run ({latest_log.name}):\")\n        print(f\"  Failed downloads: {len(failed_in_last_run)}\")\n        if failed_in_last_run:\n            print(f\"  Failed timestamps: {', '.join(failed_in_last_run[:5])}\")\n            if len(failed_in_last_run) > 5:\n                print(f\"  ... and {len(failed_in_last_run) - 5} more\")\n    \n    print(f\"\\nTotal still needed: {len(missing_timestamps)}\")\n    \n    return {\n        'missing': missing_timestamps,\n        'failed_last_run': failed_in_last_run,\n        'downloaded': downloaded_timestamps,\n        'expected': expected_timestamps\n    }\n\ndef create_retry_list(status_info, retry_only_failed=True):\n    \"\"\"Create a list of files to retry downloading\"\"\"\n    if not status_info:\n        return None\n    \n    if retry_only_failed and status_info['failed_last_run']:\n        retry_timestamps = set(status_info['failed_last_run'])\n        print(f\"\\nWill retry {len(retry_timestamps)} failed downloads from last run\")\n    else:\n        retry_timestamps = status_info['missing']\n        print(f\"\\nWill retry all {len(retry_timestamps)} missing files\")\n    \n    snapshot_csv = META_DIR / 'snapshots_found.csv'\n    snap_df = pd.read_csv(snapshot_csv)\n    retry_df = snap_df[snap_df['timestamp'].astype(str).isin(retry_timestamps)]\n    \n    return retry_df\n```\n:::\n\n\n## Parser Functions\n\n::: {#b8d8e9fd .cell execution_count=7}\n``` {.python .cell-code}\ndef clean_jurisdiction_name(name):\n    \"\"\"Clean up jurisdiction names by removing common prefixes\"\"\"\n    name = re.sub(r'^.*?Back to top\\s*', '', name)\n    name = re.sub(r'^.*?Tables by NDIS Participant\\s*', '', name)\n    name = re.sub(r'^.*?ation\\.\\s*', '', name)\n    name = name.strip()\n    return name\n\ndef standardize_jurisdiction_name(name):\n    \"\"\"Standardize jurisdiction names to handle variations\"\"\"\n    name = clean_jurisdiction_name(name)\n    if name in JURISDICTION_NAME_MAP:\n        return JURISDICTION_NAME_MAP[name]\n    return name\n\ndef extract_data_date(html_content):\n    \"\"\"Extract the 'Statistics as of' date from HTML content\"\"\"\n    match = re.search(r'Statistics as of (\\w+ \\d{4})', html_content, re.IGNORECASE)\n    if match:\n        date_str = match.group(1)\n        try:\n            # Convert \"October 2024\" to datetime\n            return datetime.strptime(date_str, \"%B %Y\")\n        except:\n            pass\n    return None\n\ndef parse_ndis_snapshot(html_file):\n    \"\"\"Parse a single NDIS snapshot file\"\"\"\n    timestamp = html_file.stem\n    year = int(timestamp[:4])\n    \n    html_content = html_file.read_text('utf-8', errors='ignore')\n    soup = BeautifulSoup(html_content, 'lxml')\n    text = soup.get_text(' ', strip=True)\n    \n    # Extract the \"as of\" date\n    data_date = extract_data_date(html_content)\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    records = []\n    \n    # Pattern for 2010 (no arrestee data)\n    if year <= 2010:\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Samples\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': '0',\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    else:\n        # Pattern for 2011+ (includes arrestee data)\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Arrestee\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Profiles\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, arrestee, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': arrestee.replace(',', ''),\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    \n    return records\n```\n:::\n\n\n## Process All Snapshots\n\n::: {#3aeb4853 .cell execution_count=8}\n``` {.python .cell-code}\ndef process_all_snapshots():\n    \"\"\"Parse all downloaded snapshots and create datasets\"\"\"\n    print(\"Processing all snapshots...\")\n    \n    all_records = []\n    html_files = sorted(HTML_DIR.glob(\"*.html\"))\n    \n    for html_file in tqdm(html_files, desc=\"Parsing HTML files\"):\n        try:\n            records = parse_ndis_snapshot(html_file)\n            all_records.extend(records)\n        except Exception as e:\n            print(f\"Error parsing {html_file.name}: {e}\")\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(all_records)\n    \n    # Convert numeric fields\n    numeric_fields = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n    for field in numeric_fields:\n        df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0).astype(int)\n    \n    # Add datetime columns\n    df['capture_datetime'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S')\n    df['capture_date'] = df['capture_datetime'].dt.date\n    \n    # Sort by timestamp and jurisdiction\n    df = df.sort_values(['timestamp', 'jurisdiction'])\n    \n    return df\n\n# Process all files\ndf_raw = process_all_snapshots()\nprint(f\"\\nProcessed {len(df_raw)} total records\")\nprint(f\"Unique jurisdictions: {df_raw['jurisdiction'].nunique()}\")\nprint(f\"Date range: {df_raw['capture_datetime'].min()} to {df_raw['capture_datetime'].max()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing all snapshots...\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"cc093802dd484dc1b2516c42778b58d6\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nProcessed 15320 total records\nUnique jurisdictions: 54\nDate range: 2010-10-14 04:38:19 to 2025-06-29 17:15:50\n```\n:::\n:::\n\n\n## Check Download Completeness\n\n::: {#8008a9c5 .cell execution_count=9}\n``` {.python .cell-code}\n# Check if we have all expected files\nprint(\"\\nChecking download completeness...\")\nstatus = analyze_download_status()\n\n# Automatically retry if there are failures\nif status and status['failed_last_run'] and len(status['failed_last_run']) > 0:\n    print(f\"\\n⚠️  Found {len(status['failed_last_run'])} failed downloads from last run. Retrying...\")\n    retry_df = create_retry_list(status, retry_only_failed=True)\n    if retry_df is not None and len(retry_df) > 0:\n        download_missing_snapshots(retry_df, HTML_DIR)\n        \n        # Re-check status after retry\n        print(\"\\nRechecking status after retry...\")\n        status = analyze_download_status()\nelif status and status['missing']:\n    print(f\"\\n⚠️  {len(status['missing'])} files are missing but weren't from a failed run.\")\n    print(\"These may be new snapshots. Run download cell manually if needed.\")\nelse:\n    print(\"\\n✅ All files successfully downloaded!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nChecking download completeness...\nLatest log: download_log_20250721_190902.txt\n\n============================================================\nDOWNLOAD STATUS SUMMARY\n============================================================\nExpected snapshots: 317\nDownloaded: 302 (95.3%)\nMissing: 15\n\nLatest run (download_log_20250721_190902.txt):\n  Failed downloads: 15\n  Failed timestamps: 20120915124215, 20160730053232, 20171214175911, 20181104153147, 20190502231236\n  ... and 10 more\n\nTotal still needed: 15\n\n⚠️  Found 15 failed downloads from last run. Retrying...\n\nWill retry 15 failed downloads from last run\n\nFiles already downloaded: 302\nFiles to download: 15\n\n============================================================\nBatch 1/1 (15 files)\nOverall progress: 302/15 total files\n============================================================\n\n[1/15] Downloading 20120915124215... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20160730053232... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20171214175911... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20181104153147... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20190502231236...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[6/15] Downloading 20190613161343...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20191102010133... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200607224349... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20201002213332... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20201230185607... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210311023951... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210924184245... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20220226123842... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20230829213846... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20240824025139... ✓ Success\n\n============================================================\nDownload session complete!\n  Successfully downloaded: 14\n  Failed downloads: 1\n\nFailed downloads:\n  20190502231236: Download failed\n\nDownload log saved to: ../raw/wayback_meta/download_log_20250721_200952.txt\n\nRechecking status after retry...\nLatest log: download_log_20250721_200952.txt\n\n============================================================\nDOWNLOAD STATUS SUMMARY\n============================================================\nExpected snapshots: 317\nDownloaded: 316 (99.7%)\nMissing: 1\n\nLatest run (download_log_20250721_200952.txt):\n  Failed downloads: 1\n  Failed timestamps: 20190502231236\n\nTotal still needed: 1\n```\n:::\n:::\n\n\n## Apply Typo Fixes\n\n::: {#74bf6091 .cell execution_count=10}\n``` {.python .cell-code}\ndef apply_typo_fixes(df):\n    \"\"\"Apply known typo corrections\"\"\"\n    df_fixed = df.copy()\n    \n    for typo in KNOWN_TYPOS:\n        mask = (\n            (df_fixed['timestamp'] == typo['timestamp']) & \n            (df_fixed['jurisdiction'] == typo['jurisdiction'])\n        )\n        if mask.any():\n            df_fixed.loc[mask, typo['field']] = int(typo['correct_value'])\n            print(f\"Fixed typo: {typo['jurisdiction']} on {typo['timestamp'][:8]} - \"\n                  f\"{typo['field']} from {typo['wrong_value']} to {typo['correct_value']}\")\n    \n    return df_fixed\n\n# Apply fixes\ndf_fixed = apply_typo_fixes(df_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFixed typo: California on 20250105 - investigations_aided from 1304657 to 130465\nFixed typo: California on 20250116 - investigations_aided from 1304657 to 130465\n```\n:::\n:::\n\n\n## Save Datasets\n\n::: {#75ed53d9 .cell execution_count=11}\n``` {.python .cell-code}\n# Save datasets\ndf_raw.to_csv(OUTPUT_DIR / 'ndis_data_raw.csv', index=False)\ndf_fixed.to_csv(OUTPUT_DIR / 'ndis_data_fixed.csv', index=False)\nprint(f\"\\nSaved raw data to: {OUTPUT_DIR / 'ndis_data_raw.csv'}\")\nprint(f\"Saved fixed data to: {OUTPUT_DIR / 'ndis_data_fixed.csv'}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSaved raw data to: ../output/ndis/ndis_data_raw.csv\nSaved fixed data to: ../output/ndis/ndis_data_fixed.csv\n```\n:::\n:::\n\n\n## Summary Statistics\n\n::: {#e11cb181 .cell execution_count=12}\n``` {.python .cell-code}\n# Calculate summary statistics\nlatest_data = df_fixed[df_fixed['capture_datetime'] == df_fixed['capture_datetime'].max()]\nlatest_data = latest_data[latest_data['jurisdiction'] != 'D.C./Metro PD']\n\nprint(\"\\nLatest Statistics Summary:\")\nprint(f\"  As of: {latest_data['capture_datetime'].iloc[0]}\")\nprint(f\"  Data from: {latest_data['data_as_of_date'].iloc[0] if latest_data['data_as_of_date'].iloc[0] else 'Unknown'}\")\nprint(f\"  Jurisdictions reporting: {len(latest_data)}\")\nprint(f\"  Total offender profiles: {latest_data['offender_profiles'].sum():,}\")\nprint(f\"  Total arrestee profiles: {latest_data['arrestee'].sum():,}\")\nprint(f\"  Total forensic profiles: {latest_data['forensic_profiles'].sum():,}\")\nprint(f\"  Total investigations aided: {latest_data['investigations_aided'].sum():,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLatest Statistics Summary:\n  As of: 2025-06-29 17:15:50\n  Data from: 2025-04-01 00:00:00\n  Jurisdictions reporting: 53\n  Total offender profiles: 18,431,162\n  Total arrestee profiles: 5,879,537\n  Total forensic profiles: 1,405,917\n  Total investigations aided: 730,426\n```\n:::\n:::\n\n\n## Visualizations\n\n::: {#40718f2d .cell fig-height='18' fig-width='16' execution_count=13}\n``` {.python .cell-code}\ndef create_visualizations(df_raw, df_fixed):\n    \"\"\"Create comprehensive visualizations\"\"\"\n    # Exclude D.C./Metro PD from visualizations\n    df_raw_viz = df_raw[df_raw['jurisdiction'] != 'D.C./Metro PD']\n    df_fixed_viz = df_fixed[df_fixed['jurisdiction'] != 'D.C./Metro PD']\n    \n    # Create figure with subplots\n    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n    \n    # 1. Jurisdictions reporting over time\n    for ax, (df, title_suffix) in zip([axes[0,0], axes[0,1]], \n                                      [(df_raw_viz, 'Raw'), (df_fixed_viz, 'Fixed')]):\n        jurisdictions_per_date = df.groupby('capture_datetime')['jurisdiction'].nunique()\n        ax.plot(jurisdictions_per_date.index, jurisdictions_per_date.values, 'b-', linewidth=2)\n        ax.set_title(f'Jurisdictions Reporting ({title_suffix})')\n        ax.set_ylabel('Number of Jurisdictions')\n        ax.grid(True, alpha=0.3)\n        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n    \n    # 2. Total investigations aided\n    for ax, (df, title_suffix) in zip([axes[1,0], axes[1,1]], \n                                      [(df_raw_viz, 'Raw with Typo'), (df_fixed_viz, 'Fixed')]):\n        total_inv = df.groupby('capture_datetime')['investigations_aided'].sum()\n        ax.plot(total_inv.index, total_inv.values / 1e3, 'purple', linewidth=2)\n        ax.set_title(f'Total Investigations Aided ({title_suffix})')\n        ax.set_ylabel('Thousands of Investigations')\n        ax.grid(True, alpha=0.3)\n        \n        # Highlight the typo in raw data\n        if 'Raw' in title_suffix:\n            typo_dates = df[(df['jurisdiction'] == 'California') & \n                          (df['investigations_aided'] > 1000000)]['capture_datetime']\n            for date in typo_dates:\n                ax.axvline(x=date, color='red', linestyle='--', alpha=0.5)\n                ax.text(date, ax.get_ylim()[1]*0.9, 'Typo', rotation=90, \n                       verticalalignment='bottom', color='red')\n    \n    # 3. Data lag analysis\n    ax = axes[2, 0]\n    df_with_lag = df_fixed_viz[df_fixed_viz['data_as_of_date'].notna()].copy()\n    df_with_lag['data_lag_days'] = (df_with_lag['capture_datetime'] - df_with_lag['data_as_of_date']).dt.days\n    \n    avg_lag = df_with_lag.groupby('capture_datetime')['data_lag_days'].mean()\n    ax.plot(avg_lag.index, avg_lag.values, 'orange', linewidth=2)\n    ax.set_title('Average Data Lag (Capture Date vs \"As Of\" Date)')\n    ax.set_ylabel('Days')\n    ax.grid(True, alpha=0.3)\n    \n    # 4. California investigations over time (showing typo fix)\n    ax = axes[2, 1]\n    cal_raw = df_raw_viz[df_raw_viz['jurisdiction'] == 'California']\n    cal_fixed = df_fixed_viz[df_fixed_viz['jurisdiction'] == 'California']\n    \n    ax.plot(cal_raw['capture_datetime'], cal_raw['investigations_aided'], \n            'r-', label='Raw (with typo)', linewidth=2, alpha=0.7)\n    ax.plot(cal_fixed['capture_datetime'], cal_fixed['investigations_aided'], \n            'g-', label='Fixed', linewidth=2)\n    ax.set_title('California Investigations Aided: Raw vs Fixed')\n    ax.set_ylabel('Investigations Aided')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / 'ndis_analysis_complete.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Create visualizations\nprint(\"\\nCreating visualizations...\")\ncreate_visualizations(df_raw, df_fixed)\nprint(\"\\nProcessing complete!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCreating visualizations...\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ndis_scraping_files/figure-html/cell-14-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nProcessing complete!\n```\n:::\n:::\n\n\n",
    "supporting": [
      "ndis_scraping_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"1fedb3f9b83a470ab88a81d678ef87fd\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_bad7065256f8485f8b702b11a7177a7f\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_6f1b064f45b846ffbf696c11a873e788\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 302/302 [00:17&lt;00:00, 50.20it/s]\"}},\"27a09b0a534040ad94928911d3fc8a62\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_d6384ce264534a9890f8d5f6bdd25fa7\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_99039b7c9f4b4300b826dcd08aea5825\",\"tabbable\":null,\"tooltip\":null,\"value\":\"Parsing HTML files: 100%\"}},\"4bcb5531c7654458b2482ed76b29c370\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"6f1b064f45b846ffbf696c11a873e788\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"98fc5f2fe0e24c6db99e7408d7413d0a\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"99039b7c9f4b4300b826dcd08aea5825\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"a9745fece93b4c63997d9745a7cf5f8d\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"bad7065256f8485f8b702b11a7177a7f\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"cc093802dd484dc1b2516c42778b58d6\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_27a09b0a534040ad94928911d3fc8a62\",\"IPY_MODEL_f660ca16c3474f949ab396fb3eed9f82\",\"IPY_MODEL_1fedb3f9b83a470ab88a81d678ef87fd\"],\"layout\":\"IPY_MODEL_a9745fece93b4c63997d9745a7cf5f8d\",\"tabbable\":null,\"tooltip\":null}},\"d6384ce264534a9890f8d5f6bdd25fa7\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"f660ca16c3474f949ab396fb3eed9f82\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_4bcb5531c7654458b2482ed76b29c370\",\"max\":302,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_98fc5f2fe0e24c6db99e7408d7413d0a\",\"tabbable\":null,\"tooltip\":null,\"value\":302}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}