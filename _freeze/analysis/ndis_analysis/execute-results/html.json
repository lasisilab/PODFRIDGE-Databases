{
  "hash": "c297a34227e883846ae1f628b1217ba9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NDIS Database Analysis\"\nsubtitle: \"Parsing FBI National DNA Index System Statistics from Wayback Machine\"\nauthor: \"Tina Lasisi\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    toc: true\n    toc-depth: 3\nexecute:\n  echo: true\n  warning: false\n  freeze: auto  # prevents re-execution unless code changes\n---\n\n## Introduction\n\nThis analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025.\n\n## Setup and Configuration\n\n::: {#c9ee66ec .cell execution_count=1}\n\n::: {.cell-output .cell-output-stdout}\n```\nInstalling beautifulsoup4...\nRequirement already satisfied: beautifulsoup4 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (4.13.4)\nRequirement already satisfied: soupsieve>1.2 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (4.14.1)\n```\n:::\n:::\n\n\n::: {#9e333109 .cell execution_count=2}\n``` {.python .cell-code}\nfrom pathlib import Path\nimport re, json, requests, time\nfrom datetime import datetime\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Configuration\n# Path setup - using current directory as base\n# This assumes you run the notebook from the project root (PODFRIDGE-Databases)\nBASE_DIR = Path(\"..\")  # Current working directory\nHTML_DIR = BASE_DIR / \"raw\" / \"wayback_html\"\nMETA_DIR = BASE_DIR / \"raw\" / \"wayback_meta\"\nOUTPUT_DIR = BASE_DIR / \"output\" / \"ndis\"\n\n# Create directories if they don't exist\nHTML_DIR.mkdir(parents=True, exist_ok=True)\nMETA_DIR.mkdir(parents=True, exist_ok=True)\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Jurisdiction name standardization mapping\nJURISDICTION_NAME_MAP = {\n    'D.C./FBI Lab': 'DC/FBI Lab',\n    'US Army': 'U.S. Army'\n}\n\n# Known data typos to fix\nKNOWN_TYPOS = [\n    {\n        'timestamp': '20250105164014',\n        'jurisdiction': 'California', \n        'field': 'investigations_aided',\n        'wrong_value': '1304657',  # How it parses\n        'correct_value': '130465'   # What it should be\n    },\n    {\n        'timestamp': '20250116205311',\n        'jurisdiction': 'California',\n        'field': 'investigations_aided', \n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    }\n]\n\nprint(f\"Working directory: {BASE_DIR.resolve()}\")\nprint(f\"HTML directory: {HTML_DIR}\")\nprint(f\"Meta directory: {META_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWorking directory: /Users/tlasisi/GitHub/PODFRIDGE-Databases\nHTML directory: ../raw/wayback_html\nMeta directory: ../raw/wayback_meta\nOutput directory: ../output/ndis\n```\n:::\n:::\n\n\n## Wayback Machine Functions\n\n::: {#f34cfea7 .cell execution_count=3}\n``` {.python .cell-code}\ndef make_request_with_retry(params, max_retries=3, initial_delay=5):\n    \"\"\"Make a request with exponential backoff retry logic\"\"\"\n    base = \"https://web.archive.org/cdx/search/cdx\"\n    \n    for attempt in range(max_retries):\n        try:\n            r = requests.get(base, params=params, timeout=30)\n            if r.status_code == 200:\n                return r\n            elif r.status_code == 429:  # Rate limited\n                wait_time = initial_delay * (2 ** attempt)\n                print(f\"    Rate limited. Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                return r\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except Exception as e:\n            print(f\"    Unexpected error: {e}\")\n            return None\n    return None\n```\n:::\n\n\n## Search for All NDIS Snapshots\n\n::: {#466eea35 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show search function code\"}\ndef search_all_ndis_snapshots():\n    \"\"\"Search for NDIS snapshots across all known URL variations\"\"\"\n    \n    # Search for both http and https variants\n    protocols = [\"http://\", \"https://\"]\n    subdomains = [\"www\", \"le\", \"*\"]  # Known subdomains plus wildcard\n    \n    all_rows = []\n    seen_timestamps = set()\n    \n    # First, try broad searches with protocol wildcards\n    print(\"Starting wildcard searches...\")\n    for protocol in protocols:\n        for subdomain in subdomains:\n            pattern = f\"{protocol}{subdomain}.fbi.gov/*ndis-statistics*\"\n            print(f\"\\nSearching: {pattern}\")\n            \n            params = {\n                \"url\":         pattern,\n                \"matchType\":   \"wildcard\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) > 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests to avoid rate limiting\n            time.sleep(2)\n    \n    # Also search your specific known URLs with both protocols\n    known_paths = [\n        \"www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n        \"www.fbi.gov/about-us/laboratory/biometric-analysis/codis/ndis-statistics\", \n        \"www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n        \"le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\",\n    ]\n    \n    print(\"\\n\\nStarting exact URL searches...\")\n    for path in known_paths:\n        for protocol in protocols:\n            url = f\"{protocol}{path}\"\n            print(f\"\\nSearching: {url}\")\n            \n            params = {\n                \"url\":         url,\n                \"matchType\":   \"exact\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) > 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests\n            time.sleep(2)\n    \n    # Create DataFrame\n    snap_df = (pd.DataFrame(\n                    all_rows,\n                    columns=[\"timestamp\", \"original\", \"mimetype\", \"status\"])\n               .sort_values(\"timestamp\")\n               .reset_index(drop=True))\n    \n    return snap_df\n\n# Check if we already have snapshot data or need to search\nsnapshot_csv = META_DIR / 'snapshots_found.csv'\nif snapshot_csv.exists():\n    print(\"Loading existing snapshot list...\")\n    snap_df = pd.read_csv(snapshot_csv)\n    print(f\"Loaded {len(snap_df)} snapshots\")\nelse:\n    print(\"Searching for all NDIS snapshots...\")\n    snap_df = search_all_ndis_snapshots()\n    if len(snap_df) > 0:\n        snap_df.to_csv(snapshot_csv, index=False)\n        print(f\"\\nSaved {len(snap_df)} snapshots to {snapshot_csv}\")\n\nif len(snap_df) > 0:\n    print(f\"\\nTotal unique snapshots found: {len(snap_df):,}\")\n    print(f\"Unique URLs found: {snap_df['original'].nunique()}\")\n    print(\"\\nUnique URL patterns found:\")\n    for url in sorted(snap_df['original'].unique()):\n        print(f\"  {url}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSearching for all NDIS snapshots...\nStarting wildcard searches...\n\nSearching: http://www.fbi.gov/*ndis-statistics*\n  → Failed after retries\n\nSearching: http://le.fbi.gov/*ndis-statistics*\n  → Failed after retries\n\nSearching: http://*.fbi.gov/*ndis-statistics*\n  → Failed after retries\n\nSearching: https://www.fbi.gov/*ndis-statistics*\n  → Failed after retries\n\nSearching: https://le.fbi.gov/*ndis-statistics*\n  → Failed after retries\n\nSearching: https://*.fbi.gov/*ndis-statistics*\n  → Failed after retries\n\n\nStarting exact URL searches...\n\nSearching: http://www.fbi.gov/about-us/lab/codis/ndis-statistics\n  → Found 41 new snapshots\n\nSearching: https://www.fbi.gov/about-us/lab/codis/ndis-statistics\n  → Found 0 new snapshots\n\nSearching: http://www.fbi.gov/about-us/laboratory/biometric-analysis/codis/ndis-statistics\n  → No results\n\nSearching: https://www.fbi.gov/about-us/laboratory/biometric-analysis/codis/ndis-statistics\n  → No results\n\nSearching: http://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\n  → Found 232 new snapshots\n\nSearching: https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\n  → Found 0 new snapshots\n\nSearching: http://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\n  → Found 40 new snapshots\n\nSearching: https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\n  → Found 4 new snapshots\n\nSaved 317 snapshots to ../raw/wayback_meta/snapshots_found.csv\n\nTotal unique snapshots found: 317\nUnique URLs found: 8\n\nUnique URL patterns found:\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics/\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics/\n  https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics/\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics//\n```\n:::\n:::\n\n\n## Download Functions\n\n::: {#966bf37e .cell execution_count=5}\n``` {.python .cell-code}\ndef download_with_retry(url, max_retries=3, initial_delay=5, consecutive_failures=0):\n    \"\"\"Download with adaptive retry logic based on consecutive failures\"\"\"\n    if consecutive_failures > 0:\n        extra_wait = consecutive_failures * 10\n        print(f\"\\n    Adding {extra_wait}s cooldown due to {consecutive_failures} consecutive failures...\")\n        time.sleep(extra_wait)\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            return response, True\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"\\n    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:\n                wait_time = initial_delay * (2 ** attempt) * 2\n                print(f\"\\n    Rate limited (429). Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"\\n    HTTP Error: {e}\")\n                return None, False\n        except Exception as e:\n            print(f\"\\n    Unexpected error: {e}\")\n            return None, False\n    return None, False\n\ndef download_missing_snapshots(snap_df, output_folder):\n    \"\"\"Download HTML snapshots with resume capability and detailed logging\"\"\"\n    \n    # Create run-specific log file\n    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_log_file = META_DIR / f\"download_log_{run_timestamp}.txt\"\n    \n    # Check what we already have\n    existing_files = list(output_folder.glob(\"*.html\"))\n    existing_timestamps = {f.stem for f in existing_files}\n    print(f\"\\nFiles already downloaded: {len(existing_files)}\")\n    \n    # Check what needs to be downloaded\n    to_download = []\n    for _, row in snap_df.iterrows():\n        timestamp = row['timestamp']\n        url = row['original']\n        filename = output_folder / f\"{timestamp}.html\"\n        \n        if timestamp not in existing_timestamps and not filename.exists():\n            to_download.append((timestamp, url, filename))\n    \n    print(f\"Files to download: {len(to_download)}\")\n    \n    # Initialize log file\n    with open(run_log_file, \"w\") as log:\n        log.write(f\"Download run started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Total snapshots in list: {len(snap_df)}\\n\")\n        log.write(f\"Already downloaded: {len(existing_files)}\\n\")\n        log.write(f\"To download: {len(to_download)}\\n\")\n        log.write(f\"{'='*60}\\n\\n\")\n    \n    if len(to_download) == 0:\n        print(\"\\n✓ All files already downloaded! Nothing to do.\")\n        with open(run_log_file, \"a\") as log:\n            log.write(\"All files already downloaded. No action needed.\\n\")\n        return\n    \n    # Download configuration\n    BATCH_SIZE = 15\n    PAUSE_BETWEEN_DOWNLOADS = 3\n    PAUSE_BETWEEN_BATCHES = 45\n    PAUSE_AFTER_FAILURE = 60\n    \n    # Track statistics\n    successful_downloads = 0\n    failed_downloads = []\n    consecutive_failures = 0\n    \n    # Download in batches\n    for i in range(0, len(to_download), BATCH_SIZE):\n        batch = to_download[i:i + BATCH_SIZE]\n        batch_num = (i // BATCH_SIZE) + 1\n        total_batches = (len(to_download) + BATCH_SIZE - 1) // BATCH_SIZE\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Batch {batch_num}/{total_batches} ({len(batch)} files)\")\n        print(f\"Overall progress: {len(existing_timestamps) + successful_downloads}/{len(snap_df)} total files\")\n        print(f\"{'='*60}\")\n        \n        with open(run_log_file, \"a\") as log:\n            log.write(f\"\\nBatch {batch_num}/{total_batches} started at {datetime.now().strftime('%H:%M:%S')}\\n\")\n        \n        for j, (timestamp, url, filename) in enumerate(batch, 1):\n            # Double-check file doesn't exist\n            if filename.exists():\n                print(f\"\\n[{j}/{len(batch)}] {timestamp} - Already exists, skipping...\")\n                with open(run_log_file, \"a\") as log:\n                    log.write(f\"{timestamp}  ⚡ already exists\\n\")\n                continue\n                \n            wayback_url = f\"https://web.archive.org/web/{timestamp}/{url}\"\n            \n            print(f\"\\n[{j}/{len(batch)}] Downloading {timestamp}...\", end=\"\")\n            \n            response, success = download_with_retry(wayback_url, consecutive_failures=consecutive_failures)\n            \n            if response and response.status_code == 200:\n                try:\n                    with open(filename, 'w', encoding='utf-8') as f:\n                        f.write(response.text)\n                    \n                    print(\" ✓ Success\")\n                    successful_downloads += 1\n                    consecutive_failures = 0\n                    \n                    with open(run_log_file, \"a\") as log:\n                        log.write(f\"{timestamp}  ✓ downloaded\\n\")\n                    \n                except Exception as e:\n                    print(f\" ✗ Error saving file: {e}\")\n                    failed_downloads.append((timestamp, url, str(e)))\n                    consecutive_failures += 1\n                    \n                    with open(run_log_file, \"a\") as log:\n                        log.write(f\"{timestamp}  ✗ failed: {str(e)}\\n\")\n            else:\n                print(\" ✗ Failed after retries\")\n                failed_downloads.append((timestamp, url, \"Download failed\"))\n                consecutive_failures += 1\n                \n                with open(run_log_file, \"a\") as log:\n                    log.write(f\"{timestamp}  ✗ failed: Download failed after retries\\n\")\n                \n                if j < len(batch):\n                    print(f\"    Taking {PAUSE_AFTER_FAILURE}s break after failure...\")\n                    time.sleep(PAUSE_AFTER_FAILURE)\n                    continue\n            \n            if j < len(batch) and consecutive_failures == 0:\n                print(f\"    Waiting {PAUSE_BETWEEN_DOWNLOADS} seconds...\")\n                time.sleep(PAUSE_BETWEEN_DOWNLOADS)\n        \n        if i + BATCH_SIZE < len(to_download):\n            print(f\"\\nBatch complete. Pausing {PAUSE_BETWEEN_BATCHES} seconds...\")\n            print(f\"This session: {successful_downloads} downloaded, {len(failed_downloads)} failed\")\n            time.sleep(PAUSE_BETWEEN_BATCHES)\n    \n    # Final summary\n    print(f\"\\n{'='*60}\")\n    print(f\"Download session complete!\")\n    print(f\"  Successfully downloaded: {successful_downloads}\")\n    print(f\"  Failed downloads: {len(failed_downloads)}\")\n    \n    # Write final summary to log\n    with open(run_log_file, \"a\") as log:\n        log.write(f\"\\n{'='*60}\\n\")\n        log.write(f\"Download run completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Successfully downloaded: {successful_downloads}\\n\")\n        log.write(f\"Failed downloads: {len(failed_downloads)}\\n\")\n        \n        if failed_downloads:\n            log.write(f\"\\nFailed downloads detail:\\n\")\n            for timestamp, url, error in failed_downloads:\n                log.write(f\"  {timestamp}: {error}\\n\")\n    \n    if failed_downloads:\n        print(f\"\\nFailed downloads:\")\n        for timestamp, url, error in failed_downloads[:10]:\n            print(f\"  {timestamp}: {error}\")\n        if len(failed_downloads) > 10:\n            print(f\"  ... and {len(failed_downloads) - 10} more\")\n    \n    print(f\"\\nDownload log saved to: {run_log_file}\")\n\n# Download missing files\nif len(snap_df) > 0:\n    download_missing_snapshots(snap_df, HTML_DIR)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFiles already downloaded: 317\nFiles to download: 0\n\n✓ All files already downloaded! Nothing to do.\n```\n:::\n:::\n\n\n## Download Status Check Functions\n\n::: {#e04298ca .cell execution_count=6}\n``` {.python .cell-code}\ndef get_latest_log():\n    \"\"\"Find and read the most recent download log\"\"\"\n    log_files = sorted(META_DIR.glob(\"download_log_*.txt\"))\n    if not log_files:\n        print(\"No download logs found.\")\n        return None\n    \n    latest_log = log_files[-1]\n    print(f\"Latest log: {latest_log.name}\")\n    return latest_log\n\ndef analyze_download_status():\n    \"\"\"Analyze the current download status and latest run results\"\"\"\n    \n    # Check what we have\n    html_files = list(HTML_DIR.glob(\"*.html\"))\n    downloaded_timestamps = {f.stem for f in html_files}\n    \n    # Check what we should have\n    snapshot_csv = META_DIR / 'snapshots_found.csv'\n    if snapshot_csv.exists():\n        snap_df = pd.read_csv(snapshot_csv)\n        expected_timestamps = set(snap_df['timestamp'].astype(str))\n    else:\n        print(\"No snapshot list found. Run search first.\")\n        return None\n    \n    # Calculate missing\n    missing_timestamps = expected_timestamps - downloaded_timestamps\n    \n    # Parse latest log for failures\n    latest_log = get_latest_log()\n    failed_in_last_run = []\n    \n    if latest_log:\n        with open(latest_log, 'r') as f:\n            for line in f:\n                if '✗ failed:' in line:\n                    timestamp = line.split()[0]\n                    if timestamp.isdigit() and len(timestamp) == 14:\n                        failed_in_last_run.append(timestamp)\n    \n    # Create summary\n    print(f\"\\n{'='*60}\")\n    print(\"DOWNLOAD STATUS SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"Expected snapshots: {len(expected_timestamps)}\")\n    print(f\"Downloaded: {len(downloaded_timestamps)} ({len(downloaded_timestamps)/len(expected_timestamps)*100:.1f}%)\")\n    print(f\"Missing: {len(missing_timestamps)}\")\n    \n    if latest_log:\n        print(f\"\\nLatest run ({latest_log.name}):\")\n        print(f\"  Failed downloads: {len(failed_in_last_run)}\")\n        if failed_in_last_run:\n            print(f\"  Failed timestamps: {', '.join(failed_in_last_run[:5])}\")\n            if len(failed_in_last_run) > 5:\n                print(f\"  ... and {len(failed_in_last_run) - 5} more\")\n    \n    print(f\"\\nTotal still needed: {len(missing_timestamps)}\")\n    \n    return {\n        'missing': missing_timestamps,\n        'failed_last_run': failed_in_last_run,\n        'downloaded': downloaded_timestamps,\n        'expected': expected_timestamps\n    }\n\ndef create_retry_list(status_info, retry_only_failed=True):\n    \"\"\"Create a list of files to retry downloading\"\"\"\n    if not status_info:\n        return None\n    \n    if retry_only_failed and status_info['failed_last_run']:\n        retry_timestamps = set(status_info['failed_last_run'])\n        print(f\"\\nWill retry {len(retry_timestamps)} failed downloads from last run\")\n    else:\n        retry_timestamps = status_info['missing']\n        print(f\"\\nWill retry all {len(retry_timestamps)} missing files\")\n    \n    snapshot_csv = META_DIR / 'snapshots_found.csv'\n    snap_df = pd.read_csv(snapshot_csv)\n    retry_df = snap_df[snap_df['timestamp'].astype(str).isin(retry_timestamps)]\n    \n    return retry_df\n```\n:::\n\n\n## Parser Functions\n\n::: {#96499d13 .cell execution_count=7}\n``` {.python .cell-code}\ndef clean_jurisdiction_name(name):\n    \"\"\"Clean up jurisdiction names by removing common prefixes\"\"\"\n    name = re.sub(r'^.*?Back to top\\s*', '', name)\n    name = re.sub(r'^.*?Tables by NDIS Participant\\s*', '', name)\n    name = re.sub(r'^.*?ation\\.\\s*', '', name)\n    name = name.strip()\n    return name\n\ndef standardize_jurisdiction_name(name):\n    \"\"\"Standardize jurisdiction names to handle variations\"\"\"\n    name = clean_jurisdiction_name(name)\n    if name in JURISDICTION_NAME_MAP:\n        return JURISDICTION_NAME_MAP[name]\n    return name\n\ndef extract_data_date(html_content):\n    \"\"\"Extract the 'Statistics as of' date from HTML content\"\"\"\n    match = re.search(r'Statistics as of (\\w+ \\d{4})', html_content, re.IGNORECASE)\n    if match:\n        date_str = match.group(1)\n        try:\n            # Convert \"October 2024\" to datetime\n            return datetime.strptime(date_str, \"%B %Y\")\n        except:\n            pass\n    return None\n\ndef parse_ndis_snapshot(html_file):\n    \"\"\"Parse a single NDIS snapshot file\"\"\"\n    timestamp = html_file.stem\n    year = int(timestamp[:4])\n    \n    html_content = html_file.read_text('utf-8', errors='ignore')\n    soup = BeautifulSoup(html_content, 'lxml')\n    text = soup.get_text(' ', strip=True)\n    \n    # Extract the \"as of\" date\n    data_date = extract_data_date(html_content)\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    records = []\n    \n    # Pattern for 2010 (no arrestee data)\n    if year <= 2010:\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Samples\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': '0',\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    else:\n        # Pattern for 2011+ (includes arrestee data)\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Arrestee\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Profiles\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, arrestee, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': arrestee.replace(',', ''),\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    \n    return records\n```\n:::\n\n\n## Process All Snapshots\n\n::: {#8d943ce2 .cell execution_count=8}\n``` {.python .cell-code}\ndef process_all_snapshots():\n    \"\"\"Parse all downloaded snapshots and create datasets\"\"\"\n    print(\"Processing all snapshots...\")\n    \n    all_records = []\n    html_files = sorted(HTML_DIR.glob(\"*.html\"))\n    \n    for html_file in tqdm(html_files, desc=\"Parsing HTML files\"):\n        try:\n            records = parse_ndis_snapshot(html_file)\n            all_records.extend(records)\n        except Exception as e:\n            print(f\"Error parsing {html_file.name}: {e}\")\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(all_records)\n    \n    # Convert numeric fields\n    numeric_fields = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n    for field in numeric_fields:\n        df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0).astype(int)\n    \n    # Add datetime columns\n    df['capture_datetime'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S')\n    df['capture_date'] = df['capture_datetime'].dt.date\n    \n    # Sort by timestamp and jurisdiction\n    df = df.sort_values(['timestamp', 'jurisdiction'])\n    \n    return df\n\n# Process all files\ndf_raw = process_all_snapshots()\nprint(f\"\\nProcessed {len(df_raw)} total records\")\nprint(f\"Unique jurisdictions: {df_raw['jurisdiction'].nunique()}\")\nprint(f\"Date range: {df_raw['capture_datetime'].min()} to {df_raw['capture_datetime'].max()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing all snapshots...\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"00d36a5465994af99953fd517106d710\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nProcessed 16118 total records\nUnique jurisdictions: 54\nDate range: 2010-10-14 04:38:19 to 2025-06-29 17:15:50\n```\n:::\n:::\n\n\n## Check Download Completeness\n\n::: {#3049d82c .cell execution_count=9}\n``` {.python .cell-code}\n# Check if we have all expected files\nprint(\"\\nChecking download completeness...\")\nstatus = analyze_download_status()\n\n# Automatically retry if there are failures\nif status and status['failed_last_run'] and len(status['failed_last_run']) > 0:\n    print(f\"\\n⚠️  Found {len(status['failed_last_run'])} failed downloads from last run. Retrying...\")\n    retry_df = create_retry_list(status, retry_only_failed=True)\n    if retry_df is not None and len(retry_df) > 0:\n        download_missing_snapshots(retry_df, HTML_DIR)\n        \n        # Re-check status after retry\n        print(\"\\nRechecking status after retry...\")\n        status = analyze_download_status()\nelif status and status['missing']:\n    print(f\"\\n⚠️  {len(status['missing'])} files are missing but weren't from a failed run.\")\n    print(\"These may be new snapshots. Run download cell manually if needed.\")\nelse:\n    print(\"\\n✅ All files successfully downloaded!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nChecking download completeness...\nLatest log: download_log_20250721_154936.txt\n\n============================================================\nDOWNLOAD STATUS SUMMARY\n============================================================\nExpected snapshots: 317\nDownloaded: 317 (100.0%)\nMissing: 0\n\nLatest run (download_log_20250721_154936.txt):\n  Failed downloads: 0\n\nTotal still needed: 0\n\n✅ All files successfully downloaded!\n```\n:::\n:::\n\n\n## Apply Typo Fixes\n\n::: {#920cd794 .cell execution_count=10}\n``` {.python .cell-code}\ndef apply_typo_fixes(df):\n    \"\"\"Apply known typo corrections\"\"\"\n    df_fixed = df.copy()\n    \n    for typo in KNOWN_TYPOS:\n        mask = (\n            (df_fixed['timestamp'] == typo['timestamp']) & \n            (df_fixed['jurisdiction'] == typo['jurisdiction'])\n        )\n        if mask.any():\n            df_fixed.loc[mask, typo['field']] = int(typo['correct_value'])\n            print(f\"Fixed typo: {typo['jurisdiction']} on {typo['timestamp'][:8]} - \"\n                  f\"{typo['field']} from {typo['wrong_value']} to {typo['correct_value']}\")\n    \n    return df_fixed\n\n# Apply fixes\ndf_fixed = apply_typo_fixes(df_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFixed typo: California on 20250105 - investigations_aided from 1304657 to 130465\nFixed typo: California on 20250116 - investigations_aided from 1304657 to 130465\n```\n:::\n:::\n\n\n## Save Datasets\n\n::: {#f973524b .cell execution_count=11}\n``` {.python .cell-code}\n# Save datasets\ndf_raw.to_csv(OUTPUT_DIR / 'ndis_data_raw.csv', index=False)\ndf_fixed.to_csv(OUTPUT_DIR / 'ndis_data_fixed.csv', index=False)\nprint(f\"\\nSaved raw data to: {OUTPUT_DIR / 'ndis_data_raw.csv'}\")\nprint(f\"Saved fixed data to: {OUTPUT_DIR / 'ndis_data_fixed.csv'}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSaved raw data to: ../output/ndis/ndis_data_raw.csv\nSaved fixed data to: ../output/ndis/ndis_data_fixed.csv\n```\n:::\n:::\n\n\n## Summary Statistics\n\n::: {#6204b8ef .cell execution_count=12}\n``` {.python .cell-code}\n# Calculate summary statistics\nlatest_data = df_fixed[df_fixed['capture_datetime'] == df_fixed['capture_datetime'].max()]\nlatest_data = latest_data[latest_data['jurisdiction'] != 'D.C./Metro PD']\n\nprint(\"\\nLatest Statistics Summary:\")\nprint(f\"  As of: {latest_data['capture_datetime'].iloc[0]}\")\nprint(f\"  Data from: {latest_data['data_as_of_date'].iloc[0] if latest_data['data_as_of_date'].iloc[0] else 'Unknown'}\")\nprint(f\"  Jurisdictions reporting: {len(latest_data)}\")\nprint(f\"  Total offender profiles: {latest_data['offender_profiles'].sum():,}\")\nprint(f\"  Total arrestee profiles: {latest_data['arrestee'].sum():,}\")\nprint(f\"  Total forensic profiles: {latest_data['forensic_profiles'].sum():,}\")\nprint(f\"  Total investigations aided: {latest_data['investigations_aided'].sum():,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLatest Statistics Summary:\n  As of: 2025-06-29 17:15:50\n  Data from: 2025-04-01 00:00:00\n  Jurisdictions reporting: 53\n  Total offender profiles: 18,431,162\n  Total arrestee profiles: 5,879,537\n  Total forensic profiles: 1,405,917\n  Total investigations aided: 730,426\n```\n:::\n:::\n\n\n## Visualizations\n\n::: {#61f199fe .cell fig-height='18' fig-width='16' execution_count=13}\n``` {.python .cell-code}\ndef create_visualizations(df_raw, df_fixed):\n    \"\"\"Create comprehensive visualizations\"\"\"\n    # Exclude D.C./Metro PD from visualizations\n    df_raw_viz = df_raw[df_raw['jurisdiction'] != 'D.C./Metro PD']\n    df_fixed_viz = df_fixed[df_fixed['jurisdiction'] != 'D.C./Metro PD']\n    \n    # Create figure with subplots\n    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n    \n    # 1. Jurisdictions reporting over time\n    for ax, (df, title_suffix) in zip([axes[0,0], axes[0,1]], \n                                      [(df_raw_viz, 'Raw'), (df_fixed_viz, 'Fixed')]):\n        jurisdictions_per_date = df.groupby('capture_datetime')['jurisdiction'].nunique()\n        ax.plot(jurisdictions_per_date.index, jurisdictions_per_date.values, 'b-', linewidth=2)\n        ax.set_title(f'Jurisdictions Reporting ({title_suffix})')\n        ax.set_ylabel('Number of Jurisdictions')\n        ax.grid(True, alpha=0.3)\n        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n    \n    # 2. Total investigations aided\n    for ax, (df, title_suffix) in zip([axes[1,0], axes[1,1]], \n                                      [(df_raw_viz, 'Raw with Typo'), (df_fixed_viz, 'Fixed')]):\n        total_inv = df.groupby('capture_datetime')['investigations_aided'].sum()\n        ax.plot(total_inv.index, total_inv.values / 1e3, 'purple', linewidth=2)\n        ax.set_title(f'Total Investigations Aided ({title_suffix})')\n        ax.set_ylabel('Thousands of Investigations')\n        ax.grid(True, alpha=0.3)\n        \n        # Highlight the typo in raw data\n        if 'Raw' in title_suffix:\n            typo_dates = df[(df['jurisdiction'] == 'California') & \n                          (df['investigations_aided'] > 1000000)]['capture_datetime']\n            for date in typo_dates:\n                ax.axvline(x=date, color='red', linestyle='--', alpha=0.5)\n                ax.text(date, ax.get_ylim()[1]*0.9, 'Typo', rotation=90, \n                       verticalalignment='bottom', color='red')\n    \n    # 3. Data lag analysis\n    ax = axes[2, 0]\n    df_with_lag = df_fixed_viz[df_fixed_viz['data_as_of_date'].notna()].copy()\n    df_with_lag['data_lag_days'] = (df_with_lag['capture_datetime'] - df_with_lag['data_as_of_date']).dt.days\n    \n    avg_lag = df_with_lag.groupby('capture_datetime')['data_lag_days'].mean()\n    ax.plot(avg_lag.index, avg_lag.values, 'orange', linewidth=2)\n    ax.set_title('Average Data Lag (Capture Date vs \"As Of\" Date)')\n    ax.set_ylabel('Days')\n    ax.grid(True, alpha=0.3)\n    \n    # 4. California investigations over time (showing typo fix)\n    ax = axes[2, 1]\n    cal_raw = df_raw_viz[df_raw_viz['jurisdiction'] == 'California']\n    cal_fixed = df_fixed_viz[df_fixed_viz['jurisdiction'] == 'California']\n    \n    ax.plot(cal_raw['capture_datetime'], cal_raw['investigations_aided'], \n            'r-', label='Raw (with typo)', linewidth=2, alpha=0.7)\n    ax.plot(cal_fixed['capture_datetime'], cal_fixed['investigations_aided'], \n            'g-', label='Fixed', linewidth=2)\n    ax.set_title('California Investigations Aided: Raw vs Fixed')\n    ax.set_ylabel('Investigations Aided')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / 'ndis_analysis_complete.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Create visualizations\nprint(\"\\nCreating visualizations...\")\ncreate_visualizations(df_raw, df_fixed)\nprint(\"\\nProcessing complete!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCreating visualizations...\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ndis_analysis_files/figure-html/cell-14-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nProcessing complete!\n```\n:::\n:::\n\n\n",
    "supporting": [
      "ndis_analysis_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"00d36a5465994af99953fd517106d710\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_15e42324fe43415a8527d41c1ea114a5\",\"IPY_MODEL_41ce528ac07049688cb0ecc3f8ceeca4\",\"IPY_MODEL_fdb8dc95a9ac40aeb826f6f9c92ef341\"],\"layout\":\"IPY_MODEL_c4236c221c7d403a977d3bc8a5ebdbea\",\"tabbable\":null,\"tooltip\":null}},\"15e42324fe43415a8527d41c1ea114a5\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_35451db4b19d4172975f5bdb921b7291\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_9ff3b573a0fe4ff3bfee5ae07e0bb841\",\"tabbable\":null,\"tooltip\":null,\"value\":\"Parsing HTML files: 100%\"}},\"35451db4b19d4172975f5bdb921b7291\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"3f09325aaf194a968acbd44755dc28e4\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"41ce528ac07049688cb0ecc3f8ceeca4\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_3f09325aaf194a968acbd44755dc28e4\",\"max\":317,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_fa0b5cdc34ec4363b8613643d2d906df\",\"tabbable\":null,\"tooltip\":null,\"value\":317}},\"48e4b69db3cf4183b57194b9b75803b2\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"6cbff3614c1d485593badf5fa7ec1895\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"9ff3b573a0fe4ff3bfee5ae07e0bb841\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"c4236c221c7d403a977d3bc8a5ebdbea\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"fa0b5cdc34ec4363b8613643d2d906df\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"fdb8dc95a9ac40aeb826f6f9c92ef341\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_6cbff3614c1d485593badf5fa7ec1895\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_48e4b69db3cf4183b57194b9b75803b2\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 317/317 [00:17&lt;00:00, 49.95it/s]\"}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}